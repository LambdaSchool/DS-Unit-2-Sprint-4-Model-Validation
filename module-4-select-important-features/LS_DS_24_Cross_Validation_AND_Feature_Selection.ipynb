{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_24_Cross-Validation-AND-Feature-Selection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "3ctjQBseh0Cw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_Lambda School Data Science - Model Validation_\n",
        "\n",
        "## Example solution to the Cross-Validation assignment — plus Feature Selection!\n",
        "\n",
        "See also Sebastian Raschka's example, [Basic Pipeline and Grid Search Setup](https://github.com/rasbt/python-machine-learning-book/blob/master/code/bonus/svm_iris_pipeline_and_gridsearch.ipynb)."
      ]
    },
    {
      "metadata": {
        "id": "K6ZRZsa-q9cs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install tsfresh\n",
        "#!pip install seaborn --upgrade\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TpdhfLv-A2HA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Here in we pull in all our feature creation functions (from Jake Vanderplas and a few from our cohort)"
      ]
    },
    {
      "metadata": {
        "id": "aHUVU_AMBKs0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We'll modify a project from Python Data Science Handbook by Jake VanderPlas\n",
        "# https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Example:-Predicting-Bicycle-Traffic\n",
        "    \n",
        "# Predicting Bicycle Traffic\n",
        "\n",
        "# As an example, let's take a look at whether we can predict the number of \n",
        "# bicycle trips across Seattle's Fremont Bridge based on weather, season, \n",
        "# and other factors.\n",
        "\n",
        "# We will join the bike data with another dataset, and try to determine the \n",
        "# extent to which weather and seasonal factors—temperature, precipitation, \n",
        "# and daylight hours—affect the volume of bicycle traffic through this corridor. \n",
        "# Fortunately, the NOAA makes available their daily weather station data \n",
        "# (I used station ID USW00024233) and we can easily use Pandas to join \n",
        "# the two data sources.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import f_regression, SelectKBest\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "\n",
        "\n",
        "def load(): \n",
        "    fremont_bridge = 'https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD'\n",
        "    \n",
        "    bicycle_weather = 'https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/data/BicycleWeather.csv'\n",
        "\n",
        "    counts = pd.read_csv(fremont_bridge, index_col='Date', parse_dates=True, \n",
        "                         infer_datetime_format=True)\n",
        "\n",
        "    weather = pd.read_csv(bicycle_weather, index_col='DATE', parse_dates=True, \n",
        "                          infer_datetime_format=True)\n",
        "\n",
        "    daily = counts.resample('d').sum()\n",
        "    daily['Total'] = daily.sum(axis=1)\n",
        "    daily = daily[['Total']] # remove other columns\n",
        "\n",
        "    weather_columns = ['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'AWND']\n",
        "    daily = daily.join(weather[weather_columns], how='inner')\n",
        "    \n",
        "    # Make a feature for yesterday's total\n",
        "    daily['Total_yesterday'] = daily.Total.shift(1)\n",
        "    daily = daily.drop(index=daily.index[0])\n",
        "  \n",
        "  \n",
        "    daily.insert(0, 'ID', range(0, len(daily)))\n",
        "\n",
        "    return daily\n",
        "\n",
        "  \n",
        "def ordinal(daily):\n",
        "    daily = daily.copy()\n",
        "    # Add a column of the dates\n",
        "    daily['date_'] = daily.index\n",
        "    \n",
        "    # Date stuff https://docs.python.org/3/library/datetime.html#datetime.date.fromisoformat\n",
        "    # Add some day context\n",
        "    daily.insert(0, 'weekday_', daily.date_.apply(lambda x: x.weekday()))\n",
        "    daily.insert(0, 'dayOfMonth_',  daily.date_.apply(lambda x: x.day))\n",
        "    daily.insert(0, 'month_',  daily.date_.apply(lambda x: x.month))\n",
        "    daily.insert(0, 'year_',  daily.date_.apply(lambda x: x.year))  \n",
        "    \n",
        "    # Drop date column\n",
        "    daily = daily.drop(columns='date_')\n",
        "    \n",
        "    return daily\n",
        "  \n",
        "    \n",
        "def split(daily):\n",
        "    # Hold out an \"out-of-time\" test set, from the last 100 days of data\n",
        "    \n",
        "    train = daily[:-100]\n",
        "    test = daily[-100:]\n",
        "    \n",
        "    X_train = train.drop(columns='Total')\n",
        "    y_train = train.Total\n",
        "\n",
        "    X_test  = test.drop(columns='Total')\n",
        "    y_test  = test.Total\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "  \n",
        "  \n",
        "def one_hot_encoded(X):\n",
        "    X = X.copy()\n",
        "\n",
        "    # patterns of use generally vary from day to day; \n",
        "    # let's add binary columns that indicate the day of the week:\n",
        "    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "    for i, day in enumerate(days):\n",
        "        X[day] = (X.index.dayofweek == i).astype(float)\n",
        "\n",
        "\n",
        "    # we might expect riders to behave differently on holidays; \n",
        "    # let's add an indicator of this as well:\n",
        "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "    cal = USFederalHolidayCalendar()\n",
        "    holidays = cal.holidays('2012', '2016')\n",
        "    X = X.join(pd.Series(1, index=holidays, name='holiday'))\n",
        "    X['holiday'].fillna(0, inplace=True)\n",
        "    \n",
        "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "    for i, month in enumerate(months):\n",
        "        X[month] = (X.index.month == i+1).astype(float)\n",
        "    \n",
        "    return X\n",
        "\n",
        "  \n",
        "\n",
        "def jake_wrangle(X):  \n",
        "    X = X.copy()\n",
        "\n",
        "    # We also might suspect that the hours of daylight would affect \n",
        "    # how many people ride; let's use the standard astronomical calculation \n",
        "    # to add this information:\n",
        "    def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
        "        \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
        "        days = (date - pd.datetime(2000, 12, 21)).days\n",
        "        m = (1. - np.tan(np.radians(latitude))\n",
        "             * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
        "        return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
        "\n",
        "    X['daylight_hrs'] = list(map(hours_of_daylight, X.index))\n",
        "\n",
        "\n",
        "    # temperatures are in 1/10 deg C; convert to C\n",
        "    X['TMIN'] /= 10\n",
        "    X['TMAX'] /= 10\n",
        "\n",
        "    # We can also calcuate the average temperature.\n",
        "    X['Temp (C)'] = 0.5 * (X['TMIN'] + X['TMAX'])\n",
        "\n",
        "\n",
        "    # precip is in 1/10 mm; convert to inches\n",
        "    X['PRCP'] /= 254\n",
        "\n",
        "    # In addition to the inches of precipitation, let's add a flag that \n",
        "    # indicates whether a day is dry (has zero precipitation):\n",
        "    X['dry day'] = (X['PRCP'] == 0).astype(int)\n",
        "\n",
        "\n",
        "    # Let's add a counter that increases from day 1, and measures how many \n",
        "    # years have passed. This will let us measure any observed annual increase \n",
        "    # or decrease in daily crossings:\n",
        "    X['annual'] = (X.index - X.index[0]).days / 365.\n",
        "\n",
        "    return X\n",
        "\n",
        "  \n",
        "\n",
        "def wrangle(X):\n",
        "    # From Daniel H (DS1 KotH)\n",
        "    X = X.copy()\n",
        "    X = X.replace(-9999, 0)\n",
        "    X = jake_wrangle(X)\n",
        "    \n",
        "    X['PRCP_yest'] = X.PRCP.shift(1).fillna(X.PRCP.mean())\n",
        "    X['Windchill'] = (((X['Temp (C)'] * (9/5) + 32) * .6215) + 34.74) - (35.75 * (X['AWND']** .16)) + (.4275 * (X['Temp (C)'])) * (X['AWND'] ** .16)\n",
        "    X['Rl_Cold'] = (((X['Temp (C)'] * (9/5) + 32) - X['Windchill']) -32) * (5/9)\n",
        "    X['TMIN_ln'] = X['TMIN'] **2\n",
        "    \n",
        "    return X\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQN2O9PCAkBn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download and join data into a dataframe\n",
        "data = load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_YElAe92dSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "87a0126a-7d46-4c63-b61d-c2b50445e7dd"
      },
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Total</th>\n",
              "      <th>PRCP</th>\n",
              "      <th>SNOW</th>\n",
              "      <th>SNWD</th>\n",
              "      <th>TMAX</th>\n",
              "      <th>TMIN</th>\n",
              "      <th>AWND</th>\n",
              "      <th>Total_yesterday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2012-10-04</th>\n",
              "      <td>0</td>\n",
              "      <td>3475.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>189</td>\n",
              "      <td>83</td>\n",
              "      <td>65</td>\n",
              "      <td>3521.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-05</th>\n",
              "      <td>1</td>\n",
              "      <td>3148.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>217</td>\n",
              "      <td>89</td>\n",
              "      <td>57</td>\n",
              "      <td>3475.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-06</th>\n",
              "      <td>2</td>\n",
              "      <td>2006.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>239</td>\n",
              "      <td>78</td>\n",
              "      <td>51</td>\n",
              "      <td>3148.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-07</th>\n",
              "      <td>3</td>\n",
              "      <td>2142.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>239</td>\n",
              "      <td>78</td>\n",
              "      <td>13</td>\n",
              "      <td>2006.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-08</th>\n",
              "      <td>4</td>\n",
              "      <td>3537.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211</td>\n",
              "      <td>78</td>\n",
              "      <td>19</td>\n",
              "      <td>2142.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            ID   Total  PRCP  SNOW  SNWD  TMAX  TMIN  AWND  Total_yesterday\n",
              "2012-10-04   0  3475.0     0     0     0   189    83    65           3521.0\n",
              "2012-10-05   1  3148.0     0     0     0   217    89    57           3475.0\n",
              "2012-10-06   2  2006.0     0     0     0   239    78    51           3148.0\n",
              "2012-10-07   3  2142.0     0     0     0   239    78    13           2006.0\n",
              "2012-10-08   4  3537.0     0     0     0   211    78    19           2142.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "bC-3OGTDE2ku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "d72ccabc-eac2-43c3-994a-e550d82ead62"
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(data.ID, data.Total)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdfd78f9358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "o6nrCnpa9uXN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### This is an example of a pipeline + parameter grid. "
      ]
    },
    {
      "metadata": {
        "id": "WvakVkHHBIAG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Remember: If your dataset is massive, be sure to take a random subset sample to try fitting with the training data to start getting scores. "
      ]
    },
    {
      "metadata": {
        "id": "g8GngZEJZ1Zd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Benchmark / Baseline using Dummy Regressor in Pipe"
      ]
    },
    {
      "metadata": {
        "id": "PTsw1_cOY3Jb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "252b5aac-d40f-45d9-f2cd-339c3a960de7"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = split(data)\n",
        "\n",
        "# Define an estimator and param_grid\n",
        "# WHEN DEFINING YOU CAN GIVE IT A NAME OTHERWISE IT WILL USE THE PIPELINE NAME AUTOGEN NAME (name of the function but lowercase)\n",
        "pipe = make_pipeline(\n",
        "    RobustScaler(), \n",
        "    DummyRegressor(strategy='mean'))\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "### Get the scores with the MAE Function\n",
        "## Predict with X_test features and Compare predictions to y_test labels\n",
        "y_pred = pipe.predict(X_train)\n",
        "dummy_train_score = mean_absolute_error(y_train, y_pred)\n",
        "print('Dummy Regression Train Score:', dummy_train_score)\n",
        "\n",
        "\n",
        "## Predict with X_test features and Compare predictions to y_test labels\n",
        "y_pred = pipe.predict(X_test)\n",
        "dummy_test_score = mean_absolute_error(y_test, y_pred)\n",
        "print('Dummy Regression Test Score:', dummy_test_score)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dummy Regression Train Score: 980.8981106765484\n",
            "Dummy Regression Test Score: 1341.2051609553478\n",
            "CPU times: user 14.9 ms, sys: 13 ms, total: 27.9 ms\n",
            "Wall time: 14 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NJrNph8IFyOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "625e7e4d-f73e-4aef-a4e1-decbe4d23d91"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Define an estimator and param_grid\n",
        "# WHEN DEFINING YOU CAN GIVE IT A NAME OTHERWISE IT WILL USE THE PIPELINE NAME AUTOGEN NAME (name of the function but lowercase)\n",
        "pipe = make_pipeline(\n",
        "    RobustScaler(), \n",
        "    SelectKBest(f_regression), \n",
        "    Ridge())\n",
        "\n",
        "param_grid = {\n",
        "    'selectkbest__k': range(1, len(X_train.columns)+1), \n",
        "    'ridge__alpha': [0.1, 1.0, 10.]\n",
        "}\n",
        "\n",
        "# Fit on the train set, with grid search cross-validation\n",
        "gs = GridSearchCV(pipe, param_grid=param_grid, cv=3, \n",
        "                  scoring='neg_mean_absolute_error', \n",
        "                  verbose=1)\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "validation_score = gs.best_score_\n",
        "print()\n",
        "print('Cross-Validation Score:', -validation_score)\n",
        "print()\n",
        "print('Best estimator:', gs.best_estimator_)\n",
        "print()\n",
        "\n",
        "\n",
        "### Get the score with the GridSearch's score method\n",
        "# It combines the predict and score calculation.\n",
        "RidgeBaseline_train_score = gs.score(X_train, y_train)\n",
        "print('RidgeBaseline Train Score:', -RidgeBaseline_train_score)\n",
        "\n",
        "RidgeBaseline_test_score = gs.score(X_test, y_test)\n",
        "print('RidgeBaseline Test Score:', -RidgeBaseline_test_score)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-Validation Score: 604.6453429336518\n",
            "\n",
            "Best estimator: Pipeline(memory=None,\n",
            "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
            "       with_scaling=True)), ('selectkbest', SelectKBest(k=4, score_func=<function f_regression at 0x7fdfdaae0c80>)), ('ridge', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "   normalize=False, random_state=None, solver='auto', tol=0.001))])\n",
            "\n",
            "RidgeBaseline Train Score: 601.7510401226398\n",
            "RidgeBaseline Test Score: 773.107268536548\n",
            "CPU times: user 784 ms, sys: 7.95 ms, total: 792 ms\n",
            "Wall time: 816 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:    0.8s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "RvIXSeL8as_q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Apply data `ordinal`"
      ]
    },
    {
      "metadata": {
        "id": "GEbQyUwpbSz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "f5a9fb44-e9fc-4903-dbdf-f0442b2432e3"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ordinal_data = ordinal(data)\n",
        "\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = split(ordinal_data)\n",
        "\n",
        "# Define an estimator and param_grid\n",
        "# WHEN DEFINING YOU CAN GIVE IT A NAME OTHERWISE IT WILL USE THE PIPELINE NAME AUTOGEN NAME (name of the function but lowercase)\n",
        "pipe = make_pipeline(\n",
        "    RobustScaler(), \n",
        "    SelectKBest(f_regression), \n",
        "    Ridge())\n",
        "\n",
        "param_grid = {\n",
        "    'selectkbest__k': range(1, len(X_train.columns)+1), \n",
        "    'ridge__alpha': [0.1, 1.0, 10.]\n",
        "}\n",
        "\n",
        "# Fit on the train set, with grid search cross-validation\n",
        "gs = GridSearchCV(pipe, param_grid=param_grid, cv=3, \n",
        "                  scoring='neg_mean_absolute_error', \n",
        "                  verbose=1)\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "validation_score = gs.best_score_\n",
        "print()\n",
        "print('Cross-Validation Score:', -validation_score)\n",
        "print()\n",
        "print('Best estimator:', gs.best_estimator_)\n",
        "print()\n",
        "\n",
        "\n",
        "### Get the score with the GridSearch's score method\n",
        "# It combines the predict and score calculation.\n",
        "ord_train_score = gs.score(X_train, y_train)\n",
        "print('Ordinal Train Score:', -ord_train_score)\n",
        "\n",
        "ord_test_score = gs.score(X_test, y_test)\n",
        "print('Ordinal Test Score:', -ord_test_score)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
            "\n",
            "Cross-Validation Score: 389.2116166662251\n",
            "\n",
            "Best estimator: Pipeline(memory=None,\n",
            "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
            "       with_scaling=True)), ('selectkbest', SelectKBest(k=5, score_func=<function f_regression at 0x7fdfdaae0c80>)), ('ridge', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "   normalize=False, random_state=None, solver='auto', tol=0.001))])\n",
            "\n",
            "Ordinal Train Score: 385.55035508465824\n",
            "Ordinal Test Score: 448.71439473552834\n",
            "CPU times: user 1.24 s, sys: 16 ms, total: 1.26 s\n",
            "Wall time: 1.25 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed:    1.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "eL9c8h5Fc7df",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Apply `one_hot_encoded` (classmates function)"
      ]
    },
    {
      "metadata": {
        "id": "K4BHDQRHarBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "9976e84d-3489-4a21-a7d5-44ef1810850b"
      },
      "cell_type": "code",
      "source": [
        "#%%time\n",
        "one_hot_data = one_hot_encoded(data)\n",
        "\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = split(one_hot_data)\n",
        "\n",
        "# Define an estimator and param_grid\n",
        "# WHEN DEFINING YOU CAN GIVE IT A NAME OTHERWISE IT WILL USE THE PIPELINE NAME AUTOGEN NAME (name of the function but lowercase)\n",
        "pipe = make_pipeline(\n",
        "    RobustScaler(), \n",
        "    SelectKBest(f_regression), \n",
        "    Ridge())\n",
        "\n",
        "param_grid = {\n",
        "    'selectkbest__k': range(1, len(X_train.columns)+1), \n",
        "    'ridge__alpha': [0.1, 1.0, 10.]\n",
        "}\n",
        "\n",
        "# Fit on the train set, with grid search cross-validation\n",
        "gs = GridSearchCV(pipe, param_grid=param_grid, cv=3, \n",
        "                  scoring='neg_mean_absolute_error', \n",
        "                  verbose=1)\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "validation_score = gs.best_score_\n",
        "print()\n",
        "print('Cross-Validation Score:', -validation_score)\n",
        "print()\n",
        "print('Best estimator:', gs.best_estimator_)\n",
        "print()\n",
        "\n",
        "\n",
        "### Get the score with the GridSearch's score method\n",
        "# It combines the predict and score calculation.\n",
        "oh_train_score = gs.score(X_train, y_train)\n",
        "print('Train Score:', -oh_train_score)\n",
        "\n",
        "oh_test_score = gs.score(X_test, y_test)\n",
        "print('Test Score:', -oh_test_score)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 84 candidates, totalling 252 fits\n",
            "\n",
            "Cross-Validation Score: 317.0056086250159\n",
            "\n",
            "Best estimator: Pipeline(memory=None,\n",
            "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
            "       with_scaling=True)), ('selectkbest', SelectKBest(k=24, score_func=<function f_regression at 0x7fdfdaae0c80>)), ('ridge', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "   normalize=False, random_state=None, solver='auto', tol=0.001))])\n",
            "\n",
            "Train Score: 290.9695287750331\n",
            "Test Score: 625.7986098051067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 252 out of 252 | elapsed:    4.4s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1h5aYb_2ljSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "b1592868-c130-46c8-b6cb-dcb715344b5b"
      },
      "cell_type": "code",
      "source": [
        "print('Ordinal Train Score:', -ord_train_score)\n",
        "print('Ordinal Test Score:', -ord_test_score)\n",
        "print('\\n')\n",
        "print('One-Hot Train Score:', -oh_train_score)\n",
        "print('One-Hot Test Score:', -oh_test_score)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ordinal Train Score: 385.55035508465824\n",
            "Ordinal Test Score: 448.71439473552834\n",
            "\n",
            "\n",
            "One-Hot Train Score: 290.9695287750331\n",
            "One-Hot Test Score: 625.7986098051067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3g5j5mDZmjXz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Just feature Wrangling"
      ]
    },
    {
      "metadata": {
        "id": "fjrsha5JDcr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "c4332c68-2f63-4293-c969-ec209a6be101"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = split(data)\n",
        "\n",
        "# Do the same wrangling to X_train and X_test\n",
        "X_train = wrangle(X_train)\n",
        "X_test = wrangle(X_test)\n",
        "\n",
        "# Define an estimator and param_grid\n",
        "# WHEN DEFINING YOU CAN GIVE IT A NAME OTHERWISE IT WILL USE THE PIPELINE NAME AUTOGEN NAME (name of the function but lowercase)\n",
        "pipe = make_pipeline(\n",
        "    RobustScaler(), \n",
        "    SelectKBest(f_regression), \n",
        "    Ridge())\n",
        "\n",
        "param_grid = {\n",
        "    'selectkbest__k': range(1, len(X_train.columns)+1), \n",
        "    'ridge__alpha': [0.1, 1.0, 10.]\n",
        "}\n",
        "\n",
        "# Fit on the train set, with grid search cross-validation\n",
        "gs = GridSearchCV(pipe, param_grid=param_grid, cv=3, \n",
        "                  scoring='neg_mean_absolute_error', \n",
        "                  verbose=1)\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "validation_score = gs.best_score_\n",
        "print()\n",
        "print('Cross-Validation Score:', -validation_score)\n",
        "print()\n",
        "print('Best estimator:', gs.best_estimator_)\n",
        "print()\n",
        "\n",
        "\n",
        "### Get the score with the GridSearch's score method\n",
        "# It combines the predict and score calculation.\n",
        "RRtrain_score = gs.score(X_train, y_train)\n",
        "print('Just Wrangling Train Score:', -RRtrain_score)\n",
        "\n",
        "RRtest_score = gs.score(X_test, y_test)\n",
        "print('Just Wrangling Test Score:', -RRtest_score)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
            "\n",
            "Cross-Validation Score: 595.6496902158523\n",
            "\n",
            "Best estimator: Pipeline(memory=None,\n",
            "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
            "       with_scaling=True)), ('selectkbest', SelectKBest(k=15, score_func=<function f_regression at 0x7fdfdaae0c80>)), ('ridge', Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "   normalize=False, random_state=None, solver='auto', tol=0.001))])\n",
            "\n",
            "Just Wrangling Train Score: 590.9809401021859\n",
            "Just Wrangling Test Score: 770.2808314805894\n",
            "CPU times: user 2.72 s, sys: 1.67 s, total: 4.39 s\n",
            "Wall time: 2.24 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 144 out of 144 | elapsed:    2.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "gWiacK_Lm5dS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Comparing 3 seperate runs. "
      ]
    },
    {
      "metadata": {
        "id": "P34vjyyCuLCH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Comparing 1 set of variables for each\n",
        "# This is how I did it at first.\n",
        "\n",
        "print('Dummy Regression Train Score:', dummy_train_score)\n",
        "print('Dummy Regression Test Score:', dummy_test_score)\n",
        "print('\\n')\n",
        "print('RidgeBaseline Train Score:', -RidgeBaseline_train_score)\n",
        "print('RidgeBaseline Test Score:', -RidgeBaseline_test_score)\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "print('Just Ordinal Train Score:', -ord_train_score)\n",
        "print('Just Ordinal Test Score:', -ord_test_score)\n",
        "print('\\n')\n",
        "\n",
        "print('Just One-Hot Train Score:', -oh_train_score)\n",
        "print('Just One-Hot Test Score:', -oh_test_score)\n",
        "print('\\n')\n",
        "\n",
        "print('Just Wrangling Train Score:', -RRtrain_score)\n",
        "print('Just Wrangling Test Score:', -RRtest_score)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lCkyXP0jwvg8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# then I wrapped a function and made it better. \n",
        "def compare(data, name):\n",
        "  X_train, X_test, y_train, y_test = split(data)\n",
        "\n",
        "  pipe = make_pipeline(\n",
        "      RobustScaler(), \n",
        "      SelectKBest(f_regression), \n",
        "      Ridge())\n",
        "\n",
        "  param_grid = {\n",
        "      'selectkbest__k': range(1, len(X_train.columns)+1), \n",
        "      'ridge__alpha': [0.1, 1.0, 10.]\n",
        "  }\n",
        "\n",
        "  # Fit on the train set, with grid search cross-validation\n",
        "  gs = GridSearchCV(pipe, param_grid=param_grid, cv=3, \n",
        "                    scoring='neg_mean_absolute_error', \n",
        "                    verbose=0)\n",
        "  gs.fit(X_train, y_train)\n",
        "  validation_score = gs.best_score_\n",
        "  print(f'\\n{name} Cross-Validation Score: {-validation_score}')\n",
        "  print(f'{name} Train Score:            {-gs.score(X_train, y_train)}')\n",
        "  print(f'{name} Test Score:             {-gs.score(X_test, y_test)}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-yqblnz6wu_w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "paAAnN6Zm420",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "8b81d226-d58a-48a7-9a91-4f7d72e71d41"
      },
      "cell_type": "code",
      "source": [
        "# Here I'm going to compare my dummy regression baseline to all of my other\n",
        "# feature engineered \n",
        "# Comparing 1 set of variables for each\n",
        "print(\"Some Comparisons\")\n",
        "\n",
        "print('Dummy Regression Train Score:', dummy_train_score)\n",
        "print('Dummy Regression Test Score:', dummy_test_score)\n",
        "print('\\n')\n",
        "\n",
        "###\n",
        "names = [\"Ridge Baseline\",'Just Ordinal','Just One Hot','Ord + One Hot','Wrangled + Ordinal','Wrangled + OH','All Combined']\n",
        "Z_data = data\n",
        "compare(Z_data, name)\n",
        "\n",
        "### \n",
        "name = 'Just Ordinal'\n",
        "A_data = ordinal(data)\n",
        "compare(A_data, name)\n",
        "\n",
        "### \n",
        "name = 'Just One Hot'\n",
        "B_data = one_hot_encoded(data)\n",
        "compare(B_data, name)\n",
        "\n",
        "### \n",
        "name = 'Ord + One Hot'\n",
        "C_data = one_hot_encoded(A_data)\n",
        "compare(C_data, name)\n",
        "\n",
        "### \n",
        "name = 'Wrangled + Ordinal'\n",
        "D_data = wrangle(A_data)\n",
        "compare(D_data, name)\n",
        "\n",
        "### \n",
        "name = 'Wrangled + OH'\n",
        "E_data = wrangle(B_data)\n",
        "compare(E_data, name)\n",
        "\n",
        "### \n",
        "name = 'All Combined'\n",
        "F_data = wrangle(C_data)\n",
        "compare(F_data, name)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some Comparisons\n",
            "Dummy Regression Train Score: 980.8981106765484\n",
            "Dummy Regression Test Score: 1341.2051609553478\n",
            "\n",
            "\n",
            "\n",
            "Ridge Baseline Cross-Validation Score: 604.6453429336518\n",
            "Ridge Baseline Train Score:            601.7510401226398\n",
            "Ridge Baseline Test Score:             773.107268536548\n",
            "\n",
            "\n",
            "Just Ordinal Cross-Validation Score: 389.2116166662251\n",
            "Just Ordinal Train Score:            385.55035508465824\n",
            "Just Ordinal Test Score:             448.71439473552834\n",
            "\n",
            "\n",
            "Just One Hot Cross-Validation Score: 317.0056086250159\n",
            "Just One Hot Train Score:            290.9695287750331\n",
            "Just One Hot Test Score:             625.7986098051067\n",
            "\n",
            "\n",
            "Ord + One Hot Cross-Validation Score: 316.7241668646224\n",
            "Ord + One Hot Train Score:            290.95264043258334\n",
            "Ord + One Hot Test Score:             635.7680843548015\n",
            "\n",
            "\n",
            "Wrangled + Ordinal Cross-Validation Score: 375.38130158393705\n",
            "Wrangled + Ordinal Train Score:            362.64459025651087\n",
            "Wrangled + Ordinal Test Score:             414.397277738424\n",
            "\n",
            "\n",
            "Wrangled + OH Cross-Validation Score: 297.1960150364704\n",
            "Wrangled + OH Train Score:            272.7878390752409\n",
            "Wrangled + OH Test Score:             335.93397588625544\n",
            "\n",
            "\n",
            "All Combined Cross-Validation Score: 297.1413242158009\n",
            "All Combined Train Score:            272.8330918063642\n",
            "All Combined Test Score:             333.8853138194594\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qrc_DQv0nb8g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Combine all 3 feature engineering sets. "
      ]
    },
    {
      "metadata": {
        "id": "zOe_jR8noXhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a9a6a927-5608-48c5-e8ba-9714e0b683de"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ordinal_data = ordinal(data)\n",
        "ord_oh_data = one_hot_encoded(ordinal_data)\n",
        "wrangled_oh_ord_data = wrangle(ord_oh_data)\n",
        "\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = split(wrangled_oh_ord_data)\n",
        "\n",
        "# Define an estimator and param_grid\n",
        "# WHEN DEFINING YOU CAN GIVE IT A NAME OTHERWISE IT WILL USE THE PIPELINE NAME AUTOGEN NAME (name of the function but lowercase)\n",
        "pipe = make_pipeline(\n",
        "    RobustScaler(), \n",
        "    SelectKBest(f_regression), \n",
        "    Ridge())\n",
        "\n",
        "param_grid = {\n",
        "    'selectkbest__k': range(1, len(X_train.columns)+1), \n",
        "    'ridge__alpha': [0.1, 1.0, 10.]\n",
        "}\n",
        "\n",
        "# Fit on the train set, with grid search cross-validation\n",
        "gs = GridSearchCV(pipe, param_grid=param_grid, cv=3, \n",
        "                  scoring='neg_mean_absolute_error', \n",
        "                  verbose=1)\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "validation_score = gs.best_score_\n",
        "print()\n",
        "print('Cross-Validation Score:', -validation_score)\n",
        "print()\n",
        "print('Best estimator:', gs.best_estimator_)\n",
        "print()\n",
        "\n",
        "\n",
        "### Get the score with the GridSearch's score method\n",
        "# It combines the predict and score calculation.\n",
        "fe_train_score = gs.score(X_train, y_train)\n",
        "print('Feature Engineered Train Score:', -fe_train_score)\n",
        "\n",
        "fe_test_score = gs.score(X_test, y_test)\n",
        "print('Feature Engineered Test Score:', -fe_test_score)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-Validation Score: 297.1413242158009\n",
            "\n",
            "Best estimator: Pipeline(memory=None,\n",
            "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
            "       with_scaling=True)), ('selectkbest', SelectKBest(k=32, score_func=<function f_regression at 0x7f212e69fd90>)), ('ridge', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "   normalize=False, random_state=None, solver='auto', tol=0.001))])\n",
            "\n",
            "Feature Engineered Train Score: 272.8330918063642\n",
            "Feature Engineered Test Score: 333.8853138194594\n",
            "CPU times: user 10.3 s, sys: 7.18 s, total: 17.5 s\n",
            "Wall time: 8.85 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 360 out of 360 | elapsed:    8.6s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Lik9uYdpmnH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "61d89201-0de0-41b8-b26d-c1723633619b"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ordinal_data = ordinal(data)\n",
        "ord_oh_data = one_hot_encoded(ordinal_data)\n",
        "wrangled_oh_ord_data = wrangle(ord_oh_data)\n",
        "\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = split(wrangled_oh_ord_data)\n",
        "\n",
        "# Define an estimator and param_grid\n",
        "# WHEN DEFINING YOU CAN GIVE IT A NAME OTHERWISE IT WILL USE THE PIPELINE NAME AUTOGEN NAME (name of the function but lowercase)\n",
        "pipe = make_pipeline(\n",
        "    RobustScaler(), \n",
        "    SelectKBest(f_regression), \n",
        "    Ridge())\n",
        "\n",
        "param_grid = {\n",
        "    'selectkbest__k': range(1, len(X_train.columns)+1), \n",
        "    'ridge__alpha': [0.1, 1.0, 10.]\n",
        "}\n",
        "\n",
        "# Fit on the train set, with grid search cross-validation\n",
        "gs = GridSearchCV(pipe, param_grid=param_grid, cv=3, \n",
        "                  scoring='neg_mean_absolute_error', \n",
        "                  verbose=1)\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "validation_score = gs.best_score_\n",
        "print()\n",
        "print('Cross-Validation Score:', -validation_score)\n",
        "print()\n",
        "print('Best estimator:', gs.best_estimator_)\n",
        "print()\n",
        "\n",
        "\n",
        "### Get the score with the GridSearch's score method\n",
        "# It combines the predict and score calculation.\n",
        "fe_train_score = gs.score(X_train, y_train)\n",
        "print('Feature Engineered Train Score:', -fe_train_score)\n",
        "\n",
        "fe_test_score = gs.score(X_test, y_test)\n",
        "print('Feature Engineered Test Score:', -fe_test_score)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-Validation Score: 297.1413242158009\n",
            "\n",
            "Best estimator: Pipeline(memory=None,\n",
            "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
            "       with_scaling=True)), ('selectkbest', SelectKBest(k=32, score_func=<function f_regression at 0x7f212e69fd90>)), ('ridge', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "   normalize=False, random_state=None, solver='auto', tol=0.001))])\n",
            "\n",
            "Feature Engineered Train Score: 272.8330918063642\n",
            "Feature Engineered Test Score: 333.8853138194594\n",
            "CPU times: user 10.2 s, sys: 7.19 s, total: 17.4 s\n",
            "Wall time: 8.82 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 360 out of 360 | elapsed:    8.6s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "W6pWKwFwNJ7Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Notes on Pipeline / GridScore"
      ]
    },
    {
      "metadata": {
        "id": "ZYJAHcNH_m2a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Then we can get the final test score (using MAE function)"
      ]
    },
    {
      "metadata": {
        "id": "ye1ZIriwALnD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using gs.predict calls all the grid score fit/transforms and applies them to our X_test data in this case. \n",
        "\n",
        "Then we can calculate our score useing those test predictions. "
      ]
    },
    {
      "metadata": {
        "id": "AR4bo95ZJFwj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "bb3306e6-a27e-4ba0-c633-28276555bd35"
      },
      "cell_type": "code",
      "source": [
        "# Predict with X_test features\n",
        "y_pred = gs.predict(X_test)\n",
        "\n",
        "# Compare predictions to y_test labels\n",
        "test_score = mean_absolute_error(y_test, y_pred)\n",
        "print('Test Score:', test_score)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score: 333.8853138194594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BAFwZ4iG_sA5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Or use the GridSearch's `score` method to get the final test score. "
      ]
    },
    {
      "metadata": {
        "id": "RhyoVpCmAbc2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This method just generates the test predictions and scores them, returning the (negative) test score to you. So you apply the negative symbol to flip the sign from negative to positive. "
      ]
    },
    {
      "metadata": {
        "id": "7MQYtnmxV159",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "1de05b60-3db7-4e95-d103-2fa1a4f14e7c"
      },
      "cell_type": "code",
      "source": [
        "train_score = gs.score(X_train, y_train)\n",
        "print('Train Score:', -train_score)\n",
        "\n",
        "# Or use the grid search's score method, \n",
        "# which combines these steps\n",
        "test_score = gs.score(X_test, y_test)\n",
        "\n",
        "print('Test Score:', -test_score)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Score: 272.8330918063642\n",
            "Test Score: 333.8853138194594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "krmq2YHOquGX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''def vary_scale(typ): \n",
        "  if typ=='robust': return RobustScaler\n",
        "  elif typ=='standard': return StandardScaler\n",
        "  else: Fail\n",
        "\n",
        "pipe = Pipeline(steps=[  ... , ('scale', FunctionTransformer(vary_scale)), ... ])\n",
        "\n",
        "param_grid = { \n",
        "    ... , \n",
        "    FunctionTransformer'vary_scale__choice': ['robust', 'standard'],\n",
        "    ... }\n",
        "\n",
        "search = GridSearchCV(pipe, param_grid, ...)''';"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vMAep3bWAqE9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now we can explore and see what features we actually ended up selecting. "
      ]
    },
    {
      "metadata": {
        "id": "7qeb9ed8bIAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "outputId": "7c254db4-41d5-472e-d149-441debc3b2c2"
      },
      "cell_type": "code",
      "source": [
        "# Which features were selected?\n",
        "# 'selectkbest' is the autogenerated name of the SelectKBest() function in the pipeline\n",
        "selector = gs.best_estimator_.named_steps['selectkbest']\n",
        "all_names = X_train.columns\n",
        "\n",
        "# get_support returns a mask of the columns in True / False\n",
        "selected_mask = selector.get_support()\n",
        "# Passing the boolean list as the column names creates a \n",
        "selected_names = all_names[selected_mask]\n",
        "unselected_names = all_names[~selected_mask]\n",
        "\n",
        "print('Features selected:')\n",
        "for name in selected_names:\n",
        "    print(name)\n",
        "\n",
        "print()\n",
        "print('Features not selected:')\n",
        "for name in unselected_names:\n",
        "    print(name)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features selected:\n",
            "year_\n",
            "weekday_\n",
            "ID\n",
            "PRCP\n",
            "TMAX\n",
            "TMIN\n",
            "AWND\n",
            "Total_yesterday\n",
            "Mon\n",
            "Tue\n",
            "Wed\n",
            "Thu\n",
            "Sat\n",
            "Sun\n",
            "holiday\n",
            "Jan\n",
            "Feb\n",
            "Mar\n",
            "May\n",
            "Jun\n",
            "Jul\n",
            "Aug\n",
            "Nov\n",
            "Dec\n",
            "daylight_hrs\n",
            "Temp (C)\n",
            "dry day\n",
            "annual\n",
            "PRCP_yest\n",
            "Windchill\n",
            "Rl_Cold\n",
            "TMIN_ln\n",
            "\n",
            "Features not selected:\n",
            "month_\n",
            "dayOfMonth_\n",
            "SNOW\n",
            "SNWD\n",
            "Fri\n",
            "Apr\n",
            "Sep\n",
            "Oct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9z11uJRB0dEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "d10ecbae-7700-4890-b768-7454004650cf"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "#%%time\n",
        "ordinal_data = ordinal(data)\n",
        "ord_oh_data = one_hot_encoded(ordinal_data)\n",
        "wrangled_oh_ord_data = wrangle(ord_oh_data)\n",
        "\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = split(wrangled_oh_ord_data)\n",
        "\n",
        "# Define an estimator and param_grid\n",
        "# WHEN DEFINING YOU CAN GIVE IT A NAME OTHERWISE IT WILL USE THE PIPELINE NAME AUTOGEN NAME (name of the function but lowercase)\n",
        "pipe = make_pipeline(\n",
        "    RobustScaler(), \n",
        "    PolynomialFeatures(degree=2),\n",
        "    SelectKBest(f_regression), \n",
        "    Ridge())\n",
        "\n",
        "param_grid = {\n",
        "    'selectkbest__k': range(1, len(X_train.columns)+1), \n",
        "    'ridge__alpha': [0.1, 1.0, 10.]\n",
        "}\n",
        "\n",
        "# Fit on the train set, with grid search cross-validation\n",
        "gs = GridSearchCV(pipe, param_grid=param_grid, cv=3, \n",
        "                  scoring='neg_mean_absolute_error', \n",
        "                  verbose=1)\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "validation_score = gs.best_score_\n",
        "print()\n",
        "print('Cross-Validation Score:', -validation_score)\n",
        "print()\n",
        "print('Best estimator:', gs.best_estimator_)\n",
        "print()\n",
        "\n",
        "\n",
        "### Get the score with the GridSearch's score method\n",
        "# It combines the predict and score calculation.\n",
        "fe_train_score = gs.score(X_train, y_train)\n",
        "print('Feature Engineered Train Score:', -fe_train_score)\n",
        "\n",
        "fe_test_score = gs.score(X_test, y_test)\n",
        "print('Feature Engineered Test Score:', -fe_test_score)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-Validation Score: 337.3047538164374\n",
            "\n",
            "Best estimator: Pipeline(memory=None,\n",
            "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
            "       with_scaling=True)), ('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('selectkbest', SelectKBest(k=36, score_func=<function f_regression at 0x7f212e69fd90>)), ('ridge', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "   normalize=False, random_state=None, solver='auto', tol=0.001))])\n",
            "\n",
            "Feature Engineered Train Score: 276.24247184972216\n",
            "Feature Engineered Test Score: 395.80984993785677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 360 out of 360 | elapsed:   30.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "w_Ya8eUJ7mii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "6abbf789-9af9-4f55-e2ce-bfdefe4588a0"
      },
      "cell_type": "code",
      "source": [
        "print('Feature Engineered Train Score:', -fe_train_score)\n",
        "print('Feature Engineered Test Score:', -fe_test_score)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature Engineered Train Score: 276.24247184972216\n",
            "Feature Engineered Test Score: 395.80984993785677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m1udZEbjrIVw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### An unecessary attempt with Tsfresh (Relevant Time Feature Augmentor)"
      ]
    },
    {
      "metadata": {
        "id": "PcgtQeE6zrp9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''# Find all the features, even the irrelevant ones.\n",
        "%%time\n",
        "from tsfresh.transformers import RelevantFeatureAugmenter\n",
        "from tsfresh import extract_features\n",
        "\n",
        "extracted_features = extract_features(wrangled_oh_ord_data, column_id=\"order\", column_sort=\"ID\")\n",
        "\n",
        "from tsfresh.utilities.dataframe_functions import impute\n",
        "\n",
        "features = impute(extracted_features)''';"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wl-Ei064wPQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "99bf5206-0f91-415e-f27e-3c86299ed56f"
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(963, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "urdgBk_Vp6ZN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BONUS: Recursive Feature Elimination!\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html"
      ]
    },
    {
      "metadata": {
        "id": "EEvISfuimpQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "outputId": "a0f9c076-82cc-42f0-b08a-b8ec32b1d1c7"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "X_train_scaled = RobustScaler().fit_transform(X_train)\n",
        "rfe = RFECV(Ridge(alpha=1.0), scoring='neg_mean_absolute_error', cv=3)\n",
        "X_train_subset = rfe.fit_transform(X_train_scaled, y_train)\n",
        "\n",
        "all_names = X_train.columns\n",
        "selected_mask = rfe.support_\n",
        "selected_names = all_names[selected_mask]\n",
        "unselected_names = all_names[~selected_mask]\n",
        "\n",
        "print('Features selected:')\n",
        "for name in selected_names:\n",
        "    print(name)\n",
        "\n",
        "print()\n",
        "print('Features not selected:')\n",
        "for name in unselected_names:\n",
        "    print(name)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features selected:\n",
            "dayOfMonth_\n",
            "weekday_\n",
            "PRCP\n",
            "TMAX\n",
            "TMIN\n",
            "AWND\n",
            "Total_yesterday\n",
            "Mon\n",
            "Tue\n",
            "Wed\n",
            "Thu\n",
            "Fri\n",
            "Sat\n",
            "Sun\n",
            "holiday\n",
            "Feb\n",
            "Mar\n",
            "Apr\n",
            "May\n",
            "Jun\n",
            "Oct\n",
            "Dec\n",
            "daylight_hrs\n",
            "Temp (C)\n",
            "dry day\n",
            "Windchill\n",
            "Rl_Cold\n",
            "TMIN_ln\n",
            "\n",
            "Features not selected:\n",
            "year_\n",
            "month_\n",
            "ID\n",
            "SNOW\n",
            "SNWD\n",
            "Jan\n",
            "Jul\n",
            "Aug\n",
            "Sep\n",
            "Nov\n",
            "annual\n",
            "PRCP_yest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hoCUHkuGpjvX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_subset = pd.DataFrame(X_train_subset, columns=selected_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZKoH475ppag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_subset = rfe.transform(X_test)\n",
        "X_test_subset = pd.DataFrame(X_test_subset, columns=selected_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mhcS0n8ApuUx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "856d3237-adbf-48e7-fe75-0f1612e85eb3"
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_train_subset.shape, X_test.shape, X_test_subset.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(963, 40) (963, 28) (100, 40) (100, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_OpHu92Yto88",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RFE again, but with polynomial features and interaction terms!"
      ]
    },
    {
      "metadata": {
        "id": "cxtlgAQTOaXW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Apply a Polynomial Transformation"
      ]
    },
    {
      "metadata": {
        "id": "ArgRECEUtyH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "48111c41-1bdf-46bb-f75f-66376e3382b2"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_train_polynomial = poly.fit_transform(X_train)\n",
        "\n",
        "print(X_train.shape, X_train_polynomial.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(963, 40) (963, 861)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YGdOekpYOemW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Apply a Scaler & RFECV"
      ]
    },
    {
      "metadata": {
        "id": "TWVHh35wsdE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "537989d3-f143-4218-eca1-0ba6f9f3e265"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_polynomial)\n",
        "\n",
        "rfe = RFECV(Ridge(alpha=1.0), scoring='neg_mean_absolute_error', \n",
        "            step=10, cv=3, verbose=0)\n",
        "\n",
        "X_train_subset = rfe.fit_transform(X_train_scaled, y_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 13.7 s, sys: 10.6 s, total: 24.3 s\n",
            "Wall time: 12.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xjWSr7JfOwlg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Look at all of those new features!\n"
      ]
    },
    {
      "metadata": {
        "id": "TrMRGvezuDD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "0e97e8e0-9395-46f9-ae52-61f0a1b40f64"
      },
      "cell_type": "code",
      "source": [
        "all_names = poly.get_feature_names(X_train.columns)\n",
        "selected_mask = rfe.support_\n",
        "selected_names = [name for name, selected in zip(all_names, selected_mask) if selected]\n",
        "\n",
        "print(f'{rfe.n_features_} Features selected')\n",
        "'''for name in selected_names:\n",
        "    print(name)''';"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "171 Features selected:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k7QIGrbrNt7b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now we can experiment with mulitple ridge settings for selecting features. "
      ]
    },
    {
      "metadata": {
        "id": "Pku8KixHv3CF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "ad9bc389-12d6-496a-a209-fe1a68c8289b"
      },
      "cell_type": "code",
      "source": [
        "# Define an estimator and param_grid\n",
        "\n",
        "ridge = Ridge()\n",
        "\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 0.75, 1.0, 2.1, 2.2, 2.3, 2.4, 2.5, 3.0, 4.0, 10.]\n",
        "}\n",
        "\n",
        "# Fit on the train set, with grid search cross-validation\n",
        "gs = GridSearchCV(ridge, param_grid=param_grid, cv=3, \n",
        "                  scoring='neg_mean_absolute_error', \n",
        "                  verbose=1)\n",
        "\n",
        "gs.fit(X_train_subset, y_train)\n",
        "validation_score = gs.best_score_\n",
        "print()\n",
        "print('Cross-Validation Score:', -validation_score)\n",
        "print()\n",
        "print('Best estimator:', gs.best_estimator_)\n",
        "print()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-Validation Score: 256.61330969002734\n",
            "\n",
            "Best estimator: Ridge(alpha=2.2, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:    0.6s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NnpbyDLSwPIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "cad7b431-6fbd-4c6a-c897-a6155dc28423"
      },
      "cell_type": "code",
      "source": [
        "# Do the same transformations to X_test (Applying the same transformations to each set so we can check the test scores)\n",
        "X_test_polynomial = poly.transform(X_test)\n",
        "X_test_scaled = scaler.transform(X_test_polynomial)\n",
        "X_test_subset = rfe.transform(X_test_scaled)\n",
        "\n",
        "# Use the grid search's score method with X_test_subset\n",
        "train_score = gs.score(X_train_subset, y_train)\n",
        "print('Train Score:', -train_score)\n",
        "test_score = gs.score(X_test_subset, y_test)\n",
        "print('Test Score:', -test_score)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Score: 197.61173458116826\n",
            "Test Score: 356.485481175808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oEoqn553SgZZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Exploring other regression\n"
      ]
    },
    {
      "metadata": {
        "id": "8Akh6ApESza8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
        "#\n",
        "# License: BSD 3 clause\n",
        "\n",
        "from __future__ import division, print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels \\\n",
        "    import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "\n",
        "def load_mauna_loa_atmospheric_co2():\n",
        "    ml_data = fetch_openml(data_id=41187)\n",
        "    months = []\n",
        "    ppmv_sums = []\n",
        "    counts = []\n",
        "\n",
        "    y = ml_data.data[:, 0]\n",
        "    m = ml_data.data[:, 1]\n",
        "    month_float = y + (m - 1) / 12\n",
        "    ppmvs = ml_data.target\n",
        "\n",
        "    for month, ppmv in zip(month_float, ppmvs):\n",
        "        if not months or month != months[-1]:\n",
        "            months.append(month)\n",
        "            ppmv_sums.append(ppmv)\n",
        "            counts.append(1)\n",
        "        else:\n",
        "            # aggregate monthly sum to produce average\n",
        "            ppmv_sums[-1] += ppmv\n",
        "            counts[-1] += 1\n",
        "\n",
        "    months = np.asarray(months).reshape(-1, 1)\n",
        "    avg_ppmvs = np.asarray(ppmv_sums) / counts\n",
        "    return months, avg_ppmvs\n",
        "\n",
        "\n",
        "X, y = load_mauna_loa_atmospheric_co2()\n",
        "\n",
        "print(load_mauna_loa_atmospheric_co2())\n",
        "\n",
        "# Kernel with parameters given in GPML book\n",
        "k1 = 66.0**2 * RBF(length_scale=67.0)  # long term smooth rising trend\n",
        "k2 = 2.4**2 * RBF(length_scale=90.0) \\\n",
        "    * ExpSineSquared(length_scale=1.3, periodicity=1.0)  # seasonal component\n",
        "# medium term irregularity\n",
        "k3 = 0.66**2 \\\n",
        "    * RationalQuadratic(length_scale=1.2, alpha=0.78)\n",
        "k4 = 0.18**2 * RBF(length_scale=0.134) \\\n",
        "    + WhiteKernel(noise_level=0.19**2)  # noise terms\n",
        "kernel_gpml = k1 + k2 + k3 + k4\n",
        "\n",
        "gp = GaussianProcessRegressor(kernel=kernel_gpml, alpha=0,\n",
        "                              optimizer=None, normalize_y=True)\n",
        "gp.fit(X, y)\n",
        "\n",
        "print(\"GPML kernel: %s\" % gp.kernel_)\n",
        "print(\"Log-marginal-likelihood: %.3f\"\n",
        "      % gp.log_marginal_likelihood(gp.kernel_.theta))\n",
        "\n",
        "# Kernel with optimized parameters\n",
        "k1 = 50.0**2 * RBF(length_scale=50.0)  # long term smooth rising trend\n",
        "k2 = 2.0**2 * RBF(length_scale=100.0) \\\n",
        "    * ExpSineSquared(length_scale=1.0, periodicity=1.0,\n",
        "                     periodicity_bounds=\"fixed\")  # seasonal component\n",
        "# medium term irregularities\n",
        "k3 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
        "k4 = 0.1**2 * RBF(length_scale=0.1) \\\n",
        "    + WhiteKernel(noise_level=0.1**2,\n",
        "                  noise_level_bounds=(1e-3, np.inf))  # noise terms\n",
        "kernel = k1 + k2 + k3 + k4\n",
        "\n",
        "gp = GaussianProcessRegressor(kernel=kernel, alpha=0,\n",
        "                              normalize_y=True)\n",
        "gp.fit(X, y)\n",
        "\n",
        "print(\"\\nLearned kernel: %s\" % gp.kernel_)\n",
        "print(\"Log-marginal-likelihood: %.3f\"\n",
        "      % gp.log_marginal_likelihood(gp.kernel_.theta))\n",
        "\n",
        "X_ = np.linspace(X.min(), X.max() + 30, 1000)[:, np.newaxis]\n",
        "y_pred, y_std = gp.predict(X_, return_std=True)\n",
        "\n",
        "# Illustration\n",
        "plt.scatter(X, y, c='k')\n",
        "plt.plot(X_, y_pred)\n",
        "plt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,\n",
        "                 alpha=0.5, color='k')\n",
        "plt.xlim(X_.min(), X_.max())\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(r\"CO$_2$ in ppm\")\n",
        "plt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\n",
        "plt.tight_layout()\n",
        "plt.show()''';"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "08G7Dgyfrpwv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}