{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My_U2S4dot4_Feature_Selection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wel51x/DS-Unit-2-Sprint-4-Model-Validation/blob/master/My_U2S4dot4_Feature_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EktuEhKabPw1",
        "colab_type": "text"
      },
      "source": [
        "_Lambda School Data Science - Model Validation_\n",
        "\n",
        "\n",
        "# Feature Selection\n",
        "\n",
        "Objectives:\n",
        "* Feature importance\n",
        "* Feature selection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqC17z_cbPw5",
        "colab_type": "text"
      },
      "source": [
        "## Yesterday we saw that...\n",
        "\n",
        "## Less isn't always more (but sometimes it is)\n",
        "\n",
        "## More isn't always better (but sometimes it is)\n",
        "\n",
        "\n",
        "![Image of Terry Crews](https://media.giphy.com/media/b8kHKZq3YFfnq/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90pyhgLYbPw8",
        "colab_type": "text"
      },
      "source": [
        "Saavas, Ando [Feature Selection (4 parts)](https://blog.datadive.net/selecting-good-features-part-i-univariate-selection/)\n",
        "\n",
        ">There are in general two reasons why feature selection is used:\n",
        "1. Reducing the number of features, to reduce overfitting and improve the generalization of models.\n",
        "2. To gain a better understanding of the features and their relationship to the response variables.\n",
        "\n",
        ">These two goals are often at odds with each other and thus require different approaches: depending on the data at hand a feature selection method that is good for goal (1) isn’t necessarily good for goal (2) and vice versa. What seems to happen often though is that people use their favourite method (or whatever is most conveniently accessible from their tool of choice) indiscriminately, especially methods more suitable for (1) for achieving (2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkCG574vbPxB",
        "colab_type": "text"
      },
      "source": [
        "While they are not always mutually exclusive, here's a little bit about what's going on with these two goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJhlNZEEbPxE",
        "colab_type": "text"
      },
      "source": [
        "### Goal 1: Reducing Features, Reducing Overfitting, Improving Generalization of Models\n",
        "\n",
        "This is when you're actually trying to engineer a packaged, machine learning pipeline that is streamlined and highly generalizable to novel data as more is collected, and you don't really care \"how\" it works as long as it does work. \n",
        "\n",
        "Approaches that are good at this tend to fail at Goal 2 because they handle multicollinearity by (sometime randomly) choosing/indicating just one of a group of strongly correlated features. This is good to reduce redundancy, but bad if you want to interpret the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfvPQq4ebPxI",
        "colab_type": "text"
      },
      "source": [
        "### Goal 2: Gaining a Better Understanding of the Features and their Relationships\n",
        "\n",
        "This is when you want a good, interpretable model or you're doing data science more for analysis than engineering. Company asks you \"How do we increase X?\" and you can tell them all the factors that correlate to it and their predictive power.\n",
        "\n",
        "Approaches that are good at this tend to fail at Goal 1 because, well, they *don't* handle the multicollinearity problem. If three features are all strongly correlated to each other as well as the output, they will all have high scores. But including all three features in a model is redundant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfJnm9g_bPxL",
        "colab_type": "text"
      },
      "source": [
        "### Each part in Saavas's Blog series describes an increasingly complex (and computationally costly) set of methods for feature selection and interpretation.\n",
        "\n",
        "The ultimate comparison is completed using an adaptation of a dataset called Friedman's 1 regression dataset from Friedman, Jerome H.'s '[Multivariate Adaptive Regression Splines](http://www.stat.ucla.edu/~cocteau/stat204/readings/mars.pdf).\n",
        ">The data is generated according to formula $y=10sin(πX_1X_2)+20(X_3–0.5)^2+10X_4+5X_5+ϵ$, where the $X_1$ to $X_5$ are drawn from uniform distribution and ϵ is the standard normal deviate N(0,1). Additionally, the original dataset had five noise variables $X_6,…,X_{10}$, independent of the response variable. We will increase the number of variables further and add four variables $X_{11},…,X_{14}$ each of which are very strongly correlated with $X_1,…,X_4$, respectively, generated by $f(x)=x+N(0,0.01)$. This yields a correlation coefficient of more than 0.999 between the variables. This will illustrate how different feature ranking methods deal with correlations in the data.\n",
        "\n",
        "**Okay, that's a lot--here's what you need to know:**\n",
        "1.   $X_1$ and $X_2$ have the same non-linear relationship to $Y$ -- though together they do have a not-quite-linear relationship to $Y$ (with sinusoidal noise--but the range of the values doesn't let it get negative)\n",
        "2.   $X_3$ has a quadratic relationship with $Y$\n",
        "3.   $X_4$ and $X_5$ have linear relationships to $Y$, with $X_4$ being weighted twice as heavily as $X_5$\n",
        "4.   $X_6$ through $X_{10}$ are random and have NO relationship to $Y$\n",
        "5.   $X_{11}$ through $X_{14}$ correlate strongly to $X_1$ through $X_4$ respectively (and thus have the same respective relationships with $Y$)\n",
        "\n",
        "\n",
        "This will help us see the difference between the models in selecting features and interpreting features\n",
        "* how well they deal with multicollinearity (#5)\n",
        "* how well they identify noise (#4)\n",
        "* how well they identify different kinds of relationships\n",
        "* how well they identify/interpret predictive power of individual variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvs96UWD4XQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import\n",
        "import numpy as np\n",
        "\n",
        "# Create the dataset\n",
        "# from https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "size = 1500 # I increased the size from what's given in the link\n",
        "Xs = np.random.uniform(0, 1, (size, 14)) \n",
        "# Changed variable name to Xs to use X later\n",
        " \n",
        "#\"Friedamn #1” regression problem\n",
        "Y = (10 * np.sin(np.pi*Xs[:,0]*Xs[:,1]) + 20*(Xs[:,2] - .5)**2 +\n",
        "     10*Xs[:,3] + 5*Xs[:,4] + np.random.normal(0,1))\n",
        "#Add 4 additional correlated variables (correlated with X1-X4)\n",
        "Xs[:,10:] = Xs[:,:4] + np.random.normal(0, .025, (size,4))\n",
        " \n",
        "names = [\"X%s\" % i for i in range(1,15)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qjVVgwKB37G",
        "colab_type": "code",
        "outputId": "e942089e-c106-4900-b1b2-32053c6cec50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Putting it into pandas--because... I like pandas. And usually you'll be\n",
        "# working with dataframes not arrays (you'll care what the column titles are)\n",
        "import pandas as pd\n",
        "\n",
        "friedmanX = pd.DataFrame(data=Xs, columns=names)\n",
        "friedmanY = pd.Series(data=Y, name='Y')\n",
        "\n",
        "friedman = friedmanX.join(friedmanY)\n",
        "\n",
        "friedman.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.374540</td>\n",
              "      <td>0.950714</td>\n",
              "      <td>0.731994</td>\n",
              "      <td>0.598658</td>\n",
              "      <td>0.156019</td>\n",
              "      <td>0.155995</td>\n",
              "      <td>0.058084</td>\n",
              "      <td>0.866176</td>\n",
              "      <td>0.601115</td>\n",
              "      <td>0.708073</td>\n",
              "      <td>0.413325</td>\n",
              "      <td>0.956360</td>\n",
              "      <td>0.698205</td>\n",
              "      <td>0.568729</td>\n",
              "      <td>16.730915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.181825</td>\n",
              "      <td>0.183405</td>\n",
              "      <td>0.304242</td>\n",
              "      <td>0.524756</td>\n",
              "      <td>0.431945</td>\n",
              "      <td>0.291229</td>\n",
              "      <td>0.611853</td>\n",
              "      <td>0.139494</td>\n",
              "      <td>0.292145</td>\n",
              "      <td>0.366362</td>\n",
              "      <td>0.223650</td>\n",
              "      <td>0.167876</td>\n",
              "      <td>0.280668</td>\n",
              "      <td>0.561691</td>\n",
              "      <td>9.112092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.592415</td>\n",
              "      <td>0.046450</td>\n",
              "      <td>0.607545</td>\n",
              "      <td>0.170524</td>\n",
              "      <td>0.065052</td>\n",
              "      <td>0.948886</td>\n",
              "      <td>0.965632</td>\n",
              "      <td>0.808397</td>\n",
              "      <td>0.304614</td>\n",
              "      <td>0.097672</td>\n",
              "      <td>0.617692</td>\n",
              "      <td>0.078302</td>\n",
              "      <td>0.583794</td>\n",
              "      <td>0.132396</td>\n",
              "      <td>3.017894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.034389</td>\n",
              "      <td>0.909320</td>\n",
              "      <td>0.258780</td>\n",
              "      <td>0.662522</td>\n",
              "      <td>0.311711</td>\n",
              "      <td>0.520068</td>\n",
              "      <td>0.546710</td>\n",
              "      <td>0.184854</td>\n",
              "      <td>0.969585</td>\n",
              "      <td>0.775133</td>\n",
              "      <td>0.037718</td>\n",
              "      <td>0.917699</td>\n",
              "      <td>0.267895</td>\n",
              "      <td>0.699477</td>\n",
              "      <td>10.220976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.088493</td>\n",
              "      <td>0.195983</td>\n",
              "      <td>0.045227</td>\n",
              "      <td>0.325330</td>\n",
              "      <td>0.388677</td>\n",
              "      <td>0.271349</td>\n",
              "      <td>0.828738</td>\n",
              "      <td>0.356753</td>\n",
              "      <td>0.280935</td>\n",
              "      <td>0.542696</td>\n",
              "      <td>0.045148</td>\n",
              "      <td>0.217475</td>\n",
              "      <td>0.017812</td>\n",
              "      <td>0.346724</td>\n",
              "      <td>9.770285</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         X1        X2        X3  ...       X13       X14          Y\n",
              "0  0.374540  0.950714  0.731994  ...  0.698205  0.568729  16.730915\n",
              "1  0.181825  0.183405  0.304242  ...  0.280668  0.561691   9.112092\n",
              "2  0.592415  0.046450  0.607545  ...  0.583794  0.132396   3.017894\n",
              "3  0.034389  0.909320  0.258780  ...  0.267895  0.699477  10.220976\n",
              "4  0.088493  0.195983  0.045227  ...  0.017812  0.346724   9.770285\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhDRfR-eF7pY",
        "colab_type": "text"
      },
      "source": [
        "We want to be able to look at classification problems too, so let's bin the Y values to create a categorical feature from the Y values. It should have *roughly* similar relationships to the X features as Y does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V1FuPAaFoUV",
        "colab_type": "code",
        "outputId": "1d918b00-1160-4a15-9fad-a1b4c7c7ff18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        }
      },
      "source": [
        "# First, let's take a look at what Y looks like\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.gcf().set_size_inches(15, 8)\n",
        "sns.distplot(friedmanY);"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAHjCAYAAABrbF1pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xl8nVWh7//v2ntnnscmTZomadK5\ntKUjnaAUpCCCA8iggjIdB456vGfQ+zrX4/V3vfd47lHkKB6lFBAVgaOoVYrM0AlKWygdKG0zNU3S\npJnnaWev3x8N3lIKTdskaw+f9+vVF3s/z0r2N/yxs79Zz7OWsdYKAAAAABB6PK4DAAAAAADODYUO\nAAAAAEIUhQ4AAAAAQhSFDgAAAABCFIUOAAAAAEIUhQ4AAAAAQhSFDgAAAABCFIUOAAAAAEIUhQ4A\nAAAAQpTPdYBTZWZm2sLCQtcxAAAAAMCJXbt2NVlrs0YyNugKXWFhoXbu3Ok6BgAAAAA4YYw5MtKx\nXHIJAAAAACGKQgcAAAAAIYpCBwAAAAAhikIHAAAAACFqRIXOGLPWGHPQGFNmjPnmac7HGGMeHz6/\n3RhTOHw8yhjzC2PMXmPMAWPMt0Y3PgAAAABErjMWOmOMV9J9kq6UNFPSTcaYmacMu11Sq7W2RNI9\nkr4/fPx6STHW2jmSFkj6m3fLHgAAAADg/Ixkhm6xpDJrbYW1dkDSY5KuPWXMtZJ+Mfz4t5LWGGOM\nJCspwRjjkxQnaUBSx6gkBwAAAIAIN5JClyfp6EnPa4aPnXaMtdYvqV1Shk6Uu25JxyRVS/p3a23L\nqS9gjLnLGLPTGLOzsbHxrH8IAAAAAIhEY70oymJJQ5ImSiqS9N+MMcWnDrLW3m+tXWitXZiVNaIN\n0QEAAAAg4o2k0NVKmnTS8/zhY6cdM3x5ZYqkZkk3S/qLtXbQWntc0lZJC883NAAAAABgZIVuh6RS\nY0yRMSZa0o2SNpwyZoOkW4cfXyfpRWut1YnLLC+VJGNMgqSlkt4ZjeAAAAAAEOnOWOiG74m7W9Iz\nkg5IesJau98Y811jzDXDw9ZLyjDGlEn6hqR3tza4T1KiMWa/ThTDh6y1e0b7hwAAAACASGROTKQF\nj4ULF9qdO3e6jgEAAAAAThhjdllrR3Sr2lgvigIAAAAAGCMUOgAAAAAIURQ6AAAAAAhRFDoAAAAA\nCFEUOgAAAAAIURQ6AAAAAAhRPtcBAAAYiUe3V4/ba928pGDcXgsAgPPBDB0AAAAAhCgKHQAAAACE\nKAodAAAAAIQoCh0AAAAAhCgKHQAAAACEKAodAAAAAIQoCh0AAAAAhCgKHQAAAACEKAodAAAAAIQo\nCh0AAAAAhCgKHQAAAACEKAodAAAAAIQoCh0AAAAAhCgKHQAAAACEKAodAAAAAIQoCh0AAAAAhCgK\nHQAAAACEKAodAAAAAIQoCh0AAAAAhCgKHQAAAACEKAodAAAAAIQoCh0AAAAAhCgKHQAAAACEKAod\nAAAAAIQoCh0AAAAAhCif6wAAAIy29t5BNXb2q6NvUJ29g2rv86uzb1AdvYOK8XmVGh+ltIRopcVH\nKS0+Wmnx0UqK9ckY4zo6AABnhUIHAAgLzV392l/XoX117app7X3Pudgoj5Jjo5QU61O/f0jv1Pep\nq9//njFp8VGak5ei2XkpstZS7gAAIYFCBwAIWc1d/dpb2659te2qa++TJOWnxWntrBzlp8cpJTZK\nSbFRiva9/w6DAX9AbT0Dau0ZVHN3vw41dGpLWZM2HW7Sn/bU6ao5ubp6zkTNzkum3AEAghaFDgAQ\nclq6B/TiOw16s7pNVlJBeryump2jWXkpSouPHtH3iPZ5lJ0cq+zkWElJWjYlUz0Dfh041qGmrgGt\n31ypn79Sodl5yfr6mqlaMyObYgcACDoUOgBAyGjvHdRLB49rZ1WLPMZoeUmmlk3JUOoIS9yZxEf7\ntGByum5eUqC2ngFt3Fuvn71Srjse2akL8lP09ctKtXoaxQ4AEDwodACAoNfU1a+n9tRpe2WLrJUW\nFabrkmnZSomLGrPXTI2P1s1LCnT9wnz9/s1a/fjFw7rt4Z2am5+ir182VZdMy6LYAQCco9ABAIKW\ntVaP7ziq7208oK4+vy4sSNOl07OVljA6M3IjEeX16NMLJ+kT8/P05Bs1+vGLZfrCwzt0UXGGvveJ\n2SrOShy3LAAAnIpCBwAISlVN3frWk3v1akWzlhana2lxhrKTYp3lifJ6dMOiAn1ifr4e31Gtf3vm\noNbeu1l3ry7RFy+ectqFVwAAGGv89gEABBX/UED3byrX2ns3aV9tu/7PJ+foN3cudVrmThbt8+hz\nFxXqhW9crMtnTtAPnzukq/5js3ZUtbiOBgCIQMzQAUAYeXR79bi91s1LCkb9ex441qF/+t0e7alp\n12UzJuh/fXy2clKCo8idKjs5VvfdfKGuu/C4/vkP+3T9z17VTYsn6ZtXzhjTe/sAADgZhQ4A4NxQ\nwGrd5gr94NmDSomL0k9unq+PzskNiUVHVk/P1nPfWKUfPX9Y67dU6pWDjfrRjfO1uCjddTQAQATg\nkksAgFM1rT26ed1r+ten39Ga6RP07N9drKsvmBgSZe5d8dE+/ferZujJLy1TtM+jG+9/VT987pD8\nQwHX0QAAYW5EM3TGmLWS7pXklfSAtfZfTzkfI+kRSQskNUu6wVpbZYz5jKR/OGnoBZIutNbuHo3w\nAIDQ9oc3a/U//rBPAWv1f6+7QNctyA+pInequZNS9eevrtS//HG//uOFw9pa1qQf3TBPk9LjP/Br\nQv0yWQCAW2ecoTPGeCXdJ+lKSTMl3WSMmXnKsNsltVprSyTdI+n7kmSt/bW1dp61dp6kz0mqpMwB\nANp7BvW3v3lTX398t6blJOnpr63S9QsnhXSZe1dijE8/+PRc3XvjPB2q79RV/7FZf3qrznUsAECY\nGskll4sllVlrK6y1A5Iek3TtKWOulfSL4ce/lbTGvP+38k3DXwsAiGDbypq09t5NenrvMf39R6bq\nsbuWqiDjg2ewQtW18/K08WsrVZqdqL/9zZv61pN71e8fch0LABBmRlLo8iQdPel5zfCx046x1vol\ntUvKOGXMDZJ+c7oXMMbcZYzZaYzZ2djYOJLcAIAQ0+8f0veeels3P7BdcVFePfnlZbr70lL5vOF7\nO/ek9Hg98TcX6cuXTNFvXq/WDT9/Tcfae13HAgCEkXH5LWqMWSKpx1q773TnrbX3W2sXWmsXZmVl\njUckAMA4eqe+Q9f+ZKvWba7UZ5cW6M9fXaEL8lNdxxoXPq9H/7h2un722QU63NCpj/14i16raHYd\nCwAQJkayKEqtpEknPc8fPna6MTXGGJ+kFJ1YHOVdN+oDZucAAOErELB6cGul/u0vB5Uc59ODn1+o\nS6dPcB3rjMZqoZI7VxbrV9urdfO613Tl7Fwtm5IRFvcNAgDcGUmh2yGp1BhTpBPF7UZJN58yZoOk\nWyW9Kuk6SS9aa60kGWM8kj4taeVohQYABL/q5h5988k92lberMtmTNC/fmqOMhNjXMdyKjs5Vl++\nZIr+a1eNntp7TLVtvfr4vDxF+8L3slMAwNg6Y6Gz1vqNMXdLekYnti140Fq73xjzXUk7rbUbJK2X\n9EtjTJmkFp0ofe9aJemotbZi9OMDAILNUMDqoa2V+vdnD8rn8ej/fHKOblwUHitYjobYKK8+s6RA\nLx9s1AsHGtTY2a9bLpqspNgo19EAACFoRPvQWWs3Stp4yrFvn/S4T9L1H/C1L0taeu4RAQCh4mB9\np/7xd3v01tE2XTo9W9/7xGzlpsS5jhV0PMbo0unZyk2J1WM7qvWfL5frlmWFykmOdR0NABBiuMYD\nAHDeBvwB3fPcIV3948062tKje2+cp/W3LqTMncGM3GTdtWqKhqzVz18p16GGTteRAAAhhkIHADhn\n1lq9dPC4rvqPzbr3hcP66JxcPf+Ni3XtvDwusRyhvNQ4feniKUqLj9Yjr1ZpeyUrYAIARm5El1wC\nAHCqg/Wd+l9Pva3Nh5tUlJmghz6/SKunZ7uOFZJS46P1N6uK9Zsd1frj7jq1dA3oitk58lCKAQBn\nQKEDAJyVrn6/nn+7Qf/8h71KjPHpf1w9U59bOpmVGs9TTJRXn1taqD/vqdPmsia19Q7q+oX58nn4\n/woA+GAUOgDAiAwOBbStvFkvHzyuwaGAbrmoUF9bU6q0hGjX0cKG12N0zdyJSouP1l/216tvcEg3\nLylQjM/rOhoAIEhR6AAAH8paq7217Xpmf71aewY1PSdJV87OVVZSjJ7eV+86XtgxxmjV1CzFR3v1\n+zdr9eCWSt26rFDx0fzKBgC8H78dAAAf6GhLjzbuPaYjLT3KSY7VbcvzVZKd6DpWRFhYmK64aK8e\n23FU92+q0BeWFykljr3qAADvRaEDALxPW8+Ann27QbuPtikxxqdPzM/TgslpLNIxzmZNTNHnl3n1\ny9eO6P5N5frC8iJlJsa4jgUACCIUOgDAX/X7h7TpUKM2H26SJF0yNUsXT81STBT3cLkyJStRd6wo\n0sPbqvTzTRW6bXkh+/sBAP6KQgcAUMBavXGkVc+93aDOfr8uyE/RFbNylBbPgifBID8tXnetKtZD\nW6v0wOZK3baiSHmplDoAABuLA0DEK2/s0n0vlenJN2uVGh+lL64q1o2LCihzQSY7KVZ3rixWjM+j\n9VsqVNva6zoSACAIMEMHAGPs0e3VriOcVkfvoP60p0776zqUGhelGxZN0gV5KTLcJxe00hOidefK\nYj2wpULrt1botuVFyk+Ldx0LAOAQM3QAEGGstdpZ1aIfvXBIB+s7dfnMCfq7y6dqbn4qZS4EpCVE\n646VxYqL8mr9lkodbelxHQkA4BCFDgAiSEv3gB7cWqkn36xVTnKcvrqmVKunZSvKy6+DUJIWf2Km\nLiHGpwe3Vqq6udt1JACAI/wGB4AIELBWW8uadO8Lh1TT2qtr503UHStZAj+UpQ6XusQYnx7cVqUj\nlDoAiEgUOgAIc519g1q3uUJP7T2m4sxEff2yqVpSlMGecmEgJS5Kd6wsVlKMTw9vq2KhFACIQBQ6\nAAhjNa09uu+lMtW19er6Bfm65aLJSomLch0LoyglLkq3ryhSXLRXD26tVH1Hn+tIAIBxRKEDgDD1\nxpFW3b+pQl6P0RcvnqL5BWksehKmUuOjdfvyIvm8Rg9uqVRTV7/rSACAcUKhA4AwMxSw+vOeOv32\njRoVZMTry5eUKDeFTajDXUZijG5bXqSAtVq/pVKtPQOuIwEAxgGFDgDCSE+/Xw9tq9S28mYtm5Kh\nLywrUkIMW45GignJsbpteZH6/UNav6VSHb2DriMBAMYYhQ4AwkRH36B+tqlc1c09uu7CfF19wUR5\nPVxiGWkmpsbp88uK1NXv1/qtlerq97uOBAAYQxQ6AAgDXf3+4RkZv25bXqQLJ6e5jgSHCtLjdctF\nk9XaPaBfbKtS/+CQ60gAgDFCoQOAENfT79eDWyrV1jOgW5cVqjAzwXUkBIHizETdvLhAx9p79evt\n1fIPBVxHAgCMAQodAISw3oEhPbj1xKqGn1taqCLKHE4yPTdZn5yfr7LGLv3XrhoNBazrSACAUUah\nA4AQ1Tc4pIe2Vaqhs1+fWTJZJdmJriMhCF04OU1rZ+Vob227/uef9staSh0AhBOWPgOAENTvH9LD\n26pU19arzyyZrGk5Sa4jIYitmpql7n6/Hnn1iDITY/TVNaWuIwEARgmFDgBCzFDA6tfbq1XT2qMb\nFxVoRm6y60gIAVfMzlFWcox++NwhZSRG6zNLJruOBAAYBRQ6AAgxz73doLLjXfrk/DzNzktxHQch\nwmOMvv+pC9TWM6h//sM+pcdH68o5ua5jAQDOE/fQAUAI2Vfbrk2HG7W4KF0LC9Ndx0GIifJ6dN/N\nF+rCgjR97bHd2lbe5DoSAOA8UegAIEQ0dvbrd2/UKD8tTlczs4JzFBft1fpbF6owM153PbJL+2rb\nXUcCAJwHCh0AhIB+/5B+vf2IvB6jmxcXyOfl7RvnLjU+Wo/ctkQpcVH6/EOvq6qp23UkAMA54hMB\nAAQ5a62efKNWjZ39unFRgVLjo11HQhjISYnVI7cvVsBKn3twu4539LmOBAA4BxQ6AAhy28qbtbe2\nXR+ZOYG95jCqpmQl6qHPL1Jz14BufWiHOvoGXUcCAJwlCh0ABLHKpm49ve+YZuYma9XULNdxEIbm\nTkrVzz67QGXHO3XHL3aqb3DIdSQAwFmg0AFAkOobHNITO48qLT5a1y3IlzHGdSSEqVVTs/Tv18/V\njqoWfe2xNzUUsK4jAQBGiEIHAEHqL/vr1dE7qE8vnKTYKK/rOAhz187L07evnqln9jfoOxv2y1pK\nHQCEAjYWB4AgVNHUpdcrW7SiJFOT0uNdx0GE+MLyItV39Onnr1QoJyVWX1ld4joSAOAMKHQAEGQG\nhwL6/Ru1Sk+I1mUzJriOgwjzT1dM1/GOfv3fZw4qKylGn144yXUkAMCHoNABQJB54UCDmrsHdPuK\nIkX7uDIe48vjMfr+py5QU1e/vvXkXmUlxmj19GzXsQAAH4BPCgAQRGpae7T5cJMWTk7TlCy2KIAb\n0T6P/vOzCzQjN0lf/vUb2n20zXUkAMAHoNABQJDwBwJ68o1aJcX6dOXsXNdxEOESY3x68POLlJkU\nrdse3qHKpm7XkQAAp0GhA4AgselQk+o7+nTtvDzFRbOqJdzLTorVI7ctkSTd8uB2He/sc5wIAHAq\nCh0ABIGGjj69dPC45uSlaEZusus4wF8VZSbowc8vUlPngG57eIe6+v2uIwEATkKhAwDHrLX6w+5a\nRXs9+tjcia7jAO8zb1KqfvrZC3XgWKe+9KtdGvAHXEcCAAwbUaEzxqw1xhw0xpQZY755mvMxxpjH\nh89vN8YUnnTuAmPMq8aY/caYvcaY2NGLDwChb19dh44092jtrBwlxrD4MILT6mnZ+tdPztHmw036\np9/tUSDAxuMAEAzO+MnBGOOVdJ+kyyXVSNphjNlgrX37pGG3S2q11pYYY26U9H1JNxhjfJJ+Jelz\n1tq3jDEZkgZH/acAgBA1OBTQX/YdU05yrBYUprmOA3yo6xdOUkNHn/792UPKTo7Rt66c4ToSAES8\nkfwpeLGkMmtthSQZYx6TdK2kkwvdtZK+M/z4t5J+Yowxkj4iaY+19i1JstY2j1JuAAgLr5Y3q7Vn\nULctz5fHGNdxgDP6yuoSNXT06+evVGhCUqxuW1H0gWMf3V49brluXlIwbq8FAMFkJJdc5kk6etLz\nmuFjpx1jrfVLapeUIWmqJGuMecYY84Yx5h9P9wLGmLuMMTuNMTsbGxvP9mcAgJDU1e/XSwePa9qE\nJJVks+ccQoMxRt+5ZpaumDVB/99Tb+upPcdcRwKAiDbWi6L4JK2Q9Jnh/37CGLPm1EHW2vuttQut\ntQuzsrLGOBIABIcXDjRocCigK2fnuI4CnBWvx+jeG+drQUGa/u6J3dp1pMV1JACIWCMpdLWSJp30\nPH/42GnHDN83lyKpWSdm8zZZa5ustT2SNkq68HxDA0Coa+jo046qFi0uSld2MmtFIfTERnl1/y0L\nNTElVnc+sktVbDwOAE6MpNDtkFRqjCkyxkRLulHShlPGbJB06/Dj6yS9aK21kp6RNMcYEz9c9C7W\ne++9A4CI9Jd99Yr2eXTp9AmuowDnLD0hWg99YbGstfrCwzvU2j3gOhIARJwzFrrhe+Lu1olydkDS\nE9ba/caY7xpjrhketl5ShjGmTNI3JH1z+GtbJf1QJ0rhbklvWGufGv0fAwBCx+GGTh1s6NTqadls\nU4CQV5SZoHW3LFRtW6/u+uVO9Q0OuY4EABFlRJ8krLUbdeJyyZOPffukx32Srv+Ar/2VTmxdAAAR\nL2CtNu47prT4KF1UnOE6DjAqFham6wfXz9Xf/uZN/cNv9+jeG+bJ42HVVgAYD/xpGADG0a6qVjV0\n9OumxQXyecd6XSpg/Hxs7kTVtPbq+395R5PS4vSPa6e7jgQAEYFCBwDjZHAooOffadDk9HjNnpjs\nOg4w6r54cbGqW7r105fLVZiR4DoOAEQE/jwMAOPk9coWdfb5dfmsCTJsIo4wZIzRd6+drRUlmfrn\nP+xTdTMrXwLAWKPQAcA4GPAH9PKhRk3JSlBxJpuII3xFeT36yc3zlZMSq19vr1Z776DrSAAQ1ih0\nADAOtlc2q7vfr8tmsE0Bwl9qfLQeuHWh+ocC+vX2IxocCriOBABhi0IHAGOs3z+kVw41qjQ7UZO5\nrwgRYuqEJH16Qb5qWnv1x921OrE9LQBgtFHoAGCMvVberJ6BIWbnEHFmTkzRmunZeqO6TdvKm13H\nAYCwRKEDgDHU2TeoTYebNG1Ckialx7uOA4y71dOzNTM3WRv3HlPZ8S7XcQAg7FDoAGAMPby1Sr2D\nzM4hcnmM0fUL8pWdHKPfvF6tlu4B15EAIKxQ6ABgjLT3Dmrd5grNyE1WXlqc6ziAMzFRXn1uaaEk\n6TevV8sfYJEUABgtFDoAGCMPbqlUR59fa6Znu44COJeeEK1PXZin2rZePbu/wXUcAAgbFDoAGAPt\nPYN6cEul1s7K0cRUZucA6cQiKUuL07WlrEkH6ztcxwGAsEChA4Ax8MCWCnX2+/X1y0tdRwGCypWz\nc5WTHKv/2lWjDjYdB4DzRqEDgFHW3juoh7dW6aNzcjU9J9l1HCCoRHk9unHxJA0OBfTEzqMKsD8d\nAJwXCh0AjLJfvXZEnf1+fXn1FNdRgKCUnRSra+ZOVEVTt14+2Og6DgCENAodAIyi3oEhPbilUpdM\ny9KsiSmu4wBB68KCNM3NT9GL7zSoqqnbdRwACFkUOgAYRY/vqFZz94C+srrEdRQgqBljdO28PKXG\nR+vxnUfVM+B3HQkAQhKFDgBGyYA/oPs3VWhRYZoWFaa7jgMEvdgor25cNEldfX79ec8x13EAICRR\n6ABglPxxd63q2vv0ZWbngBHLT4vXxdOytPtoG1sZAMA58LkOAADhYChg9Z+vlGtmbrIumZrlOg5w\nWo9ur3Yd4bQumZqlfbXt+sPuOn1tTYJio7yuIwFAyGCGDgBGwbP761XR2K0vXTJFxhjXcYCQ4vN6\n9MkL89XRO6hn9te7jgMAIYVCBwDnyVqrn75crsKMeF01J9d1HCAkFaTHa9mUDG2vbFElq14CwIhR\n6ADgPG0+3KS9te364sVT5PUwOwecq8tn5igtPkq/f7NGg0MB13EAICRQ6ADgPP305TLlJMfqExfm\nuY4ChLRon0cfn5+npq4BvfjOcddxACAkUOgA4DzsOtKq1ypadMfKIsX4WMgBOF+l2UlaUJCmzYcb\nVdfW6zoOAAQ9Ch0AnIf/fLlcafFRumlxgesoQNi4ak6u4qN9evKNGg0FrOs4ABDUKHQAcI7Kjnfq\n+QMNuuWiQiXEsAsMMFrior26Zu5E1bX36dXyJtdxACCoUegA4Byt21SpGJ9Ht1w02XUUIOzMzkvR\n1AmJevHgcXX1+13HAYCgRaEDgHNwvKNPv3+zVtcvzFdGYozrOEBYump2rgb8Ab1woMF1FAAIWhQ6\nADgHD2+r0mAgoDtWFLuOAoSt7ORYLSnK0OuVLarv6HMdBwCCEoUOAM5SV79fv3rtiNbOylFhZoLr\nOEBYWzM9W7FRXm3cc0zWskAKAJyKQgcAZ+mJHUfV0efXXauYnQPGWnyMT5dOz1ZZY5cO1ne6jgMA\nQYdCBwBnwT8U0PotlVpUmKb5BWmu4wARYWlxhjITY7Rx3zG2MQCAU1DoAOAsPLX3mGrbenXXqimu\nowARw+sxumpOjpq6BvRaRbPrOAAQVCh0ADBC1lrdv6lCU7IStGZ6tus4QESZNiFJpdmJeuGdBvWw\njQEA/BWFDgBGaFt5s/bXdejOlcXyeIzrOEBEMcboyjm56h8M6Pl3jruOAwBBg0IHACP0800VykyM\n0cfn57mOAkSknORYLS5K1+uVzWpgGwMAkEShA4AROXCsQ5sONeoLywsVG+V1HQeIWGtmTFCU16Pn\n2WwcACRR6ABgRNZtrlB8tFefWVLgOgoQ0RJjfFpekqn9dR2qa+t1HQcAnKPQAcAZ1Lf3acPuOn16\n4SSlxke7jgNEvOVTMhUb5dELzNIBAIUOAM7k4W1VClir21cUuY4CQFJctFcrSrJ0oL5TNa09ruMA\ngFMUOgD4EN39fj26/YjWzs7RpPR413EADFs2JUNxUV7upQMQ8Sh0APAhnth5VB19ft2xsth1FAAn\niY3yatXULB1q6FJ1c7frOADgDIUOAD7AUMDqwa2VWjA5TRcWpLmOA+AUS4vTlRDt1fMH2JcOQOQa\nUaEzxqw1xhw0xpQZY755mvMxxpjHh89vN8YUDh8vNMb0GmN2D//72ejGB4Cx88z+eh1t6dWdK7l3\nDghGMT6vLp6apbLGLm2vaHYdBwCcOGOhM8Z4Jd0n6UpJMyXdZIyZecqw2yW1WmtLJN0j6fsnnSu3\n1s4b/vfFUcoNAGNu3eYKTc6I1+Uzc1xHAfABFhdlKCnGpx8+d0jWWtdxAGDcjWSGbrGkMmtthbV2\nQNJjkq49Zcy1kn4x/Pi3ktYYY8zoxQSA8bXrSIverG7TbcuL5PXwdgYEq2ifRxdPy9L2yha9Ws4s\nHYDIM5JClyfp6EnPa4aPnXaMtdYvqV1SxvC5ImPMm8aYV4wxK88zLwCMi3WbKpUSF6XrF+a7jgLg\nDBYVpisnOZZZOgARaawXRTkmqcBaO1/SNyQ9aoxJPnWQMeYuY8xOY8zOxsbGMY4EAB/uSHO3nnm7\nXp9ZUqD4aJ/rOADOIMrr0VcuLdHOI63adLjJdRwAGFcjKXS1kiad9Dx/+NhpxxhjfJJSJDVba/ut\ntc2SZK3dJalc0tRTX8Bae7+1dqG1dmFWVtbZ/xQAMIoe3FIpn8fo1mWFrqMAGKEbFk5SbkqsfvpS\nmesoADCuRlLodkgqNcYUGWOiJd0oacMpYzZIunX48XWSXrTWWmNM1vCiKjLGFEsqlVQxOtEBYPS1\n9QzoiZ01umZuniYkx7qOA2CEon0e3bGyWNsrW7TrSKvrOAAwbs5Y6Ibvibtb0jOSDkh6wlq73xjz\nXWPMNcPD1kvKMMaU6cSlle9zd3CeAAAgAElEQVRubbBK0h5jzG6dWCzli9baltH+IQBgtPx6e7V6\nB4d0B1sVACHnxkWTlBofpf98udx1FAAYNyO6OcRau1HSxlOOffukx32Srj/N1/1O0u/OMyMAjIsB\nf0C/2FallaWZmpH7vtt9AQS5hBifbr2oUPe+cFiHGjo1dUKS60gAMObGelEUAAgZG96q0/HOft2x\nsth1FADn6PPLChUX5dXPXmGWDkBkoNABgCRrrR7YXKFpE5K0qjTTdRwA5ygtIVo3LS7Qht11qmnt\ncR0HAMYchQ4AJG0pa9I79Z26fWWRjGEjcSCUvXsP7AObKx0nAYCxR6EDAEnrNlcqKylG186b6DoK\ngPM0MTVOH5+fp8d2VKule8B1HAAYUxQ6ABHvYH2nNh1q1K0XTVaMz+s6DoBR8MWLi9U3GNDDW5ml\nAxDeKHQAIt4DmysUG+XRZ5ZMdh0FwCgpyU7SR2ZO0C9ePaKufr/rOAAwZih0ACLa8c4+/XF3na5f\nMElpCdGu4wAYRV+6ZIraewf12OvVrqMAwJih0AGIaI9sO6LBQEC3r2AjcSDczC9I00XFGVq3uUL9\n/iHXcQBgTFDoAESsngG/frX9iC6fMUGFmQmu4wAYA1+6ZIoaOvr1xzfrXEcBgDFBoQMQsX63q0Zt\nPYO6cxUbiQPhamVppmbkJmvd5gpZa13HAYBRR6EDEJGGAlbrt1Rq7qRULZyc5joOgDFijNGdK4t0\n+HiXXj7U6DoOAIw6Ch2AiPT8gQZVNffoTjYSB8Le1RdM1ITkGD2wucJ1FAAYdRQ6ABHpgc0VykuN\n09pZOa6jABhj0T6PvrC8SFvLmrW/rt11HAAYVRQ6ABFn99E27ahq1W0riuTz8jYIRIKbFhcoIdqr\nBzaz0TiA8MInGQARZ93mCiXF+nTDokmuowAYJylxUbphUYH+9FadjrX3uo4DAKOGQgcgohxt6dHT\ne4/p5sUFSozxuY4DYBx9YXmhAtbq4a1VrqMAwKih0AGIKA9trZLHGH1+eaHrKADG2aT0eF05J1eP\nbq9WZ9+g6zgAMCoodAAiRnvvoB7fUa2rL8hVbkqc6zgAHLhrZbE6+/16fMdR11EAYFRQ6ABEjMde\nr1b3wJDuWMlG4kCkmjspVYsL0/XQ1ir5hwKu4wDAeaPQAYgIA/6AHtpapWVTMjQ7L8V1HAAO3bGy\nSLVtvXp6X73rKABw3ih0ACLCU3vrVN/RpzuZnQMi3mUzJqgoM0HrNlfIWus6DgCcFwodgLBnrdW6\nTZUqyU7UxVOzXMcB4JjHY3T7iiLtqWnX65UtruMAwHmh0AEIe6+WN+vtYx26Y0WRPB7jOg6AIPCp\nC/OVnhCtdWw0DiDEUegAhL11myuUmRitj8/Pcx0FQJCIi/bqs0sn6/kDDSpv7HIdBwDOGYUOQFgr\nO96plw426nNLCxUb5XUdB0AQueWiyYr2ebR+C7N0AEKXz3UAABhLD2yuVIzPo88uLXjP8Ue3VztK\nBCBYZCbG6FMX5ul3u2r03y6fqozEGNeRAOCsMUMHIGw1dvbryTdr9akF+XxQA3Bat68oVr8/oF++\ndsR1FAA4JxQ6AGHrl68d0YA/oNtXFLmOAiBIlWQnas30bP3y1SPqGxxyHQcAzhqFDkBY6hsc0q9e\nO6LLZmRrSlai6zgAgtgdK4vV3D2gJ9+odR0FAM4ahQ5AWPrdGzVq6R7QHWwkDuAMlhana3Zesh7Y\nUqFAgI3GAYQWFkUBEBRGc5GSgLX60fOHlJcap/LjXapo7B617w0gOJ3ve8is3BQ9vvOo/mXDfs3I\nTf7QsTcvKfjQ8wAwnpihAxB2DtZ3qqlrQCtKMmUMG4kDOLPZeSlKiYvSlrIm11EA4KxQ6ACEnS1l\nTUqJi9LsvBTXUQCECK/HaNmUDFU2daumtcd1HAAYMQodgLBS29qryqZuLZuSIa+H2TkAI7eoMF0x\nPg+zdABCCoUOQFjZXNaoGJ9HiwrTXUcBEGJio7xaVJiufbXtausZcB0HAEaEQgcgbLT1DGhfbbsW\nFaYrNsrrOg6AELRsSoYkaVt5s+MkADAyFDoAYePdD2DvfiADgLOVGh+tOXkp2lHVwkbjAEIChQ5A\nWOgbHNKOqhbNzktRany06zgAQtiK0iz1+wPaUdXiOgoAnBGFDkBY2FnVon5/QCtKMl1HARDi8lLj\nVJyZoG3lzRpio3EAQY5CByDkDQWstpU3qzAjQflp8a7jAAgDK0oz1d47qL21ba6jAMCHotABCHn7\n6trV1juolaXMzgEYHVMnJCkrMUZbDjfJWmbpAAQvCh2AkGat1ZbDTcpMjNa0nCTXcQCECY8xWlGa\nqbr2PlU0dbuOAwAfiEIHIKRVNfeotq1Xy0sy5TFsJA5g9MyblKqEGJ+2HGajcQDBi0IHIKRtOdyo\n+Giv5k9Kcx0FQJiJ8np0UXG6DjZ0qqGjz3UcADgtCh2AkNXU2a936ju1pChD0T7ezgCMviVFGYry\nGm0tY5YOQHAa0ScgY8xaY8xBY0yZMeabpzkfY4x5fPj8dmNM4SnnC4wxXcaYvx+d2AAgbSlvktdj\ntLQ43XUUAGEqIcan+QVpevNomzr7Bl3HAYD3OWOhM8Z4Jd0n6UpJMyXdZIyZecqw2yW1WmtLJN0j\n6funnP+hpKfPPy4AnNDd79cbR1o1b1KqkmKjXMcBEMZWTMlUIGD1WkWz6ygA8D4jmaFbLKnMWlth\nrR2Q9Jika08Zc62kXww//q2kNcacWJ3AGPNxSZWS9o9OZACQtlc2yx+wWs5G4gDGWGZSjKbnJmt7\nZYsG/AHXcQDgPUZS6PIkHT3pec3wsdOOsdb6JbVLyjDGJEr6J0n/88NewBhzlzFmpzFmZ2Nj40iz\nA4hQg0MBvVrRoqkTEjUhOdZ1HAARYEVJpnoGhrTrSIvrKADwHmO9isB3JN1jre36sEHW2vuttQut\ntQuzsrLGOBKAUPfW0TZ19/u1ooT3CwDjozAjXpPT47XpcBOzdACCykgKXa2kSSc9zx8+dtoxxhif\npBRJzZKWSPo3Y0yVpK9L+u/GmLvPMzOACGat1ZayJuWmxGpKVoLrOAAihDFGq6dnq713UL9/s8Z1\nHAD4q5EUuh2SSo0xRcaYaEk3StpwypgNkm4dfnydpBftCSuttYXW2kJJP5L0v621Pxml7AAi0KGG\nLh3v7NeKkkwZNhIHMI5KsxOVlxqnn75cLv8Qs3QAgsMZC93wPXF3S3pG0gFJT1hr9xtjvmuMuWZ4\n2HqduGeuTNI3JL1vawMAGA1byhqVHOvTnPwU11EARBhjjFZPy9KR5h79ec8x13EAQJLkG8kga+1G\nSRtPOfbtkx73Sbr+DN/jO+eQDwD+qq6tV+WN3bpiVo58HjYSBzD+pucma9qEJP3kpTJdM3eiPB6u\nFADgFp+IAISMrWVNivZ6tLiQjcQBuOExRl+5tERlx7v0zP5613EAgEIHIDS09w7qrZo2LShMU1y0\n13UcABHso3NyVZSZoB+/WCZrres4ACIchQ5ASHi1vFnWSsunsJE4ALe8HqMvXzJFbx/r0EsHj7uO\nAyDCUegABL1+/5Ber2rWrInJSk+Idh0HAPTx+XnKT4vTf7zALB0Atyh0AILeriOt6hsMaGUpG4kD\nCA5RXo++dMkU7T7apm3lza7jAIhgFDoAQW0oYLW1rEmT0+M1KT3edRwA+KvrFuRrQnKMfvziYddR\nAEQwCh2AoPb2sQ619gxqRSn3zgEILjE+r/5m1RS9VtGiHVUtruMAiFAUOgBBy1qrLYcblZ4QrRm5\nya7jAMD73LS4QJmJMfr3Zw5yLx0AJyh0AIJWdUuPjrb2anlJpjyGzXsBBJ+4aK/uXj1F2ytbtKWs\nyXUcABGIQgcgaG0+3KS4KK8WFKS5jgIAH+imJQXKS41jlg6AExQ6AEGpuatfB451aElRuqJ9vFUB\nCF4xPq++tqZUb9W067m3G1zHARBh+JQEIChtLW+Sx2O0dEqG6ygAcEafvDBPxZkJ+sGzhzQUYJYO\nwPih0AEIOj0Dfu060qq5+alKjo1yHQcAzsjn9ejrl0/VwYZO/XlPnes4ACIIhQ5A0Hm9skWDQ1Yr\nStiqAEDouHpOrqbnJOme5w5pcCjgOg6ACEGhAxBU/EMBvVrerNLsROWkxLqOAwAj5vEY/f1Hpqmq\nuUe/21XjOg6ACEGhAxBU3qppV2e/n9k5ACFpzYxszZuUqntfOKy+wSHXcQBEAAodgKBhrdXWsibl\nJMeqJDvRdRwAOGvGGP3DFdN0rL1Pj26vdh0HQASg0AEIGmXHu1Tf0aflJZkybCQOIEQtL8nURcUZ\n+unLZeoZ8LuOAyDMUegABI0tZU1KivFpbn6K6ygAcF7+/oppauoa0LpNla6jAAhzFDoAQaG+vU+H\nj3fpoikZ8nl5awIQ2hZMTtPaWTn6+aZyHe/ocx0HQBjjUxOAoLClrElRXqPFRemuowDAqPjmldM1\nOBTQD5495DoKgDBGoQPg3PGOPr11tE0LJqcpPtrnOg4AjIrCzATdclGhnth1VAeOdbiOAyBMUegA\nOPeLV6sUsFbLp7BVAYDw8tVLS5USF6XvPXVA1lrXcQCEIQodAKd6Bvz61WvVmpGbrIzEGNdxAGBU\npcRH6auXlmpLWZNePtjoOg6AMEShA+DUb3fVqL13UCtLmZ0DEJ4+u3SyCjPi9b2NB+QfCriOAyDM\nUOgAODMUsFq/pVLzJqWqID3edRwAGBPRPo++ddUMlR3v0mM7jrqOAyDMUOgAOPPc2w060tyjO1cW\ns5E4gLD2kZkTtLgoXfc8d0idfYOu4wAIIxQ6AM48sLlC+WlxumLWBNdRAGBMGWP0zx+doebuAf30\n5XLXcQCEEQodACferG7VziOtum15ERuJA4gIF+Sn6pPz87R+S6VqWntcxwEQJvgUBcCJBzZXKinW\np08vmuQ6CgCMm7+/Ypo8RvrfGw+4jgIgTFDoAIy7oy09enrfMd28pECJMWwkDiByTEyN01cuKdHG\nvfXaWtbkOg6AMEChAzDuHtxaKY8x+vyyQtdRAGDc3bmqWAXp8fqXDfs1yDYGAM4TfxoHMK7aewf1\nxI6j+tjcicpNiXMdBwDO2qPbq8/7e1w8NUu/fO2IvvbYbq0o+eB9OG9eUnDerwUgvDFDB2Bc/eb1\nanUPDOmOlUWuowCAM9NzkjR1QqJeONDANgYAzguFDsC4GfAH9PDWKi2bkqFZE1NcxwEAZ4wxunrO\nRPmHrJ7ZX+86DoAQRqEDMG6e2lun+o4+3bmy2HUUAHAuMylGy0sy9UZ1m6qbu13HARCiKHQAxoW1\nVus2VaokO1EXT81yHQcAgsLq6VlKjvVpw546Bax1HQdACKLQARgXr5Y36+1jHbpjRZE8HuM6DgAE\nhRifV1fOzlVdW592VrW6jgMgBFHoAIyLdZsrlJkYrY/Pz3MdBQCCygX5KSrMSNCzb9erZ8DvOg6A\nEEOhAzDmDjd06qWDjbrlokLFRnldxwGAoGKM0cfm5qpvcEjPvt3gOg6AEEOhAzDmHthcqdgojz67\ndLLrKAAQlHJT4rS0OEM7KltU09rjOg6AEEKhAzCmjnf26fdv1uq6BflKT4h2HQcAgtZlMyYoMcan\nP+5mgRQAI0ehAzCmHtl2RIOBgG5fwVYFAPBhYqO8unJOrmrberWjqsV1HAAhgkIHYMz0DPj1q+1H\n9JGZE1SUmeA6DgAEvbn5KSrOTNCz+xvU1c8CKQDObESFzhiz1hhz0BhTZoz55mnOxxhjHh8+v90Y\nUzh8fLExZvfwv7eMMZ8Y3fgAgtlvd9WorWeQjcQBYIROLJAyUf3+IT2zr951HAAh4IyFzhjjlXSf\npCslzZR0kzFm5inDbpfUaq0tkXSPpO8PH98naaG1dp6ktZJ+bozxjVZ4AMFrKGD1wOZKzS9I1YLJ\naa7jAEDImJAcqxUlWdpV3aqdXHoJ4AxGUq4WSyqz1lZIkjHmMUnXSnr7pDHXSvrO8OPfSvqJMcZY\na09epilWEnf4AhHiubfrVd3So29dOV3GsJE4AJyNS6dn662aNt396Jv6yuoSeT3j8z5685KCcXkd\nAKNnJJdc5kk6etLzmuFjpx1jrfVLapeUIUnGmCXGmP2S9kr64vD59zDG3GWM2WmM2dnY2Hj2PwWA\noHP/pgoVpMfrI7NyXEcBgJAT7fPoo3NyVd/Rp9cqml3HARDExnxRFGvtdmvtLEmLJH3LGBN7mjH3\nW2sXWmsXZmVljXUkAGNs15EWvVHdpttXFI3bX5UBINzMmpisqRMS9fyBBnX0DrqOAyBIjaTQ1Uqa\ndNLz/OFjpx0zfI9ciqT3/DnJWntAUpek2ecaFkBoWLepUilxUbp+Yb7rKAAQsowx+tgFEzUUsNq4\n75jrOACC1EgK3Q5JpcaYImNMtKQbJW04ZcwGSbcOP75O0ovWWjv8NT5JMsZMljRdUtWoJAcQlKqa\nuvXM2/X67NICxUezBhIAnI+MxBitmpqlPTXtKm/sch0HQBA6Y6EbvuftbknPSDog6Qlr7X5jzHeN\nMdcMD1svKcMYUybpG5Le3dpghaS3jDG7Jf1e0pettU2j/UMACB7rt1QqyuPRrRcVuo4CAGHh4qlZ\nSouP0obddfIHAq7jAAgyI/rzubV2o6SNpxz79kmP+yRdf5qv+6WkX55nRgCOPLq9+qzG9/T79diO\nal2Qn6rnDxwfo1QAEFmivB59bO5EPfLqEW0ta9bFU1lvAMD/M+aLogCIHK9VtmhwyGpFSabrKAAQ\nVqbnJGtmbrJefKdBbT0DruMACCIUOgCjYnAooFcrmjV1QqImJL9vMVsAwHn66AW5kqSn9rJACoD/\nh0IHYFS8dbRN3f1+rSzlUiAAGAtp8dFaPS1b++s6dKih03UcAEGCQgfgvAWs1eayJuWmxKo4M8F1\nHAAIWytKM5WZGKMNb9VpcIgFUgBQ6ACMgkMNnWrs7NfK0kwZw0biADBWfB6Prpk7US3dA9p0uNF1\nHABBgEIH4LxtOdyklLgozclLdR0FAMJeSXai5uSl6JWDjWrpZoEUINJR6ACcl9q2XlU0dWvZlAx5\nPczOAcB4+OicXHk8Rk/tqXMdBYBjFDoA52XL4UbF+DxaVJjuOgoARIzkuChdOi1bB+o7WSAFiHAU\nOgDnrK1nQHtr27WoMF2xUV7XcQAgoiybkqGMhGj9eU+d/AEWSAEiFYUOwDnbVt4s6cSHCgDA+PJ5\nPbr6golq6hrQtrJm13EAOEKhA3BO+gaHtKOqRXPyUpQaH+06DgBEpGk5SZqek6QXDx5XR++g6zgA\nHKDQATgnO6pa1O8PaAUbiQOAUx+dk6uhgNUz++tdRwHgAIUOwFnzBwLaWtak4swE5aXGuY4DABEt\nIzFGK0sy9ebRNh1p7nYdB8A4o9ABOGtvHW1TR59fq6YyOwcAweDiaVlKjvXpT3vqFLDWdRwA44hC\nB+CsBKzVpkNNyk2JVWl2ous4AABJMT6vrpyTq7q2Pu2sanUdB8A4otABOCvvHOtQY1e/VpVmyRg2\nEgeAYHFBXooKMxL07Nv16hnwu44DYJxQ6ACMmLVWrxxqVFp8lGbnpbiOAwA4iTFGH5ubq96BIb34\nznHXcQCMEwodgBGrau7R0dZerSjNktfD7BwABJvclDgtLEzXaxXNauzsdx0HwDig0AEYsU2HGhUf\n7dWCgjTXUQAAH+CyGdmK8nr09L5jrqMAGAcUOgAjUt/ep4MNnVo2JVPRPt46ACBYJcVG6ZJp2Xqn\nvlNlx7tcxwEwxvhUBmBENh1uVLTXo6XF6a6jAADOYNmUDKXFR2nj3mNsYwCEOQodgDNq7RnQnpo2\nLSpMU3y0z3UcAMAZRHk9Wjs7V/UdbGMAhDsKHYAz2lLWJElaXpLpOAkAYKRmT0zW5Ix4PXegQX2D\nQ67jABgjFDoAH6qn36+dVS2aNylVqfHRruMAAEbIGKOPzslVd79fLx9sdB0HwBih0AH4UK9WNGtw\nyGplaZbrKACAs5SfFq/5k1K1tbxJLd0DruMAGAMUOgAfqH9wSNvKmzUjJ0kTkmNdxwEAnIOPzMqR\nx0h/YRsDICxR6AB8oO2VLeodHNIl07JdRwEAnKOUuCitKs3SvroOVTV1u44DYJRR6ACcVt/gkDaX\nNak0O1GT0uNdxwEAnIeVpVlKjvXp6X3HZNnGAAgrFDoAp/XY69Xq7vczOwcAYSDa59GaGRN0tLVX\n++s6XMcBMIoodADep98/pJ9vqlBhRryKMhNcxwEAjIILC9KUlRSjZ/bXayjALB0QLih0AN7nyTdq\nday9T6uZnQOAsOH1GK2dlaPm7gHtqGpxHQfAKKHQAXgP/1BAP325THPzU1SSneg6DgBgFE3PSVJh\nRrxefOe4+v1sNg6EAwodgPfY8Fadjrb06u5LS2WMcR0HADCKjDFaOztXXf1+bTnc5DoOgFFAoQPw\nV4GA1X0vlWl6TpLWTOdySwAIRwXp8Zo1MVmbDzeps2/QdRwA54lCB+Cv/rK/XuWN3frK6hJ5PMzO\nAUC4umJmjvyBgF5857jrKADOE4UOgCTJWqsfv1im4swEXTUn13UcAMAYykyK0aLCdO2oalFjZ7/r\nOADOA4UOgCTpxXeO68CxDn15dYm8zM4BQNi7dHq2fF6Pnn273nUUAOeBQgdA1lrd+8Jh5afF6dp5\nE13HAQCMg6TYKK0szdT+ug5VN3e7jgPgHFHoAOj5A8e1p6ZdX720VFFe3hYAIFKsKMlUYoxPT++v\nl7VsNg6EIj65AREuELD6wbMHVZgRr09emOc6DgBgHMX4vFozI1tHmnt04Fin6zgAzgGFDohwT++r\n1zv1nfraZaXyMTsHABFn4eR0ZSbG6Jn99fIPBVzHAXCW+PQGRLChgNU9zx9SSXairpnL7BwARCKv\nx+iKWRPU2NWv/9pV4zoOgLNEoQMi2J/eqlPZ8S793WVTWdkSACLYzNxkFaTH657nDqlnwO86DoCz\nQKEDIpR/KKB7Xzis6TlJunJ2jus4AACHjDG6cnaOjnf2a/3mStdxAJyFERU6Y8xaY8zB/7+9O4+P\nqr7XOP75TvaN7GxJSNiRTZaIAoq1bkhVrAuurXrb2vbqtdVaa72tbbW3rba2tbWbW6/WvYoWFYtW\nsSpCWGXfwhIgLNlIyL7N7/4xozciSIDAmZk879crr5lz5iR58HBwnjnn/H5mVmxmdxzg9Tgzey74\nepGZFQTXn21mS8xsZfDx810bX0SO1MxlpWypqOfWs4fg09k5EZFuLz8ziXOG9+Iv726msk6TjYuE\ni0MWOjOLAv4AnAcMB640s+H7bfYVYK9zbhDwG+De4PoK4ALn3CjgWuBvXRVcRI5cS5ufB/61kdG5\nqZw9vJfXcUREJETcPnUYja3t/P7tYq+jiEgndeYM3QSg2Dm32TnXAjwLTN9vm+nA48HnLwBnmpk5\n55Y553YG168GEswsriuCi8iRe37xdkqrG7nl7CGY6eyciIgEDOqZzIzCPJ5cUMLWCk02LhIOOlPo\ncoDtHZZ3BNcdcBvnXBtQA2Tut80lwFLn3KfO4ZvZDWa22MwWl5eXdza7iByBptZ2Hny7mHH90vjc\nkGyv44iISIi55azBxET5+OUb672OIiKdcFwGRTGzEQQuw/z6gV53zj3knCt0zhVmZ+sNpsix9MzC\nbeze18Rt5wzV2TkREfmUnj3i+dpp/XltxS4+3F7tdRwROYTOFLpSIK/Dcm5w3QG3MbNoIBWoDC7n\nAi8BX3bObTrawCJy5PY1tfL7t4uZOCCTSYOyvI4jIiIh6obTB5KZFMsvXl+Lc87rOCLyGTpT6BYB\ng82sv5nFAlcAs/bbZhaBQU8ALgXeds45M0sDXgPucM7N66rQInJk/vzOJqrqW7hz2gleRxERkRCW\nHBfNzWcOZsHmKt7ZoNthRELZIQtd8J64m4A5wFrgeefcajO728wuDG72KJBpZsXArcBHUxvcBAwC\n7jKzD4NfPbv8TyEih7SzupFH39/CRWP6Mio31es4IiIS4q6c0I9+GYnc+/o62v06SycSqqI7s5Fz\nbjYwe791d3V43gRcdoDv+ynw06PMKCJd4FdvrMcBt5071OsoIiISBmKjfdx27lBufmYZ//iwlIvH\n5XodSUQO4LgMiiIi3lpVWsNLy0q5fnIBuemJXscREZEwcf6oPozKSeX+NzbQ1NrudRwROQAVOpEI\n55zj56+vJS0hhv/83CCv44iISBjx+Yw7zhtGaXUjTy4o8TqOiByACp1IhHtnQznziiu5+czBpCbE\neB1HRETCzORBWZw2OIsH5xZT09jqdRwR2Y8KnUgEa2v38/PZaynITOTqk/O9jiMiImHqe1OHUd3Q\nyl/+rRmoREKNCp1IBHthyQ427Knje1OHERutw11ERI7MyJxUpo/py2PztrC7psnrOCLSgd7hiUSo\n+uY27n9zA+Pz05k6srfXcUREJMzdds5Q2v2OB97a4HUUEelAhU4kQj307mbKa5u5c9oJmJnXcURE\nJMzlZSRyzSn5PLdoO8VltV7HEZEgFTqRCFRSWc+f/r2J80f3YXx+utdxREQkQvzX5weTFBvNL15f\n53UUEQlSoROJMM45fjxrNTE+4wdfGO51HBERiSAZSbF884yB/GttGfM3VXodR0RQoROJOG+s2cPc\n9eXccvYQeqfGex1HREQizH9M7k/f1Hh+Nnstfr/zOo5It6dCJxJBGlrauPuVNQzrncJ1kwq8jiMi\nIhEoPiaK284dysrSGl5ZsdPrOCLdngqdSAR58O1iSqsbueeikURH6fAWEZFj46IxOYzo24P7/rme\nptZ2r+OIdGt6xycSIYrL6nj4vc1cOj6XkwoyvI4jIiIRzOcz7px2AqXVjTz+wVav44h0ayp0IhHA\nOcdd/1hFQkwUd5w3zOs4IiLSDUwelMUZQ7N5cG4xe+tbvI4j0m2p0IlEgFdW7OKDTZV8d+owspLj\nvI4jIiLdxPennUB9c9lPATYAABwtSURBVBu/e3uj11FEui0VOpEwV9vUyk9fXcPo3FSumtDP6zgi\nItKNDOmVwuUn5fHkghK2VtR7HUekW1KhEwlz97+xgfK6Zu6ZPpIon3kdR0REuplbzhpCTJSP++Zo\nsnERL6jQiYSxBZsr+d8PtnLtxAJOzEvzOo6IiHRDPXvE87XTBjB75W6WlFR5HUek21GhEwlT9c1t\nfPeF5RRkJnL71KFexxERkW7shikD6NUjjp+8skaTjYscZyp0ImHqZ7PXsmNvI7+67EQSY6O9jiMi\nIt1YUlw035s6jBU7api5rNTrOCLdigqdSBh6b2M5TxVt46un9qdQc86JiEgIuGhMDmPy0rj3n+uo\na27zOo5It6FCJxJm9jW1cvsLKxiYncR3ztGlliIiEhp8PuNHFwynvLaZP84t9jqOSLeh67REwsz1\njy1id00T3zh9IDOX6rIWEREJHWP7pXPx2BweeW8LV5zUj36ZiV5HEol4OkMnEkbeWruHJdv2MmVI\nNnkZ+p+kiIiEntunDiPKZ/xs9lqvo4h0Cyp0ImGiuqGFO2aupHePeM4c1tPrOCIiIgfUOzWeG88Y\nyD9X7+aDTRVexxGJeCp0ImHAOcedL61kb30Ll47PJTpKh66IiISur542gNz0BO5+ZQ1t7X6v44hE\nNL0rFAkDT8wvYfbK3XznnKH0TUvwOo6IiMhnio+J4s5pJ7Budy3PLtrudRyRiKZCJxLilm+v5qev\nreHMYT35+pQBXscRERHplPNG9ubk/hnc/8Z6ahpavY4jErFU6ERCWE1DK//51FJ6psRz/4wT8fnM\n60giIiKdYmb86IIR1DS28qs31nsdRyRiqdCJhCjnHN/5+4eU1Tbx4FVjSUuM9TqSiIjIYRnetwdf\nnljAk0UlLN9e7XUckYikQicSoh56dzP/WlvGndNOYGy/dK/jiIiIHJHvnDOE7OQ4fvDyKtr9zus4\nIhFHhU4kBC3aWsV9c9Zz3sjeXDepwOs4IiIiRywlPoYfnj+claU1PLmgxOs4IhFHhU4kxFTWNXPT\n00vJS0/g3ktHY6b75kREJLydP7oPpw3O4ldz1lNW2+R1HJGIokInEkJa2vzc+PRS9ja08oerx9Ej\nPsbrSCIiIkfNzPjJhSNobvPzP6+t9TqOSESJ9jqAiAQ45/j+zJUs2FzFby8fw4i+qV5HEhGRbubp\nom3H9OefOjiLf3y4k8ykOO66YPgx/V0i3YXO0ImEiD++s4kXl+7gW2cO5qKxOV7HERER6XKnD8km\nIymWWctLaW5r9zqOSERQoRMJAa8s38kv56znojF9+fZZg72OIyIickzERPm48MS+VNS18NC/N3sd\nRyQi6JJLkS5wNJeobKus55H3t5Cfmci4fuk8s3B7FyYTEREJLUN6pTAyJ5UH5xZz4Zi+5GcmeR1J\nJKzpDJ2Ih6rqW/jbghJ6JMRwzcn5REfpkBQRkcj3hVF9iI3ycfsLK/BrbjqRo6J3jyIeaWxp5/H5\nW/E7uHZiAUlxOmEuIiLdQ2pCDD84/wSKtlTxVJHmphM5Gip0Ih5oafPztwUlVNY1c/XJ/chOifM6\nkoiIyHE1ozCP0wZn8fPX17G9qsHrOCJhS4VO5Dhra/fzVFEJJZX1XFaYx4DsZK8jiYiIHHdmxi8u\nGY3PjDtmrsA5XXopciRU6ESOo3a/45mF29hYVsfF43I4MTfN60giIiKeyUlL4PvThjGvuFKDgokc\nIRU6kePE7xzPL97O2t21XHBiX8bnZ3gdSURExHNXTejHpIGZ/Gz2WkqrG72OIxJ2OlXozGyqma03\ns2Izu+MAr8eZ2XPB14vMrCC4PtPM5ppZnZk92LXRRcKH3zlmLi1lZWkNU0f0ZuKATK8jiYiIhAQz\n495LRuN3jjte1KWXIofrkIXOzKKAPwDnAcOBK81s+H6bfQXY65wbBPwGuDe4vgn4IXBblyUWCTPO\nOV5ZvpOl2/by+WE9mTIk2+tIIiIiISUvI5HvTR3Gexsr+PviHV7HEQkrnTlDNwEods5tds61AM8C\n0/fbZjrwePD5C8CZZmbOuXrn3PsEip1It+OcY/bKXRRtqeK0wVmcOayn15FERERC0pdOyWdC/wzu\neW0NO3XppUindabQ5QAd71LdEVx3wG2cc21ADaBryqRb8zvHzGWlzNtUyaSBmUwd0Rsz8zqWiIhI\nSPL5jF9eOpp2v+OW5z6kXROOi3RKSAyKYmY3mNliM1tcXl7udRyRo9bW7ueZhdtYUhK4zPILo/qo\nzImIiBxCfmYSd08fSdGWKv44t9jrOCJhoTOFrhTI67CcG1x3wG3MLBpIBSo7G8I595BzrtA5V5id\nrfuLJLx9NGn46p37mDaqD2ed0EtlTkREpJMuGZfDhSf25bdvbWRJyV6v44iEvM4UukXAYDPrb2ax\nwBXArP22mQVcG3x+KfC20xBF0g01trTz2LwtFJfVcfHYHE4dlOV1JBERkbBiZvz0iyPpkxrPzc8s\no6ax1etIIiHtkIUueE/cTcAcYC3wvHNutZndbWYXBjd7FMg0s2LgVuDjqQ3MbCvwa+A6M9txgBEy\nRSJCXXMbj7y/mdK9jVw5oR+FBZpnTkRE5Ej0iI/hd1eOZfe+Jv77pZWaykDkM0R3ZiPn3Gxg9n7r\n7urwvAm47CDfW3AU+UTCQtm+Jh6fv5W65ja+NDGfIb1SvI4kIiIS1sb1S+fWs4fwyznrmTIkmxmF\neYf+JpFuKCQGRREJZ/OKK/jzu5tobXd89dQBKnMiIiJd5BunD2TigEx+PGs1m8rrvI4jEpJU6ESO\nwjMLt3HtYwtJTYjhm58bSF5GoteRREREIkaUz/jN5WOIi/Zx8zPLaG5r9zqSSMhRoRM5An6/4+ez\n1/L9mSuZPCiLr08ZSHpirNexREREIk7v1Hjuu/REVu/cx49nrfY6jkjIUaETOUyNLe1886kl/OXd\nzXzplHwevbaQ+Jgor2OJiIhErLOH9+LGMwbyzMLtPFVU4nUckZDSqUFRRCSgpLKebzy5lPW79/Gj\nC4Zz3aQCzTEnIiJyHNx69lDWBM/SDemVwkkaTVoE0Bk6kU6bu66MC37/PjurG3n0upO4fnJ/lTkR\nEZHjJMpn/PaKseSmJ/LNJ5eyq6bR60giIUGFTuQQ/H7Hb97cwH88vojc9EReuelUzhja0+tYIiIi\n3U5qQgwPf3k8Ta3tfONvS2hq1SApIrrkUuQz1DS08u3nljF3fTmXjMvlf744UvfLiYiIdIGni7Yd\n8fdeNCaHJ4tKuOrhBVwyLveQV8xcdXK/I/5dIqFOhU7kIFbvrPn4ko57LhrJNSf30yWWIiIiIWB4\n3x58flhP3l5XRt+0BCYNzPI6kohnVOhE9uOc438/2MrPZ68jIymWZ2+YyPj8dK9jiYiISAefH9aT\nXdWNzF65i6zkOIb0SvE6kogndA+dSAdV9S187YnF/OSVNZw2OIvZ3zpNZU5ERCQE+cy4rDCPXj3i\nebpoG9urGryOJOIJFTqRoPmbKpn2wHu8u6GCu84fziPXFpKRpMnCRUREQlV8TBTXTSogOT6ax+dv\npby22etIIsedCp10e23tfn79xnquemQBCbFRzPzPSfzHqZqSQEREJBykxMdwfXBe2L/O20JNY6vX\nkUSOKxU66da2VtQz4y/z+d3bxVw8NpdX/+tURuakeh1LREREDkNmchzXTSqgsbWd//1gC40tms5A\nug8VOumWnHM8uaCE8x54j+KyOh64Ygz3zziRpDiNEyQiIhKOctISuOaUfCrqWnhi/lZa2vxeRxI5\nLlTopNvZXdPEtX9dxA9eXkVhQTpzbpnC9DE5XscSERGRozQwO5kZhXlsq2rg2UXbaPc7ryOJHHM6\nHSHdyqzlO/nhy6tobmvnnukjuOaUfN0rJyIiEkFG5aRSf2JfZi3fyTMLt3HFSXleRxI5plTopFso\nr23mR7NWMXvlbsb2S+PXM8bQPyvJ61giIiJyDJwyIBO/c7y6Yhd/W1DCZYV5JMRGeR1L5JhQoZOI\n5pzjpWWl3P3qGhqa2/nuuUP5+pQBREfpamMREZFINmlgFrFRPl5aVsq1f13IY9edRLLulZcIpL/V\nErF2Vjdy50sreWd9OePz07n3ktEM6pnsdSwRERE5TgoLMoiJ8vHC0h1c/UgRT1w/gdTEGK9jiXQp\nnaaQiOP3B0awPOc371K0uYofXTCc578+UWVORESkGzoxL40/XT2OtTv3ccXDC6io0+TjEllU6CSi\nbNhTyxUPLeAHL6/ixLxU3rhlCtdP7k+UTwOfiIiIdFfnjOjNw9cWsqWijsv/Mp/tVQ1eRxLpMip0\nEhEaW9q575/rmPbAe2woq+W+S0bz5FdOJi8j0etoIiIiEgJOH5LN49dPoKy2mQsffJ/5myq9jiTS\nJVToJOzNXV/GOb/9N398ZxPTx+Tw1q2nM+OkPE1HICIiIp9w8oBM/nHjZDKSYrnm0SKemL8V5zRX\nnYQ3DYoix83TRdu69OfVNLby2oqdrNq5j+zkOL56an8GZCczZ/WeLv09IiIiEjkGZCfz8o2T+faz\nH3LXP1azunQfd180grhoTWsg4UmFTsJOW7uf94sreGd9OX7nOOuEXkwZnKWpCERERKRTUuJjePjL\nhfz6zQ08OLeYjWW1/PlL4+mZEu91NJHDpkInYcM5x/rdtby6chdV9S0M79ODaaP6kJEU63U0ERER\nCTM+n3HbuUM5oU8Pbvv7ci78/Tzun3EikwdleR1N5LCo0ElYKK9t5rWVO9mwp47s5Diun1TA4F4p\nXscSERGRMPeF0X3on5XETU8v5epHivjSKfnccd4wkjQJuYQJ/U2VkNbQ3Mbc9WUs2FxFdJQxbVQf\nJg7I1DQEIiIi0mWG9+3Bazefxq/eWM9j87bw7w3l/PLS0Zw8INPraCKHpEInIam13c/8TZW8s6GM\n5lY/4/PTOXt4L1LiY7yOJiIiIhEoITaKH54/nHNH9Oa7Lyzn8ocWcP3kAm4/dxgJsRowRUKXCp2E\nFL9zLN9ezZtr9lDd2MrQXimcO7I3vXvoJmURERE59ib0z+D1b53Gva+v46/ztjJ3XRn//YXhnHVC\nT02JJCFJhU5CgnOODXvqeGPNbnbVNJGTlsAl43MZmJ3sdTQRERHpZhJjo/nJ9JGcO7I3P3hpFV97\nYjETCjK4Y9owxvVL9zqeyCeo0ImnPipyb63bw469jaQnxnB5YR6jclPx6VMwERER8dCkgVnMuWUK\nzy3azm//tZGL//gBU0f05vapQxmgD50lRKjQiSecc2wsq+OttXvYvreRtMQYvjg2h7H90oj2aT45\nERERCQ0xUT6uOSWfL47N4ZH3tvCXdzfx5to9XHFSHjdMGUB+ZpLXEaWbU6GT48ofnEvunfVlgSKX\nEMMXx+QwNl9FTkREREJXUlw03zprMFed3I/fvbWRZxZu4+mF2zhzWE+um9SfyYMydY+deEKFTo6L\nptZ2irZUMq+4koq6ZtISYrhoTA7jVOREREQkjGSnxHHPRSO58YxBPFVUwtNF2/jX2iIG9UzmukkF\nXDwuh8RYvcWW48ecc15n+ITCwkK3ePFir2NIF6moa+aJ+SU8uaCEqvoWctISOHVQFiNzUjWXnIiI\niBwXV53c75j97KbWdl5bsYu/frCFVaX7SImL5pwRvTl/dB8mD8oiNlofXMvhM7MlzrnCzmyrjw+k\nyznnWLiliucWbefVlbtobfdz5rBe9M9KoiAzUZcjiIiISMSIj4nikvG5XDwuh6Xb9vJ00XbeWLOb\nF5fuoEd8oNx9YXQfJg9UuZNjQ4VOukxZbRMvLinl+cXb2VJRT0pcNJcX5nH95AIGZCfzdNE2ryOK\niIiIHBNmxvj8DMbnZ9DcNpJ5xRW8umIXc1bv5oUlO0iJj2bSwEwmDcxi8qBMBmYn60Nu6RIqdHJU\nGlraeGd9OS8vK+WtdWW0+x0T+mdw0xmDmDaqDwmxUV5HFBERkW7Oqw+VC/MzGJObRnFZHWt27WPh\nlirmrN4DQI/4aAZkJzMgK4m8jESyU+KOaMqmY3k5qYQHFTo5bLVNrby9rozXV+7mnQ1lNLX6yUqO\n46un9WdGYZ4mAxcREREJio7yMaxPD4b16QFAVX0Lm8rq2FRRx8ayOj7cXg1AbLSPvqkJ5KYnkJOe\nQG5aAulJsZqXVw5JhU4OyTlHSWUD7xdX8M76Mt7dUEFLu5+eKXHMKMzjvJF9mNA/Q4OciIiIiBxC\nRlIsGf0zOKl/Bs45ymub2VHdyI69jZTubWDB5kra/IFBC2OijJ4p8fRMiaNXj3h69oijV0o8qYkx\nKnryMRU6OaDy2mY+2FTBB8WVvF9cQWl1IwA5aQlcc0o+00b1Zly/dHwqcSIiIiJHxMzo2SOenj3i\nGdcvHYA2v5+yfc2UVjdStq+JPbXNFJfXsSx4Jg8gyoy0xBgyk2NZt3sf/TISyc9MIj8zkX4ZicTH\n6JaX7kSFTmhoaWNV6T5W7KhmxY4aVuyoZmtlAxC4vnviwEy+cfoAJg3KYkBWkm7gFRERETlGon0+\n+qYl0Dct4RPrG1va2bOvifLaZirrW6iqb6aqvoWXlpZS29z2iW17psQFy10S/TIS6ZMWT5/UwFfv\n1ASS41QBIon2ZjfS0NLG5vJ6NpXXffy4YU8txWV1BM/s0yc1ntG5qcw4KY/JAzVfnIiIiEgoSIiN\noiAriYKspE+sv3JCHtUNrZRUNVBSWc+2ygZKqhrYVtXAvOIKXtzX9KmflRwXTe+PCl6P/y96vVMD\nl3ZmJceRnhiraRbCRKcKnZlNBR4AooBHnHO/2O/1OOAJYDxQCVzunNsafO37wFeAduBm59ycLksv\nQOAet4aWdvY2tFBZ18KumiZ21TSyu6bp4+c79jayq+b/D2ifQW56IoN6JnPeyD6Mzk1lVG4qPVPi\nPfyTiIiIiMjhMDPSk2JJT4plTF7ap15vbmunbF/zp94f7tkXeNy4p4Ky2qaPP9zvKCU+msyk2MB9\nf0lxZCbFkpkcWA48xpGRGEtKfDTJ8dEkx0UTF+3T1VzH2SELnZlFAX8AzgZ2AIvMbJZzbk2Hzb4C\n7HXODTKzK4B7gcvNbDhwBTAC6Av8y8yGOOfau/oPcjw55/A78DuH3zmcA9dh2e+A/ZYdge3a/Y6W\nNj+t7X6a2/y0tPtpDT5+Yn1wXXOrn4aWNuqa26lvbqO+uY265jZqm9rY29BCdUMrVQ0ttLT5P5Uz\nNtr38en1iQMD850MyEpiQHYy+Zm6vlpEREQk0sVFR5GXkUheRuJBt2lr91NeFyh9ZfuaqKwPnCSo\nqm/5+PLOHXsbWLGjmqr6lo8HbTmQmCgjOe6jghdDSvB5Ulyg8CXHRREXHUVctI/YaF/wMbAcF+Mj\nNspHXExU8DGw7DMjymdE+cBn9vGyz2f4LHBPoc9ngUczfD4Cr3fc1ojYotmZM3QTgGLn3GYAM3sW\nmA50LHTTgR8Hn78APGiB/2LTgWedc83AFjMrDv68+V0T//j4++Lt3DFz5cflzQuxUT6S4qI6HAzR\n5GUkMjo3JvCpTGIsGYmBT0x6p8bTNy2B9MSYiP2LKyIiIiJdIzrKR5/UBPqkJhxyW+cc+xrbqAze\nw1dV30J9Sxt1TW3UNgce65o/uVxe28zWinpqgycnmlrbD3hG8Hj56O2xESh5q39yblif6OhMocsB\ntndY3gGcfLBtnHNtZlYDZAbXL9jve3P2/wVmdgNwQ3CxzszWdyq9HI0soMLrEHLEtP/Cm/ZfeNP+\nC2/af+FN+28/V3sd4PCE5P5L+LnXCQ4ov7MbhsSgKM65h4CHvM7RnZjZYudcodc55Mho/4U37b/w\npv0X3rT/wpv2X3jT/js2OjN0TSmQ12E5N7jugNuYWTSQSmBwlM58r4iIiIiIiByBzhS6RcBgM+tv\nZrEEBjmZtd82s4Brg88vBd52zrng+ivMLM7M+gODgYVdE11ERERERKR7O+Qll8F74m4C5hCYtuAx\n59xqM7sbWOycmwU8CvwtOOhJFYHSR3C75wkMoNIG3BjuI1xGEF3iGt60/8Kb9l940/4Lb9p/4U37\nL7xp/x0D5rwatlFERERERESOiqZ/FxERERERCVMqdCIiIiIiImFKha6bMbOpZrbezIrN7A6v88jh\nMbOtZrbSzD40s8Ve55HPZmaPmVmZma3qsC7DzN40s43Bx3QvM8rBHWT//djMSoPH4IdmNs3LjHJw\nZpZnZnPNbI2ZrTazbwXX6xgMA5+x/3QMhgEzizezhWa2PLj/fhJc39/MioLvQ58LDrgoR0n30HUj\nZhYFbADOJjDJ+yLgSufcGk+DSaeZ2Vag0DkXcpNyyqeZ2RSgDnjCOTcyuO4+oMo594vghyrpzrnv\neZlTDuwg++/HQJ1z7ldeZpNDM7M+QB/n3FIzSwGWABcB16FjMOR9xv6bgY7BkGdmBiQ55+rMLAZ4\nH/gWcCsw0zn3rJn9GVjunPuTl1kjgc7QdS8TgGLn3GbnXAvwLDDd40wiEcs59y6BkX87mg48Hnz+\nOIE3KBKCDrL/JEw453Y555YGn9cCa4EcdAyGhc/YfxIGXEBdcDEm+OWAzwMvBNfr+OsiKnTdSw6w\nvcPyDvSPY7hxwBtmtsTMbvA6jByRXs65XcHnu4FeXoaRI3KTma0IXpKpy/XCgJkVAGOBInQMhp39\n9h/oGAwLZhZlZh8CZcCbwCag2jnXFtxE70O7iAqdSHg51Tk3DjgPuDF4SZiEKRe45l3XvYeXPwED\ngTHALuB+b+PIoZhZMvAi8G3n3L6Or+kYDH0H2H86BsOEc67dOTcGyCVwldgwjyNFLBW67qUUyOuw\nnBtcJ2HCOVcafCwDXiLwD6SElz3Be0M+ukekzOM8chicc3uCb1L8wMPoGAxpwXt3XgSecs7NDK7W\nMRgmDrT/dAyGH+dcNTAXmAikmVl08CW9D+0iKnTdyyJgcHCEoVjgCmCWx5mkk8wsKXhjOGaWBJwD\nrPrs75IQNAu4Nvj8WuAfHmaRw/RREQj6IjoGQ1ZwUIZHgbXOuV93eEnHYBg42P7TMRgezCzbzNKC\nzxMIDMi3lkCxuzS4mY6/LqJRLruZ4PC+vwWigMecc//jcSTpJDMbQOCsHEA08LT2X2gzs2eAzwFZ\nwB7gR8DLwPNAP6AEmOGc08AbIegg++9zBC71csBW4Osd7seSEGJmpwLvASsBf3D1nQTuw9IxGOI+\nY/9diY7BkGdmowkMehJF4ATS8865u4PvZZ4FMoBlwDXOuWbvkkYGFToREREREZEwpUsuRURERERE\nwpQKnYiIiIiISJhSoRMREREREQlTKnQiIiIiIiJhSoVOREREREQkTKnQiYiIEJj3yszeN7PzOqy7\nzMz+6WUuERGRz6JpC0RERILMbCTwd2AsgfkelwFTnXObPA0mIiJyECp0IiIiHZjZfUA9kATUOufu\n8TiSiIjIQanQiYiIdGBmScBSoAUodM41exxJRETkoKK9DiAiIhJKnHP1ZvYcUKcyJyIioU6DooiI\niHyaP/glIiIS0lToREREREREwpQKnYiIiIiISJjSoCgiIiIiIiJhSmfoREREREREwpQKnYiIiIiI\nSJhSoRMREREREQlTKnQiIiIiIiJhSoVOREREREQkTKnQiYiIiIiIhCkVOhERERERkTD1f0/B7rEy\nV5noAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvQeWRthGvht",
        "colab_type": "text"
      },
      "source": [
        "That's pretty normal, let's make two binary categories--one balanced, one unbalanced, to see the difference.\n",
        "* balanced binary variable will be split evenly in half\n",
        "* unbalanced binary variable will indicate whether $Y <5$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSQHngeGHrbj",
        "colab_type": "code",
        "outputId": "9881a791-f0bc-4b3e-9254-794e9db9a5d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "friedman['Y_bal'] = friedman['Y'].apply(lambda y: 1 if (y < friedman.Y.median()) else 0)\n",
        "friedman['Y_un'] = friedman['Y'].apply(lambda y: 1 if (y < 5) else 0)\n",
        "\n",
        "print(friedman.Y_bal.value_counts(), '\\n\\n', friedman.Y_un.value_counts())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    750\n",
            "0    750\n",
            "Name: Y_bal, dtype: int64 \n",
            "\n",
            " 0    1461\n",
            "1      39\n",
            "Name: Y_un, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keb3N-12oT9U",
        "colab_type": "code",
        "outputId": "e6b007a3-6892-4c7d-8d5a-4673d77ec07f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "friedman.sample(8)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>Y</th>\n",
              "      <th>Y_bal</th>\n",
              "      <th>Y_un</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>0.653163</td>\n",
              "      <td>0.938305</td>\n",
              "      <td>0.871204</td>\n",
              "      <td>0.766065</td>\n",
              "      <td>0.788447</td>\n",
              "      <td>0.664985</td>\n",
              "      <td>0.260287</td>\n",
              "      <td>0.907195</td>\n",
              "      <td>0.670732</td>\n",
              "      <td>0.560441</td>\n",
              "      <td>0.648586</td>\n",
              "      <td>0.912552</td>\n",
              "      <td>0.841731</td>\n",
              "      <td>0.818493</td>\n",
              "      <td>23.629323</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1308</th>\n",
              "      <td>0.468035</td>\n",
              "      <td>0.213449</td>\n",
              "      <td>0.934605</td>\n",
              "      <td>0.796600</td>\n",
              "      <td>0.055666</td>\n",
              "      <td>0.998943</td>\n",
              "      <td>0.694927</td>\n",
              "      <td>0.160267</td>\n",
              "      <td>0.237168</td>\n",
              "      <td>0.692445</td>\n",
              "      <td>0.468666</td>\n",
              "      <td>0.257326</td>\n",
              "      <td>0.922035</td>\n",
              "      <td>0.767075</td>\n",
              "      <td>15.001858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>0.435262</td>\n",
              "      <td>0.920691</td>\n",
              "      <td>0.345017</td>\n",
              "      <td>0.054733</td>\n",
              "      <td>0.204242</td>\n",
              "      <td>0.418695</td>\n",
              "      <td>0.219635</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.915002</td>\n",
              "      <td>0.843879</td>\n",
              "      <td>0.473871</td>\n",
              "      <td>0.865579</td>\n",
              "      <td>0.318038</td>\n",
              "      <td>0.049064</td>\n",
              "      <td>11.459331</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>0.691720</td>\n",
              "      <td>0.661343</td>\n",
              "      <td>0.038401</td>\n",
              "      <td>0.368250</td>\n",
              "      <td>0.979032</td>\n",
              "      <td>0.420737</td>\n",
              "      <td>0.501945</td>\n",
              "      <td>0.909930</td>\n",
              "      <td>0.717262</td>\n",
              "      <td>0.582070</td>\n",
              "      <td>0.706203</td>\n",
              "      <td>0.684562</td>\n",
              "      <td>0.027449</td>\n",
              "      <td>0.386265</td>\n",
              "      <td>22.642635</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>0.677038</td>\n",
              "      <td>0.803212</td>\n",
              "      <td>0.561877</td>\n",
              "      <td>0.740883</td>\n",
              "      <td>0.661860</td>\n",
              "      <td>0.355530</td>\n",
              "      <td>0.808859</td>\n",
              "      <td>0.294701</td>\n",
              "      <td>0.571213</td>\n",
              "      <td>0.745398</td>\n",
              "      <td>0.713179</td>\n",
              "      <td>0.795122</td>\n",
              "      <td>0.574424</td>\n",
              "      <td>0.793364</td>\n",
              "      <td>20.592819</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1148</th>\n",
              "      <td>0.964686</td>\n",
              "      <td>0.875557</td>\n",
              "      <td>0.875934</td>\n",
              "      <td>0.176067</td>\n",
              "      <td>0.264907</td>\n",
              "      <td>0.076393</td>\n",
              "      <td>0.778468</td>\n",
              "      <td>0.069816</td>\n",
              "      <td>0.556696</td>\n",
              "      <td>0.247876</td>\n",
              "      <td>0.976942</td>\n",
              "      <td>0.875351</td>\n",
              "      <td>0.877360</td>\n",
              "      <td>0.174028</td>\n",
              "      <td>10.493763</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>788</th>\n",
              "      <td>0.415711</td>\n",
              "      <td>0.728316</td>\n",
              "      <td>0.798772</td>\n",
              "      <td>0.945190</td>\n",
              "      <td>0.608505</td>\n",
              "      <td>0.254627</td>\n",
              "      <td>0.924148</td>\n",
              "      <td>0.775316</td>\n",
              "      <td>0.574428</td>\n",
              "      <td>0.695640</td>\n",
              "      <td>0.418045</td>\n",
              "      <td>0.711830</td>\n",
              "      <td>0.798872</td>\n",
              "      <td>0.927330</td>\n",
              "      <td>22.313361</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>0.915090</td>\n",
              "      <td>0.533029</td>\n",
              "      <td>0.157955</td>\n",
              "      <td>0.695899</td>\n",
              "      <td>0.793261</td>\n",
              "      <td>0.316762</td>\n",
              "      <td>0.857179</td>\n",
              "      <td>0.906143</td>\n",
              "      <td>0.276904</td>\n",
              "      <td>0.983521</td>\n",
              "      <td>0.973598</td>\n",
              "      <td>0.516610</td>\n",
              "      <td>0.199860</td>\n",
              "      <td>0.676132</td>\n",
              "      <td>23.150468</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            X1        X2        X3        X4  ...       X14          Y  Y_bal  Y_un\n",
              "156   0.653163  0.938305  0.871204  0.766065  ...  0.818493  23.629323      0     0\n",
              "1308  0.468035  0.213449  0.934605  0.796600  ...  0.767075  15.001858      0     0\n",
              "337   0.435262  0.920691  0.345017  0.054733  ...  0.049064  11.459331      1     0\n",
              "374   0.691720  0.661343  0.038401  0.368250  ...  0.386265  22.642635      0     0\n",
              "478   0.677038  0.803212  0.561877  0.740883  ...  0.793364  20.592819      0     0\n",
              "1148  0.964686  0.875557  0.875934  0.176067  ...  0.174028  10.493763      1     0\n",
              "788   0.415711  0.728316  0.798772  0.945190  ...  0.927330  22.313361      0     0\n",
              "150   0.915090  0.533029  0.157955  0.695899  ...  0.676132  23.150468      0     0\n",
              "\n",
              "[8 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8Ab5FKxjkDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally, let's put it all into our usual X and y's\n",
        "# (I already have the X dataframe as friedmanX, but I'm working backward to\n",
        "# follow a usual flow)\n",
        "\n",
        "X = friedman.drop(columns=['Y', 'Y_bal', 'Y_un'])\n",
        "\n",
        "y = friedman.Y\n",
        "\n",
        "y_bal = friedman.Y_bal\n",
        "\n",
        "y_un = friedman.Y_un"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q11QTvNXI87Z",
        "colab_type": "text"
      },
      "source": [
        "#### Alright! Let's get to it! Remember, with each part, we are increasing complexity of the analysis and thereby increasing the computational costs and runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_7irYK2QfES",
        "colab_type": "text"
      },
      "source": [
        "So even before univariate selection--which compares each feature to the output feature one by one--there is a [VarianceThreshold](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) object in sklearn.feature_selection. It defaults to getting rid of any features that are the same across all samples. Great for cleaning data in that respect. \n",
        "\n",
        "The `threshold` parameter defaults to `0` to show the above behavior. if you change it, make sure you have good reason. Use with caution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flvDcuYAbPxP",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: univariate selection\n",
        "* Best for goal 2 - getting \"a better understanding of the data, its structure and characteristics\"\n",
        "* unable to remove redundancy (for example selecting only the best feature among a subset of strongly correlated features)\n",
        "* Super fast - can be used for baseline models or just after baseline\n",
        "\n",
        "[sci-kit's univariariate feature selection objects and techniques](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZqzkGVmM_aj",
        "colab_type": "text"
      },
      "source": [
        "#### Y (continuous output)\n",
        "\n",
        "options (they do what they sound like they do)\n",
        "* SelectKBest\n",
        "* SelectPercentile\n",
        "\n",
        "both take the same parameter options for `score_func`\n",
        "* `f_regression`: scores by correlation coefficient, f value, p value--basically automates what you can do by looking at a correlation matrix except without the ability to recognize collinearity\n",
        "* `mutual_info_regression`: can capture non-linear correlations, but doesn't handle noise well\n",
        "\n",
        "Let's take a look at mutual information (MI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsDZpC8PM-cc",
        "colab_type": "code",
        "outputId": "9c0b1812-92cd-4498-875a-852929698e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import sklearn.feature_selection as fe\n",
        "\n",
        "MIR = fe.SelectKBest(fe.mutual_info_regression, k='all').fit(X, y)\n",
        "\n",
        "MIR_scores = pd.Series(data=MIR.scores_, name='MI_Reg_Scores', index=names)\n",
        "\n",
        "MIR_scores"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.117499\n",
              "X2     0.155141\n",
              "X3     0.064104\n",
              "X4     0.239914\n",
              "X5     0.059098\n",
              "X6     0.006906\n",
              "X7     0.000000\n",
              "X8     0.000000\n",
              "X9     0.000000\n",
              "X10    0.000000\n",
              "X11    0.069175\n",
              "X12    0.113326\n",
              "X13    0.036673\n",
              "X14    0.220115\n",
              "Name: MI_Reg_Scores, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIUBoCf8NsIA",
        "colab_type": "text"
      },
      "source": [
        "#### Y_bal (balanced binary output)\n",
        "\n",
        "options\n",
        "* SelectKBest\n",
        "* SelectPercentile\n",
        "\n",
        "these options will cut out features with error rates above a certain tolerance level, define in parameter -`alpha`\n",
        "* SelectFpr (false positive rate--false positives predicted/total negatives in dataset)\n",
        "* SelectFdr (false discovery rate--false positives predicted/total positives predicted)\n",
        "* ~~SelectFwe (family-wise error--for multinomial classification tasks)~~\n",
        "\n",
        "all have the same optons for parameter `score_func`\n",
        "* `chi2`\n",
        "* `f_classif`\n",
        "* `mutual_info_classif`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEkioNaHbPxR",
        "colab_type": "code",
        "outputId": "58941633-1ea1-49b6-f56a-7b97c594b4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "MIC_b = fe.SelectFpr(fe.mutual_info_classif).fit(X, y_bal)\n",
        "\n",
        "MIC_b_scores = pd.Series(data=MIC_b.scores_, \n",
        "                              name='MIC_Bal_Scores', index=names)\n",
        "\n",
        "MIC_b_scores"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.061386\n",
              "X2     0.111060\n",
              "X3     0.006320\n",
              "X4     0.113995\n",
              "X5     0.000000\n",
              "X6     0.000000\n",
              "X7     0.000000\n",
              "X8     0.000000\n",
              "X9     0.000000\n",
              "X10    0.018296\n",
              "X11    0.063916\n",
              "X12    0.091263\n",
              "X13    0.028924\n",
              "X14    0.109699\n",
              "Name: MIC_Bal_Scores, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0fi6NoHXG7u",
        "colab_type": "text"
      },
      "source": [
        "#### Y_un (unbalanced binary output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWdeeg0XGhh",
        "colab_type": "code",
        "outputId": "efae1f17-4b2e-482a-8a52-474a5ad130e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "MIC_u = fe.SelectFpr(fe.mutual_info_classif).fit(X, y_un)\n",
        "\n",
        "MIC_u_scores = pd.Series(data=MIC_u.scores_, \n",
        "                              name='MIC_Unbal_Scores', index=names)\n",
        "\n",
        "MIC_u_scores"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.007352\n",
              "X2     0.011942\n",
              "X3     0.010933\n",
              "X4     0.042207\n",
              "X5     0.015932\n",
              "X6     0.000000\n",
              "X7     0.000000\n",
              "X8     0.006586\n",
              "X9     0.000000\n",
              "X10    0.000000\n",
              "X11    0.004096\n",
              "X12    0.011106\n",
              "X13    0.006392\n",
              "X14    0.033513\n",
              "Name: MIC_Unbal_Scores, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBavXOYvbPxb",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: linear models and regularization\n",
        "* L1 Regularization (Lasso for regression) is best for goal 1: \"produces sparse solutions and as such is very useful selecting a strong subset of features for improving model performance\" (forces coefficients to zero, telling you which you could remove--but doesn't handle multicollinearity)\n",
        "* L2 Regularization (Ridge for regression) is best for goal 2: \"can be used for data interpretation due to its stability and the fact that useful features tend to have non-zero coefficients\n",
        "* Also fast\n",
        "\n",
        "[sci-kit's L1 feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection) (can easily be switched to L2 using the parameter `penalty='l2'` for categorical targets or using `Ridge` instead of Lasso for continuous targets)\n",
        "\n",
        "We won't do this here, because\n",
        "1.   You know regression\n",
        "2.   The same principles apply as shown in Part 3 below with `SelectFromModel`\n",
        "3.   There's way cooler stuff coming up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrTUHbfwbPxk",
        "colab_type": "text"
      },
      "source": [
        "## Part 3: random forests\n",
        "* Best for goal 1, not 2 because:\n",
        " * strong features can end up with low scores \n",
        " * biased towards variables with many categories\n",
        "* \"require very little feature engineering and parameter tuning\"\n",
        "* Takes a little more time depending on your dataset - but a popular technique\n",
        "\n",
        "[sci-kit's implementation of tree-based feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKDsc9AjP4fD",
        "colab_type": "text"
      },
      "source": [
        "#### Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pP7OuQTbPxo",
        "colab_type": "code",
        "outputId": "8ffafa0f-8b73-4d27-c324-f8d7bf079f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor as RFR\n",
        "\n",
        "# Fitting a random forest regression\n",
        "rfr = RFR().fit(X, y)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "rfr_scores = pd.Series(data=rfr.feature_importances_, name='RFR', index=names)\n",
        "\n",
        "rfr_scores"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.163012\n",
              "X2     0.103816\n",
              "X3     0.027583\n",
              "X4     0.275405\n",
              "X5     0.088368\n",
              "X6     0.006412\n",
              "X7     0.007571\n",
              "X8     0.006643\n",
              "X9     0.007580\n",
              "X10    0.006867\n",
              "X11    0.068369\n",
              "X12    0.111822\n",
              "X13    0.025019\n",
              "X14    0.101533\n",
              "Name: RFR, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dEoMeW2Z88Z",
        "colab_type": "text"
      },
      "source": [
        "#### Y_bal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9JuV3HCaAJw",
        "colab_type": "code",
        "outputId": "7b3674ea-4ad1-4879-bdb2-642b981d1f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "\n",
        "# Fitting a Random Forest Classifier\n",
        "rfc_b = RFC().fit(X, y_bal)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "rfc_b_scores = pd.Series(data=rfc_b.feature_importances_, name='RFC_bal', \n",
        "                           index=names)\n",
        "\n",
        "rfc_b_scores"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.139816\n",
              "X2     0.128631\n",
              "X3     0.048489\n",
              "X4     0.166418\n",
              "X5     0.082785\n",
              "X6     0.013498\n",
              "X7     0.024150\n",
              "X8     0.025102\n",
              "X9     0.018298\n",
              "X10    0.024272\n",
              "X11    0.078391\n",
              "X12    0.103563\n",
              "X13    0.041178\n",
              "X14    0.105407\n",
              "Name: RFC_bal, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUt0DSzRaNWF",
        "colab_type": "text"
      },
      "source": [
        "#### Y_un"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTuAcM7baYUv",
        "colab_type": "code",
        "outputId": "5094db82-6546-4cef-cd0f-5c2d57ab83b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "# Fitting a Random Forest Classifier\n",
        "rfc_u = RFC().fit(X, y_un)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "rfc_u_scores = pd.Series(data=rfc_u.feature_importances_, \n",
        "                             name='RFC_unbal', index=names)\n",
        "\n",
        "rfc_u_scores"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.068201\n",
              "X2     0.142481\n",
              "X3     0.049184\n",
              "X4     0.101110\n",
              "X5     0.096744\n",
              "X6     0.030833\n",
              "X7     0.052951\n",
              "X8     0.029340\n",
              "X9     0.040692\n",
              "X10    0.069171\n",
              "X11    0.067560\n",
              "X12    0.084063\n",
              "X13    0.046844\n",
              "X14    0.120828\n",
              "Name: RFC_unbal, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR0vdLluatex",
        "colab_type": "text"
      },
      "source": [
        "### SelectFromModel \n",
        "is a meta-transformer that can be used along with any estimator that has a `coef_` or `feature_importances_` attribute after fitting. The features are considered unimportant and removed, if the corresponding `coef_` or `feature_importances_` values are below the provided `threshold` parameter. Apart from specifying the `threshold` numerically, there are built-in heuristics for finding a `threshold` using a string argument. Available heuristics are `'mean'`, `'median'` and float multiples of these like `'0.1*mean'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZeUn8PTatE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random forest regression transformation of X (elimination of least important\n",
        "# features)\n",
        "rfr_transform = fe.SelectFromModel(rfr, prefit=True)\n",
        "\n",
        "X_rfr = rfr_transform.transform(X)\n",
        "\n",
        "\n",
        "# Random forest classifier transformation of X_bal (elimination of least important\n",
        "# features)\n",
        "rfc_b_transform = fe.SelectFromModel(rfc_b, prefit=True)\n",
        "\n",
        "X_rfc_b = rfc_b_transform.transform(X)\n",
        "\n",
        "\n",
        "# Random forest classifier transformation of X_un (elimination of least important\n",
        "# features)\n",
        "rfc_u_transform = fe.SelectFromModel(rfc_u, prefit=True)\n",
        "\n",
        "X_rfc_u = rfc_u_transform.transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnXIN6lkc5hr",
        "colab_type": "code",
        "outputId": "d9b4aa11-4ba8-48df-caea-875dd1e09ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "source": [
        "RF_comparisons = pd.DataFrame(data=np.array([rfr_transform.get_support(),\n",
        "                                    rfc_b_transform.get_support(),\n",
        "                                    rfc_u_transform.get_support()]).T,\n",
        "                              columns=['RF_Regressor', 'RF_balanced_classifier',\n",
        "                                     'RF_unbalanced_classifier'],\n",
        "                              index=names)\n",
        "\n",
        "RF_comparisons"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RF_Regressor</th>\n",
              "      <th>RF_balanced_classifier</th>\n",
              "      <th>RF_unbalanced_classifier</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X1</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X2</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X4</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X5</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X6</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X7</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X8</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X9</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X10</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X11</th>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X12</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X13</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X14</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     RF_Regressor  RF_balanced_classifier  RF_unbalanced_classifier\n",
              "X1           True                    True                     False\n",
              "X2           True                    True                      True\n",
              "X3          False                   False                     False\n",
              "X4           True                    True                      True\n",
              "X5           True                    True                      True\n",
              "X6          False                   False                     False\n",
              "X7          False                   False                     False\n",
              "X8          False                   False                     False\n",
              "X9          False                   False                     False\n",
              "X10         False                   False                     False\n",
              "X11         False                    True                     False\n",
              "X12          True                    True                      True\n",
              "X13         False                   False                     False\n",
              "X14          True                    True                      True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzas2GMebPxv",
        "colab_type": "text"
      },
      "source": [
        "## Part 4: stability selection, RFE, and everything side by side\n",
        "* These methods take longer since they are *wrapper methods* and build multiple ML models before giving results. \"They both build on top of other (model based) selection methods such as regression or SVM, building models on different subsets of data and extracting the ranking from the aggregates.\"\n",
        "* Stability selection is good for both goal 1 and 2: \"among the top performing methods for many different datasets and settings\"\n",
        " * For categorical targets\n",
        "   * ~~[RandomizedLogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLogisticRegression.html)~~ (Deprecated) use [RandomizedLogisticRegression](https://thuijskens.github.io/stability-selection/docs/randomized_lasso.html#stability_selection.randomized_lasso.RandomizedLogisticRegression)\n",
        "  \n",
        "   * [ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier)\n",
        " \n",
        " * For continuous targets\n",
        "   * ~~[RandomizedLasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLasso.html)~~ (Deprecated) use [RandomizedLasso](https://thuijskens.github.io/stability-selection/docs/randomized_lasso.html#stability_selection.randomized_lasso.RandomizedLogisticRegression) \n",
        "   * [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor)\n",
        "   \n",
        "    Welcome to open-source, folks! [Here](https://github.com/scikit-learn/scikit-learn/issues/8995) is the original discussion to deprecate `RandomizedLogisticRegression` and `RandomizedLasso`. [Here](https://github.com/scikit-learn/scikit-learn/issues/9657) is a failed attempt to resurrect it. It looks like it'll be gone for good soon. So we shouldn't get dependent on it.\n",
        "\n",
        "    The alternatives from the deprecated scikit objects come from an official scikit-learn-contrib module called [stability_selection](https://github.com/scikit-learn-contrib/stability-selection). They also have a `StabilitySelection` object that acts similarly scikit's `SelectFromModel`.\n",
        "\n",
        "* recursive feature elimination (RFE) is best for goal 1\n",
        " * [sci-kit's RFE and RFECV (RFE with built-in cross-validation)](https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6722EqvCbPxx",
        "colab_type": "code",
        "outputId": "ac7dce0c-211c-4cad-91e0-19c71deef90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "!pip install git+https://github.com/scikit-learn-contrib/stability-selection.git"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/scikit-learn-contrib/stability-selection.git\n",
            "  Cloning https://github.com/scikit-learn-contrib/stability-selection.git to /tmp/pip-req-build-ugu0oi8_\n",
            "  Running command git clone -q https://github.com/scikit-learn-contrib/stability-selection.git /tmp/pip-req-build-ugu0oi8_\n",
            "Requirement already satisfied: nose>=1.1.2 in /usr/local/lib/python3.6/dist-packages (from stability-selection==0.0.1) (1.3.7)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.6/dist-packages (from stability-selection==0.0.1) (0.21.3)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from stability-selection==0.0.1) (3.0.3)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from stability-selection==0.0.1) (1.16.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19->stability-selection==0.0.1) (0.13.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19->stability-selection==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->stability-selection==0.0.1) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->stability-selection==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->stability-selection==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->stability-selection==0.0.1) (2.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0.0->stability-selection==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->stability-selection==0.0.1) (41.0.1)\n",
            "Building wheels for collected packages: stability-selection\n",
            "  Building wheel for stability-selection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stability-selection: filename=stability_selection-0.0.1-cp36-none-any.whl size=14463 sha256=67d4fe6973d43fbdf646c469d4c6d7305ac92dcdc42657076db9254a8b864b7d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hs7i_pqf/wheels/58/be/39/79880712b91ffa56e341ff10586a1956527813437ddd759473\n",
            "Successfully built stability-selection\n",
            "Installing collected packages: stability-selection\n",
            "Successfully installed stability-selection-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhLZ-il_rm_a",
        "colab_type": "text"
      },
      "source": [
        "Okay, I tried this package... it seems to have some problems... hopefully a good implementation of stability selection for Lasso and Logistic Regression will be created soon! In the meantime, scikit's RandomLasso and RandomLogisticRegression have not been removed, so you can fiddle some! Just alter the commented out code!\n",
        "* import from scikit instead of stability-selection\n",
        "* use scikit's `SelectFromModel` as shown above!\n",
        "\n",
        "Ta Da!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etgVb1BqkyGX",
        "colab_type": "text"
      },
      "source": [
        "#### Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViYGwRzVbk7N",
        "colab_type": "code",
        "outputId": "0747c4e3-713e-4114-dda8-76ea014700aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "\n",
        "from stability_selection import (RandomizedLogisticRegression,\n",
        "                                 RandomizedLasso, StabilitySelection,\n",
        "                                 plot_stability_path)\n",
        "\n",
        "# Stability selection using randomized lasso method\n",
        "rl = RandomizedLasso(max_iter=2000)\n",
        "rl_selector = StabilitySelection(base_estimator=rl, lambda_name='alpha',\n",
        "                                 n_jobs=2)\n",
        "rl_selector.fit(X, y);\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnza3BUPmiRo",
        "colab_type": "code",
        "outputId": "5ffe6525-b708-456b-d077-48c0105a91de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor as ETR\n",
        "\n",
        "# Stability selection using randomized decision trees\n",
        "etr = ETR(n_estimators=50).fit(X, y)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "etr_scores = pd.Series(data=etr.feature_importances_, \n",
        "                             name='ETR', index=names)\n",
        "\n",
        "etr_scores"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.124206\n",
              "X2     0.117021\n",
              "X3     0.029651\n",
              "X4     0.200774\n",
              "X5     0.086547\n",
              "X6     0.007626\n",
              "X7     0.007762\n",
              "X8     0.007422\n",
              "X9     0.008203\n",
              "X10    0.008077\n",
              "X11    0.109957\n",
              "X12    0.105444\n",
              "X13    0.027643\n",
              "X14    0.159667\n",
              "Name: ETR, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiwjyAya5YWL",
        "colab_type": "code",
        "outputId": "b1cb3edf-4146-4d05-98a9-3cce6e260589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Recursive feature elimination with cross validaiton using linear regression \n",
        "# as the model\n",
        "lr = LinearRegression()\n",
        "# rank all features, i.e continue the elimination until the last one\n",
        "rfe = fe.RFECV(lr)\n",
        "rfe.fit(X, y)\n",
        "\n",
        "rfe_score = pd.Series(data=(-1*rfe.ranking_), name='RFE', index=names)\n",
        "\n",
        "rfe_score"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1    -1\n",
              "X2    -1\n",
              "X3    -1\n",
              "X4    -1\n",
              "X5    -1\n",
              "X6    -8\n",
              "X7    -5\n",
              "X8    -7\n",
              "X9    -6\n",
              "X10   -9\n",
              "X11   -2\n",
              "X12   -4\n",
              "X13   -1\n",
              "X14   -3\n",
              "Name: RFE, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcGTYITXk0tb",
        "colab_type": "text"
      },
      "source": [
        "#### Y_bal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwfhOxBykwPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stability selection using randomized logistic regression\n",
        "rlr_b = RandomizedLogisticRegression()\n",
        "rlr_b_selector = StabilitySelection(base_estimator=rlr_b, lambda_name='C',\n",
        "                                 n_jobs=2)\n",
        "rlr_b_selector.fit(X, y_bal);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TLTMsPkoLzX",
        "colab_type": "code",
        "outputId": "00af412f-601d-4460-91a6-cb394ee0c418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier as ETC\n",
        "\n",
        "# Stability selection using randomized decision trees\n",
        "etc_b = ETC(n_estimators=50).fit(X, y_bal)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "etc_b_scores = pd.Series(data=etc_b.feature_importances_, \n",
        "                             name='ETC_bal', index=names)\n",
        "\n",
        "etc_b_scores"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.114290\n",
              "X2     0.099273\n",
              "X3     0.046700\n",
              "X4     0.141563\n",
              "X5     0.078125\n",
              "X6     0.027664\n",
              "X7     0.027829\n",
              "X8     0.028930\n",
              "X9     0.030802\n",
              "X10    0.026378\n",
              "X11    0.104147\n",
              "X12    0.110390\n",
              "X13    0.043142\n",
              "X14    0.120767\n",
              "Name: ETC_bal, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaI4yA97kLo",
        "colab_type": "code",
        "outputId": "7c32500d-5bc6-4bc1-f6b9-7dde51c8f0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Recursive feature elimination with cross validaiton using logistic regression \n",
        "# as the model\n",
        "logr_b = LogisticRegression(solver='lbfgs')\n",
        "# rank all features, i.e continue the elimination until the last one\n",
        "rfe_b = fe.RFECV(logr_b)\n",
        "rfe_b.fit(X, y_bal)\n",
        "\n",
        "rfe_b_score = pd.Series(data=(-1*rfe_b.ranking_), name='RFE_bal', index=names)\n",
        "\n",
        "rfe_b_score"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1    -1\n",
              "X2    -1\n",
              "X3    -1\n",
              "X4    -1\n",
              "X5    -1\n",
              "X6    -4\n",
              "X7    -1\n",
              "X8    -2\n",
              "X9    -3\n",
              "X10   -1\n",
              "X11   -1\n",
              "X12   -1\n",
              "X13   -5\n",
              "X14   -1\n",
              "Name: RFE_bal, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDe5AYa1lO53",
        "colab_type": "text"
      },
      "source": [
        "#### Y_un"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jThOwdBqlSEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stability selection uisng randomized logistic regression\n",
        "rlr_u = RandomizedLogisticRegression(max_iter=2000)\n",
        "rlr_u_selector = StabilitySelection(base_estimator=rlr_u, lambda_name='C')\n",
        "\n",
        "rlr_u_selector.fit(X, y_un);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2og3pRNoYXo",
        "colab_type": "code",
        "outputId": "87891acd-73b4-4f36-ed29-65d04c16cf6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Stability selection using randomized decision trees\n",
        "etc_u = ETC(n_estimators=50).fit(X, y_un)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "etc_u_scores = pd.Series(data=etc_u.feature_importances_, \n",
        "                             name='ETC_unbal', index=names)\n",
        "\n",
        "etc_u_scores"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.070536\n",
              "X2     0.082431\n",
              "X3     0.057799\n",
              "X4     0.121182\n",
              "X5     0.091107\n",
              "X6     0.045279\n",
              "X7     0.045399\n",
              "X8     0.042903\n",
              "X9     0.050784\n",
              "X10    0.049616\n",
              "X11    0.073390\n",
              "X12    0.097007\n",
              "X13    0.049434\n",
              "X14    0.123132\n",
              "Name: ETC_unbal, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCLO-2tH8KBe",
        "colab_type": "code",
        "outputId": "201c9666-2ea8-436f-8912-d30f75cab04e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "# Recursive feature elimination with cross validaiton using logistic regression \n",
        "# as the model\n",
        "logr_u = LogisticRegression(solver='lbfgs')\n",
        "# rank all features, i.e continue the elimination until the last one\n",
        "rfe_u = fe.RFECV(logr_u)\n",
        "rfe_u.fit(X, y_un)\n",
        "\n",
        "rfe_u_score = pd.Series(data=(-1*rfe_u.ranking_), name='RFE_unbal', index=names)\n",
        "\n",
        "rfe_u_score"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1    -1\n",
              "X2    -1\n",
              "X3    -6\n",
              "X4    -1\n",
              "X5    -1\n",
              "X6    -4\n",
              "X7    -5\n",
              "X8    -1\n",
              "X9    -3\n",
              "X10   -1\n",
              "X11   -1\n",
              "X12   -1\n",
              "X13   -2\n",
              "X14   -1\n",
              "Name: RFE_unbal, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RND1UIMRo6kk",
        "colab_type": "code",
        "outputId": "ae375812-f1c6-4c6d-dba7-72828a977d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "source": [
        "RL_comparisons = pd.DataFrame(data=np.array([rl_selector.get_support(),\n",
        "                                    rlr_b_selector.get_support(),\n",
        "                                    rlr_u_selector.get_support()]).T,\n",
        "                              columns=['RandomLasso', 'RandomLog_bal',\n",
        "                                     'RandomLog_unbal'],\n",
        "                              index=names)\n",
        "\n",
        "RL_comparisons"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomLasso</th>\n",
              "      <th>RandomLog_bal</th>\n",
              "      <th>RandomLog_unbal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X1</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X2</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X3</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X4</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X5</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X6</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X7</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X8</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X9</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X10</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X11</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X12</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X13</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X14</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     RandomLasso  RandomLog_bal  RandomLog_unbal\n",
              "X1          True          False            False\n",
              "X2          True          False            False\n",
              "X3          True          False            False\n",
              "X4          True          False            False\n",
              "X5          True          False            False\n",
              "X6          True          False            False\n",
              "X7          True          False            False\n",
              "X8          True          False            False\n",
              "X9          True          False            False\n",
              "X10         True          False            False\n",
              "X11         True          False            False\n",
              "X12         True          False            False\n",
              "X13         True          False            False\n",
              "X14         True          False            False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0-G9GGjpq1Z",
        "colab_type": "code",
        "outputId": "d34336db-716f-4f85-c31e-a0bcccc7393f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "source": [
        "comparisons = pd.concat([MIR_scores, MIC_b_scores, MIC_u_scores, rfr_scores,\n",
        "                         rfc_b_scores, rfc_u_scores, etr_scores, etc_b_scores,\n",
        "                         etc_u_scores, rfe_score, rfe_b_score, rfe_u_score], \n",
        "                        axis=1)\n",
        "\n",
        "comparisons"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MI_Reg_Scores</th>\n",
              "      <th>MIC_Bal_Scores</th>\n",
              "      <th>MIC_Unbal_Scores</th>\n",
              "      <th>RFR</th>\n",
              "      <th>RFC_bal</th>\n",
              "      <th>RFC_unbal</th>\n",
              "      <th>ETR</th>\n",
              "      <th>ETC_bal</th>\n",
              "      <th>ETC_unbal</th>\n",
              "      <th>RFE</th>\n",
              "      <th>RFE_bal</th>\n",
              "      <th>RFE_unbal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X1</th>\n",
              "      <td>0.117499</td>\n",
              "      <td>0.061386</td>\n",
              "      <td>0.007352</td>\n",
              "      <td>0.163012</td>\n",
              "      <td>0.139816</td>\n",
              "      <td>0.068201</td>\n",
              "      <td>0.124206</td>\n",
              "      <td>0.114290</td>\n",
              "      <td>0.070536</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X2</th>\n",
              "      <td>0.155141</td>\n",
              "      <td>0.111060</td>\n",
              "      <td>0.011942</td>\n",
              "      <td>0.103816</td>\n",
              "      <td>0.128631</td>\n",
              "      <td>0.142481</td>\n",
              "      <td>0.117021</td>\n",
              "      <td>0.099273</td>\n",
              "      <td>0.082431</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X3</th>\n",
              "      <td>0.064104</td>\n",
              "      <td>0.006320</td>\n",
              "      <td>0.010933</td>\n",
              "      <td>0.027583</td>\n",
              "      <td>0.048489</td>\n",
              "      <td>0.049184</td>\n",
              "      <td>0.029651</td>\n",
              "      <td>0.046700</td>\n",
              "      <td>0.057799</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X4</th>\n",
              "      <td>0.239914</td>\n",
              "      <td>0.113995</td>\n",
              "      <td>0.042207</td>\n",
              "      <td>0.275405</td>\n",
              "      <td>0.166418</td>\n",
              "      <td>0.101110</td>\n",
              "      <td>0.200774</td>\n",
              "      <td>0.141563</td>\n",
              "      <td>0.121182</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X5</th>\n",
              "      <td>0.059098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015932</td>\n",
              "      <td>0.088368</td>\n",
              "      <td>0.082785</td>\n",
              "      <td>0.096744</td>\n",
              "      <td>0.086547</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>0.091107</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X6</th>\n",
              "      <td>0.006906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006412</td>\n",
              "      <td>0.013498</td>\n",
              "      <td>0.030833</td>\n",
              "      <td>0.007626</td>\n",
              "      <td>0.027664</td>\n",
              "      <td>0.045279</td>\n",
              "      <td>-8</td>\n",
              "      <td>-4</td>\n",
              "      <td>-4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007571</td>\n",
              "      <td>0.024150</td>\n",
              "      <td>0.052951</td>\n",
              "      <td>0.007762</td>\n",
              "      <td>0.027829</td>\n",
              "      <td>0.045399</td>\n",
              "      <td>-5</td>\n",
              "      <td>-1</td>\n",
              "      <td>-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006586</td>\n",
              "      <td>0.006643</td>\n",
              "      <td>0.025102</td>\n",
              "      <td>0.029340</td>\n",
              "      <td>0.007422</td>\n",
              "      <td>0.028930</td>\n",
              "      <td>0.042903</td>\n",
              "      <td>-7</td>\n",
              "      <td>-2</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007580</td>\n",
              "      <td>0.018298</td>\n",
              "      <td>0.040692</td>\n",
              "      <td>0.008203</td>\n",
              "      <td>0.030802</td>\n",
              "      <td>0.050784</td>\n",
              "      <td>-6</td>\n",
              "      <td>-3</td>\n",
              "      <td>-3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018296</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006867</td>\n",
              "      <td>0.024272</td>\n",
              "      <td>0.069171</td>\n",
              "      <td>0.008077</td>\n",
              "      <td>0.026378</td>\n",
              "      <td>0.049616</td>\n",
              "      <td>-9</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X11</th>\n",
              "      <td>0.069175</td>\n",
              "      <td>0.063916</td>\n",
              "      <td>0.004096</td>\n",
              "      <td>0.068369</td>\n",
              "      <td>0.078391</td>\n",
              "      <td>0.067560</td>\n",
              "      <td>0.109957</td>\n",
              "      <td>0.104147</td>\n",
              "      <td>0.073390</td>\n",
              "      <td>-2</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X12</th>\n",
              "      <td>0.113326</td>\n",
              "      <td>0.091263</td>\n",
              "      <td>0.011106</td>\n",
              "      <td>0.111822</td>\n",
              "      <td>0.103563</td>\n",
              "      <td>0.084063</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.110390</td>\n",
              "      <td>0.097007</td>\n",
              "      <td>-4</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X13</th>\n",
              "      <td>0.036673</td>\n",
              "      <td>0.028924</td>\n",
              "      <td>0.006392</td>\n",
              "      <td>0.025019</td>\n",
              "      <td>0.041178</td>\n",
              "      <td>0.046844</td>\n",
              "      <td>0.027643</td>\n",
              "      <td>0.043142</td>\n",
              "      <td>0.049434</td>\n",
              "      <td>-1</td>\n",
              "      <td>-5</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X14</th>\n",
              "      <td>0.220115</td>\n",
              "      <td>0.109699</td>\n",
              "      <td>0.033513</td>\n",
              "      <td>0.101533</td>\n",
              "      <td>0.105407</td>\n",
              "      <td>0.120828</td>\n",
              "      <td>0.159667</td>\n",
              "      <td>0.120767</td>\n",
              "      <td>0.123132</td>\n",
              "      <td>-3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     MI_Reg_Scores  MIC_Bal_Scores  MIC_Unbal_Scores  ...  RFE  RFE_bal  RFE_unbal\n",
              "X1        0.117499        0.061386          0.007352  ...   -1       -1         -1\n",
              "X2        0.155141        0.111060          0.011942  ...   -1       -1         -1\n",
              "X3        0.064104        0.006320          0.010933  ...   -1       -1         -6\n",
              "X4        0.239914        0.113995          0.042207  ...   -1       -1         -1\n",
              "X5        0.059098        0.000000          0.015932  ...   -1       -1         -1\n",
              "X6        0.006906        0.000000          0.000000  ...   -8       -4         -4\n",
              "X7        0.000000        0.000000          0.000000  ...   -5       -1         -5\n",
              "X8        0.000000        0.000000          0.006586  ...   -7       -2         -1\n",
              "X9        0.000000        0.000000          0.000000  ...   -6       -3         -3\n",
              "X10       0.000000        0.018296          0.000000  ...   -9       -1         -1\n",
              "X11       0.069175        0.063916          0.004096  ...   -2       -1         -1\n",
              "X12       0.113326        0.091263          0.011106  ...   -4       -1         -1\n",
              "X13       0.036673        0.028924          0.006392  ...   -1       -5         -2\n",
              "X14       0.220115        0.109699          0.033513  ...   -3       -1         -1\n",
              "\n",
              "[14 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf-zJWLnt7xG",
        "colab_type": "code",
        "outputId": "90ed53fe-21c7-4abe-9339-2234230c5015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_df = scaler.fit_transform(comparisons)\n",
        "scaled_comparisons = pd.DataFrame(scaled_df, columns=comparisons.columns,\n",
        "                                 index=names)\n",
        "\n",
        "scaled_comparisons"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MI_Reg_Scores</th>\n",
              "      <th>MIC_Bal_Scores</th>\n",
              "      <th>MIC_Unbal_Scores</th>\n",
              "      <th>RFR</th>\n",
              "      <th>RFC_bal</th>\n",
              "      <th>RFC_unbal</th>\n",
              "      <th>ETR</th>\n",
              "      <th>ETC_bal</th>\n",
              "      <th>ETC_unbal</th>\n",
              "      <th>RFE</th>\n",
              "      <th>RFE_bal</th>\n",
              "      <th>RFE_unbal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X1</th>\n",
              "      <td>0.489754</td>\n",
              "      <td>0.538498</td>\n",
              "      <td>0.174183</td>\n",
              "      <td>0.582169</td>\n",
              "      <td>0.826039</td>\n",
              "      <td>0.343473</td>\n",
              "      <td>0.603999</td>\n",
              "      <td>0.763221</td>\n",
              "      <td>0.344431</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X2</th>\n",
              "      <td>0.646649</td>\n",
              "      <td>0.974248</td>\n",
              "      <td>0.282945</td>\n",
              "      <td>0.362105</td>\n",
              "      <td>0.752895</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.566836</td>\n",
              "      <td>0.632850</td>\n",
              "      <td>0.492689</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X3</th>\n",
              "      <td>0.267194</td>\n",
              "      <td>0.055444</td>\n",
              "      <td>0.259036</td>\n",
              "      <td>0.078702</td>\n",
              "      <td>0.228816</td>\n",
              "      <td>0.175393</td>\n",
              "      <td>0.114967</td>\n",
              "      <td>0.176431</td>\n",
              "      <td>0.185672</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X4</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.634346</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.975687</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X5</th>\n",
              "      <td>0.246328</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.377462</td>\n",
              "      <td>0.304676</td>\n",
              "      <td>0.453093</td>\n",
              "      <td>0.595752</td>\n",
              "      <td>0.409228</td>\n",
              "      <td>0.449247</td>\n",
              "      <td>0.600828</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X6</th>\n",
              "      <td>0.028786</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013196</td>\n",
              "      <td>0.001051</td>\n",
              "      <td>0.011160</td>\n",
              "      <td>0.029614</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004309</td>\n",
              "      <td>0.069659</td>\n",
              "      <td>0.208686</td>\n",
              "      <td>0.001758</td>\n",
              "      <td>0.012594</td>\n",
              "      <td>0.031120</td>\n",
              "      <td>0.500</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156048</td>\n",
              "      <td>0.000858</td>\n",
              "      <td>0.075885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022159</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004340</td>\n",
              "      <td>0.031391</td>\n",
              "      <td>0.100341</td>\n",
              "      <td>0.004038</td>\n",
              "      <td>0.038404</td>\n",
              "      <td>0.098240</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001690</td>\n",
              "      <td>0.070454</td>\n",
              "      <td>0.352047</td>\n",
              "      <td>0.003388</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083679</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X11</th>\n",
              "      <td>0.288332</td>\n",
              "      <td>0.560691</td>\n",
              "      <td>0.097037</td>\n",
              "      <td>0.230326</td>\n",
              "      <td>0.424361</td>\n",
              "      <td>0.337810</td>\n",
              "      <td>0.530301</td>\n",
              "      <td>0.675167</td>\n",
              "      <td>0.380003</td>\n",
              "      <td>0.875</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X12</th>\n",
              "      <td>0.472359</td>\n",
              "      <td>0.800586</td>\n",
              "      <td>0.263120</td>\n",
              "      <td>0.391868</td>\n",
              "      <td>0.588969</td>\n",
              "      <td>0.483670</td>\n",
              "      <td>0.506963</td>\n",
              "      <td>0.729360</td>\n",
              "      <td>0.674365</td>\n",
              "      <td>0.625</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X13</th>\n",
              "      <td>0.152859</td>\n",
              "      <td>0.253733</td>\n",
              "      <td>0.151435</td>\n",
              "      <td>0.069169</td>\n",
              "      <td>0.181012</td>\n",
              "      <td>0.154714</td>\n",
              "      <td>0.104582</td>\n",
              "      <td>0.145540</td>\n",
              "      <td>0.081413</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X14</th>\n",
              "      <td>0.917472</td>\n",
              "      <td>0.962307</td>\n",
              "      <td>0.794023</td>\n",
              "      <td>0.353618</td>\n",
              "      <td>0.601025</td>\n",
              "      <td>0.808623</td>\n",
              "      <td>0.787398</td>\n",
              "      <td>0.819454</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.750</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     MI_Reg_Scores  MIC_Bal_Scores  MIC_Unbal_Scores  ...    RFE  RFE_bal  RFE_unbal\n",
              "X1        0.489754        0.538498          0.174183  ...  1.000     1.00        1.0\n",
              "X2        0.646649        0.974248          0.282945  ...  1.000     1.00        1.0\n",
              "X3        0.267194        0.055444          0.259036  ...  1.000     1.00        0.0\n",
              "X4        1.000000        1.000000          1.000000  ...  1.000     1.00        1.0\n",
              "X5        0.246328        0.000000          0.377462  ...  1.000     1.00        1.0\n",
              "X6        0.028786        0.000000          0.000000  ...  0.125     0.25        0.4\n",
              "X7        0.000000        0.000000          0.000000  ...  0.500     1.00        0.2\n",
              "X8        0.000000        0.000000          0.156048  ...  0.250     0.75        1.0\n",
              "X9        0.000000        0.000000          0.000000  ...  0.375     0.50        0.6\n",
              "X10       0.000000        0.160500          0.000000  ...  0.000     1.00        1.0\n",
              "X11       0.288332        0.560691          0.097037  ...  0.875     1.00        1.0\n",
              "X12       0.472359        0.800586          0.263120  ...  0.625     1.00        1.0\n",
              "X13       0.152859        0.253733          0.151435  ...  1.000     0.00        0.8\n",
              "X14       0.917472        0.962307          0.794023  ...  0.750     1.00        1.0\n",
              "\n",
              "[14 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOOPHKCkv0XE",
        "colab_type": "text"
      },
      "source": [
        "### What do you notice from the diagram below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI33JVjEvUKh",
        "colab_type": "code",
        "outputId": "8c2539ec-6b40-44d2-b2bf-22fc3d9d7eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        }
      },
      "source": [
        "plt.gcf().set_size_inches(10, 10)\n",
        "sns.heatmap(scaled_comparisons);"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAKTCAYAAADYN0gwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trnLEttcbPx2",
        "colab_type": "text"
      },
      "source": [
        "Head, Tim [Cross Validation Gone Wrong](https://betatim.github.io/posts/cross-validation-gone-wrong/)\n",
        "\n",
        ">Choosing your input features is just one of the many choices you have to make when building your machine-learning application. Remember to make all decisions during the cross validation, otherwise you are in for a rude awakening when your model is confronted with unseen data for the first time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlcV0N7lbPx4",
        "colab_type": "text"
      },
      "source": [
        "![Image of Machine Learning](https://www.capgemini.com/wp-content/uploads/2017/07/machinelearning_v2.png)\n",
        "\n",
        "\\[Image by Angarita, Natalia for [Capgemini](https://www.capgemini.com/2016/05/machine-learning-has-transformed-many-aspects-of-our-everyday-life/)\\]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nfX15mDbPx7",
        "colab_type": "text"
      },
      "source": [
        "**I don't fully support this diagram for a few reasons.**\n",
        "\n",
        "* I would replace \"Feature Engineering\" with \"Data Cleaning\"\n",
        "* Feature engineering can be done alongside either data cleaning or training your model--it can be done before *or* after splitting your data. (But it will need to be part of the final pipeline.)\n",
        "* Any feature standardization happens **after** the split\n",
        "* And you can use cross validation instead of an independent validation set\n",
        "\n",
        "However **feature selection (Goal 1) is part of choosing and training a model and should happen *after* splitting**. Feature selection belongs safely **inside the dotted line**.\n",
        "\n",
        "\"But doesn't it make sense to make your decisions based on all the information?\"\n",
        "\n",
        "NO! Mr. Head has a point!\n",
        "\n",
        "## The number of features you end up using *is* a hyperparameter. Don't cross the dotted line while hyperparameter tuning!!! Work on goal 1 AFTER splitting.\n",
        "\n",
        "I know you want to see how your model is performing... \"just real quick\"... but don't do it!\n",
        "\n",
        "...\n",
        "\n",
        "Don't!\n",
        "\n",
        "*(Kaggle does the initial train-test split for you. It doesn't even let you **see** the target values for the test data. How you like dem apples?)*\n",
        "\n",
        "![](https://media.giphy.com/media/3o7TKGoQ8721rQQ0es/giphy.gif)\n",
        "\n",
        "What you **can** do is create multiple \"final\" models by hyperparameter tuning different types of models (all inside the dotted line!), then use the final hold-out test to see which does best.\n",
        "\n",
        "**All this is said with the caveat that you have a large enough dataset to support three way validation or a test set plus cross-validation**\n",
        "\n",
        "## On the flip side, feature *interpretation* (Goal 2) can be done with all the data, before splitting, since you are looking to get a full understanding underlying the relationships in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc6bcXVrbPx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}