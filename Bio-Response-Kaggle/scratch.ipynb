{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, log_loss\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "import category_encoders as ce\n",
    "from numpy.testing import assert_almost_equal\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1776) (751, 1776) (3000,) (751,)\n"
     ]
    }
   ],
   "source": [
    "## KAGGLE bioresponse https://www.kaggle.com/c/bioresponse#Evaluation\n",
    "\n",
    "train_url = 'data/train.csv'\n",
    "test_url = 'data/test.csv' ## iggnoring-- it doesn't have 'Activity' \n",
    "\n",
    "df_ = pd.read_csv(train_url)\n",
    "#df_test = pd.read_csv(test_url) # doesn't have 'Activity'\n",
    "assert all([x==0 for x in df_.isna().sum().values])\n",
    "assert all([pd.api.types.is_numeric_dtype(df_[feat]) for feat in df_.columns])\n",
    "dependent='Activity'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_.drop(dependent, axis=1), \n",
    "                                                    df_[dependent], \n",
    "                                                    train_size=0.8, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# df_.dtypes.value_counts()\n",
    "\n",
    "# #df_.describe()\n",
    "\n",
    "# df_.mean().array.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipeline1 = make_pipeline(\n",
    "#     #ce.OneHotEncoder(use_cat_names=True), # no categoricals. \n",
    "#     #StandardScaler(), # means are very much near zero anyway, we don't really need this. \n",
    "#     LogisticRegression()\n",
    "# )\n",
    "\n",
    "\n",
    "# pipeline1.fit(X_train, y_train)\n",
    "# y_pred = pipeline1.predict(X_test)\n",
    "# #accuracy_score(y_test, y_pred)\n",
    "# log_loss(y_test, y_pred)\n",
    "\n",
    "# scores = cross_val_score(pipeline1, X_train, y_train, cv=10, scoring='neg_log_loss') \n",
    "\n",
    "# scores#.mean() # don't run this a lot because it is EXPENSIVE\n",
    "\n",
    "# # when test_size=0.15, scores.mean is better than accuracy_score... but when test_size=0.2, scores.mena is WORSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The above is the \"dumbest possible model\" we start with. Now we're going to play more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://scikit-learn.org/stable/modules/compose.html#pipeline\n",
    "\n",
    "\n",
    "# # We create the preprocessing pipelines for both numeric and categorical data.\n",
    "# numeric_features = ['age', 'fare']\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler())])\n",
    "\n",
    "# categorical_features = ['embarked', 'sex', 'pclass']\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# # Append classifier to preprocessing pipeline.\n",
    "# # Now we have a full prediction pipeline.\n",
    "# clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                       ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets dimension-reduction\n",
    "\n",
    "we'll split the data into two different things, ints and floats. \n",
    "\n",
    "a commenter [here](https://stats.stackexchange.com/questions/159705/would-pca-work-for-boolean-binary-data-types) said that a \"cosine\"ish version of PCA is more appropriate for bool 0,1. \n",
    "\n",
    "### also, We're gonna use the heuristic that, for N=#observations and M=#features, N > 5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3751, 834) (3751, 942)\n"
     ]
    }
   ],
   "source": [
    "ints = df_.drop(dependent, axis=1).select_dtypes(include='int')\n",
    "floats = df_.drop(dependent, axis=1).select_dtypes(include='float')\n",
    "\n",
    "print(ints.shape, floats.shape)\n",
    "\n",
    "B = 5.15\n",
    "\n",
    "ints_weight = np.divide(ints.shape[1], ints.shape[1] + floats.shape[1] + 1)\n",
    "floats_weight = np.divide(floats.shape[1], ints.shape[1] + floats.shape[1] + 1)\n",
    "assert_almost_equal(ints_weight+floats_weight, 1, 3)\n",
    "\n",
    "n = int(np.divide(df_.shape[0], B))\n",
    "\n",
    "LogiR = ('logistic_regression', SGDClassifier(loss='log', tol=np.exp(-B/2), max_iter=1234))\n",
    "\n",
    "estimators_c = [('PCA', PCA(n_components=int(floats_weight * n))), \n",
    "                LogiR]\n",
    "\n",
    "pipe_c = Pipeline(steps=estimators_c)\n",
    "\n",
    "estimators_d = [('FA', FeatureAgglomeration(n_clusters=int(ints_weight * n), affinity='cosine', linkage='complete')), \n",
    "                LogiR]\n",
    "\n",
    "pipe_d = Pipeline(steps=estimators_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 834) (751, 834) (3000, 942) (751, 942)\n"
     ]
    }
   ],
   "source": [
    "ints_train = X_train.select_dtypes(include='int')\n",
    "ints_test = X_test.select_dtypes(include='int')\n",
    "floats_train = X_train.select_dtypes(include='float')\n",
    "floats_test = X_test.select_dtypes(include='float')\n",
    "\n",
    "print(ints_train.shape, ints_test.shape, floats_train.shape, floats_test.shape)\n",
    "\n",
    "pipe_c.fit(floats_train, y_train)\n",
    "y_pred_c = pipe_c.predict_proba(floats_test)\n",
    "\n",
    "pipe_d.fit(ints_train, y_train)\n",
    "y_pred_d = pipe_d.predict_proba(ints_test)\n",
    "\n",
    "\n",
    "# log_loss(y_test, y_pred_c), log_loss(y_test, y_pred_d)\n",
    "\n",
    "#y_pred_d.shape, y_pred_c.shape, y_test.shape, df_.shape\n",
    "\n",
    "#y_pred_c, y_pred_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7683930364476979 2.2833224568938246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7361823955874933"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(log_loss(y_test, y_pred_c), log_loss(y_test, y_pred_d))\n",
    "\n",
    "def weighted_pred(p1, y1, p2, y2, epsilon=7): \n",
    "    assert_almost_equal(p1+p2, 1, epsilon)\n",
    "    return p1*y1 + p2*y2\n",
    "\n",
    "y_pred = weighted_pred(ints_weight, y_pred_d, floats_weight, y_pred_c, epsilon=3)\n",
    "\n",
    "log_loss(y_test, y_pred) ## an improvement! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6652527725861623"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_c = cross_val_score(pipe_c, floats_train, y_train, cv=8, scoring='neg_log_loss') \n",
    "\n",
    "#scores_d = cross_val_score(pipe_d, ints_train, y_train, cv=5, scoring='neg_log_loss') \n",
    "\n",
    "-scores_c.mean()\n",
    "\n",
    "## ok so when I \n",
    "## get rid of the discretes it doesn't get an error. \n",
    "# i don't understand SGDClassifier well enough. \n",
    "# or feature aggregation. \n",
    "\n",
    "# but these are.... not bad??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b4064d038c17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNCm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "np.arange(b,NCm, a**b), np.logspace(-b**b, b**b, a),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
