{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_243_Select_models_and_parameters.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chadeowen/DS-Unit-2-Sprint-4-Model-Validation/blob/master/module-3-select-models-and-parameters/LS_DS_243_Select_models_and_parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "O67uhlT4MExK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_Lambda School Data Science — Model Validation_\n",
        "\n",
        "# Select models and parameters\n",
        "\n",
        "Objectives\n",
        "- Hyperparameter optimization\n",
        "- Model selection"
      ]
    },
    {
      "metadata": {
        "id": "VE4rfZd4NUGA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Today we'll use this process:\n",
        "\n",
        "## \"A universal workflow of machine learning\"\n",
        "\n",
        "_Excerpt from Francois Chollet, [Deep Learning with Python](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/README.md), Chapter 4: Fundamentals of machine learning_\n",
        " \n",
        "**1. Define the problem at hand and the data on which you’ll train.** Collect this data, or annotate it with labels if need be.\n",
        "\n",
        "**2. Choose how you’ll measure success on your problem.** Which metrics will you monitor on your validation data?\n",
        "\n",
        "**3. Determine your evaluation protocol:** hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
        "\n",
        "**4. Develop a first model that does better than a basic baseline:** a model with statistical power.\n",
        "\n",
        "**5. Develop a model that overfits.** The universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\n",
        "\n",
        "**6. Regularize your model and tune its hyperparameters, based on performance on the validation data.** Repeatedly modify your model, train it, evaluate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. \n",
        "\n",
        "Iterate on feature engineering: add new features, or remove features that don’t seem to be informative. Once you’ve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set.\n"
      ]
    },
    {
      "metadata": {
        "id": "3kt6bzEcOIaa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Define the problem at hand and the data on which you'll train"
      ]
    },
    {
      "metadata": {
        "id": "di16k7vpRg67",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll apply the workflow to a [project from _Python Data Science Handbook_](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Example:-Predicting-Bicycle-Traffic) by Jake VanderPlas:\n",
        "\n",
        "> **Predicting Bicycle Traffic**\n",
        "\n",
        "> As an example, let's take a look at whether we can predict the number of bicycle trips across Seattle's Fremont Bridge based on weather, season, and other factors.\n",
        "\n",
        "> We will join the bike data with another dataset, and try to determine the extent to which weather and seasonal factors—temperature, precipitation, and daylight hours—affect the volume of bicycle traffic through this corridor. Fortunately, the NOAA makes available their daily [weather station data](http://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND) (I used station ID USW00024233) and we can easily use Pandas to join the two data sources.\n",
        "\n",
        "> Let's start by loading the two datasets, indexing by date:"
      ]
    },
    {
      "metadata": {
        "id": "19dpb_d0R1A6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So this is a regression problem, not a classification problem. We'll define the target, choose an evaluation metric, and choose models that are appropriate for regression problems.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "os1zruXQ30KM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download data"
      ]
    },
    {
      "metadata": {
        "id": "5XVu-HSeMDtV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "24c2eff5-1fb1-4e68-89d1-58f82bce2228"
      },
      "cell_type": "code",
      "source": [
        "!curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1536k    0 1536k    0     0   219k      0 --:--:--  0:00:06 --:--:--  343k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sih_7mTzMdfr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e81460ee-2693-4976-de21-7b893c9f47d6"
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/data/BicycleWeather.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-30 17:19:34--  https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/data/BicycleWeather.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 234945 (229K) [text/plain]\n",
            "Saving to: ‘BicycleWeather.csv’\n",
            "\n",
            "\rBicycleWeather.csv    0%[                    ]       0  --.-KB/s               \rBicycleWeather.csv  100%[===================>] 229.44K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-01-30 17:19:34 (7.59 MB/s) - ‘BicycleWeather.csv’ saved [234945/234945]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9GYm74kD34OQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ]
    },
    {
      "metadata": {
        "id": "BfQ7gE28MNdF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Modified from cells 15, 16, and 20, at\n",
        "# https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Example:-Predicting-Bicycle-Traffic\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "counts = pd.read_csv('FremontBridge.csv', index_col='Date', parse_dates=True, \n",
        "                     infer_datetime_format=True)\n",
        "\n",
        "weather = pd.read_csv('BicycleWeather.csv', index_col='DATE', parse_dates=True, \n",
        "                      infer_datetime_format=True)\n",
        "\n",
        "daily = counts.resample('d').sum()\n",
        "daily['Total'] = daily.sum(axis=1)\n",
        "daily = daily[['Total']] # remove other columns\n",
        "\n",
        "weather_columns = ['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'AWND']\n",
        "daily = daily.join(weather[weather_columns], how='inner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i0YYD6rvypb4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make a feature for yesterday's total\n",
        "daily['Total_yesterday'] = daily.Total.shift(1)\n",
        "\n",
        "daily = daily.drop(index=daily.index[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VVB3g4704An5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### First fast look at the data\n",
        "- What's the shape?\n",
        "- What's the date range?\n",
        "- What's the target and the features?"
      ]
    },
    {
      "metadata": {
        "id": "t50E2fTUWBBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80f7520f-b1c3-4d55-f6b0-56e294d16916"
      },
      "cell_type": "code",
      "source": [
        "daily.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1063, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "KlJqmx5Rx-ER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5f4fb26e-45e9-4bbc-92c8-c7db614b76bd"
      },
      "cell_type": "code",
      "source": [
        "daily.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total</th>\n",
              "      <th>PRCP</th>\n",
              "      <th>SNOW</th>\n",
              "      <th>SNWD</th>\n",
              "      <th>TMAX</th>\n",
              "      <th>TMIN</th>\n",
              "      <th>AWND</th>\n",
              "      <th>Total_yesterday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2012-10-04</th>\n",
              "      <td>3475.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>189</td>\n",
              "      <td>83</td>\n",
              "      <td>65</td>\n",
              "      <td>3521.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-05</th>\n",
              "      <td>3148.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>217</td>\n",
              "      <td>89</td>\n",
              "      <td>57</td>\n",
              "      <td>3475.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-06</th>\n",
              "      <td>2006.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>239</td>\n",
              "      <td>78</td>\n",
              "      <td>51</td>\n",
              "      <td>3148.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-07</th>\n",
              "      <td>2142.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>239</td>\n",
              "      <td>78</td>\n",
              "      <td>13</td>\n",
              "      <td>2006.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-08</th>\n",
              "      <td>3537.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211</td>\n",
              "      <td>78</td>\n",
              "      <td>19</td>\n",
              "      <td>2142.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Total  PRCP  SNOW  SNWD  TMAX  TMIN  AWND  Total_yesterday\n",
              "2012-10-04  3475.0     0     0     0   189    83    65           3521.0\n",
              "2012-10-05  3148.0     0     0     0   217    89    57           3475.0\n",
              "2012-10-06  2006.0     0     0     0   239    78    51           3148.0\n",
              "2012-10-07  2142.0     0     0     0   239    78    13           2006.0\n",
              "2012-10-08  3537.0     0     0     0   211    78    19           2142.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "ne-GZsK2x_48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1144fc90-75bb-4723-d295-ae2bf0fc0937"
      },
      "cell_type": "code",
      "source": [
        "daily.tail()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total</th>\n",
              "      <th>PRCP</th>\n",
              "      <th>SNOW</th>\n",
              "      <th>SNWD</th>\n",
              "      <th>TMAX</th>\n",
              "      <th>TMIN</th>\n",
              "      <th>AWND</th>\n",
              "      <th>Total_yesterday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-08-28</th>\n",
              "      <td>2653.0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>233</td>\n",
              "      <td>156</td>\n",
              "      <td>26</td>\n",
              "      <td>4336.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-08-29</th>\n",
              "      <td>699.0</td>\n",
              "      <td>325</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>222</td>\n",
              "      <td>133</td>\n",
              "      <td>58</td>\n",
              "      <td>2653.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-08-30</th>\n",
              "      <td>1213.0</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>200</td>\n",
              "      <td>128</td>\n",
              "      <td>47</td>\n",
              "      <td>699.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-08-31</th>\n",
              "      <td>2823.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>189</td>\n",
              "      <td>161</td>\n",
              "      <td>58</td>\n",
              "      <td>1213.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-09-01</th>\n",
              "      <td>2876.0</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>194</td>\n",
              "      <td>139</td>\n",
              "      <td>-9999</td>\n",
              "      <td>2823.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Total  PRCP  SNOW  SNWD  TMAX  TMIN  AWND  Total_yesterday\n",
              "2015-08-28  2653.0     5     0     0   233   156    26           4336.0\n",
              "2015-08-29   699.0   325     0     0   222   133    58           2653.0\n",
              "2015-08-30  1213.0   102     0     0   200   128    47            699.0\n",
              "2015-08-31  2823.0     0     0     0   189   161    58           1213.0\n",
              "2015-09-01  2876.0    58     0     0   194   139 -9999           2823.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "CqNCrd6Ez6Ew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "2d1aa7f5-fbdf-41de-cd47-0a748570df7d"
      },
      "cell_type": "code",
      "source": [
        "daily.describe()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total</th>\n",
              "      <th>PRCP</th>\n",
              "      <th>SNOW</th>\n",
              "      <th>SNWD</th>\n",
              "      <th>TMAX</th>\n",
              "      <th>TMIN</th>\n",
              "      <th>AWND</th>\n",
              "      <th>Total_yesterday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1063.000000</td>\n",
              "      <td>1063.000000</td>\n",
              "      <td>1063.000000</td>\n",
              "      <td>1063.000000</td>\n",
              "      <td>1063.000000</td>\n",
              "      <td>1063.000000</td>\n",
              "      <td>1063.000000</td>\n",
              "      <td>1063.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2632.449671</td>\n",
              "      <td>29.350894</td>\n",
              "      <td>-37.496707</td>\n",
              "      <td>0.098777</td>\n",
              "      <td>166.863594</td>\n",
              "      <td>84.472248</td>\n",
              "      <td>22.338664</td>\n",
              "      <td>2633.056444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1252.864020</td>\n",
              "      <td>65.813053</td>\n",
              "      <td>612.512583</td>\n",
              "      <td>2.570041</td>\n",
              "      <td>74.779734</td>\n",
              "      <td>50.916006</td>\n",
              "      <td>307.984292</td>\n",
              "      <td>1253.138245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>98.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9999.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-16.000000</td>\n",
              "      <td>-71.000000</td>\n",
              "      <td>-9999.000000</td>\n",
              "      <td>98.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1806.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>1806.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2435.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>2435.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3574.500000</td>\n",
              "      <td>26.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>3574.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>6088.000000</td>\n",
              "      <td>559.000000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>356.000000</td>\n",
              "      <td>183.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>6088.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Total         PRCP         SNOW         SNWD         TMAX  \\\n",
              "count  1063.000000  1063.000000  1063.000000  1063.000000  1063.000000   \n",
              "mean   2632.449671    29.350894   -37.496707     0.098777   166.863594   \n",
              "std    1252.864020    65.813053   612.512583     2.570041    74.779734   \n",
              "min      98.000000     0.000000 -9999.000000     0.000000   -16.000000   \n",
              "25%    1806.000000     0.000000     0.000000     0.000000   111.000000   \n",
              "50%    2435.000000     0.000000     0.000000     0.000000   150.000000   \n",
              "75%    3574.500000    26.500000     0.000000     0.000000   222.000000   \n",
              "max    6088.000000   559.000000    74.000000    80.000000   356.000000   \n",
              "\n",
              "              TMIN         AWND  Total_yesterday  \n",
              "count  1063.000000  1063.000000      1063.000000  \n",
              "mean     84.472248    22.338664      2633.056444  \n",
              "std      50.916006   307.984292      1253.138245  \n",
              "min     -71.000000 -9999.000000        98.000000  \n",
              "25%      44.000000    22.000000      1806.000000  \n",
              "50%      83.000000    29.000000      2435.000000  \n",
              "75%     128.000000    40.000000      3574.500000  \n",
              "max     183.000000    95.000000      6088.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "K7GkQ3FQ09z5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "fc533871-7b73-427b-c203-a69cb0707d07"
      },
      "cell_type": "code",
      "source": [
        "# show everything BUT last 100 days of data\n",
        "\n",
        "daily[:-100]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total</th>\n",
              "      <th>PRCP</th>\n",
              "      <th>SNOW</th>\n",
              "      <th>SNWD</th>\n",
              "      <th>TMAX</th>\n",
              "      <th>TMIN</th>\n",
              "      <th>AWND</th>\n",
              "      <th>Total_yesterday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2012-10-04</th>\n",
              "      <td>3475.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>189</td>\n",
              "      <td>83</td>\n",
              "      <td>65</td>\n",
              "      <td>3521.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-05</th>\n",
              "      <td>3148.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>217</td>\n",
              "      <td>89</td>\n",
              "      <td>57</td>\n",
              "      <td>3475.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-06</th>\n",
              "      <td>2006.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>239</td>\n",
              "      <td>78</td>\n",
              "      <td>51</td>\n",
              "      <td>3148.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-07</th>\n",
              "      <td>2142.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>239</td>\n",
              "      <td>78</td>\n",
              "      <td>13</td>\n",
              "      <td>2006.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-08</th>\n",
              "      <td>3537.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211</td>\n",
              "      <td>78</td>\n",
              "      <td>19</td>\n",
              "      <td>2142.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-09</th>\n",
              "      <td>3501.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>161</td>\n",
              "      <td>89</td>\n",
              "      <td>16</td>\n",
              "      <td>3537.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-10</th>\n",
              "      <td>3235.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>122</td>\n",
              "      <td>83</td>\n",
              "      <td>14</td>\n",
              "      <td>3501.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-11</th>\n",
              "      <td>3047.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>139</td>\n",
              "      <td>72</td>\n",
              "      <td>13</td>\n",
              "      <td>3235.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-12</th>\n",
              "      <td>2011.0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>139</td>\n",
              "      <td>89</td>\n",
              "      <td>46</td>\n",
              "      <td>3047.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-13</th>\n",
              "      <td>766.0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>156</td>\n",
              "      <td>122</td>\n",
              "      <td>39</td>\n",
              "      <td>2011.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-14</th>\n",
              "      <td>698.0</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>178</td>\n",
              "      <td>133</td>\n",
              "      <td>34</td>\n",
              "      <td>766.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-15</th>\n",
              "      <td>2273.0</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>111</td>\n",
              "      <td>46</td>\n",
              "      <td>698.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-16</th>\n",
              "      <td>3036.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>161</td>\n",
              "      <td>83</td>\n",
              "      <td>55</td>\n",
              "      <td>2273.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-17</th>\n",
              "      <td>3243.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>61</td>\n",
              "      <td>16</td>\n",
              "      <td>3036.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-18</th>\n",
              "      <td>2923.0</td>\n",
              "      <td>208</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>178</td>\n",
              "      <td>67</td>\n",
              "      <td>20</td>\n",
              "      <td>3243.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-19</th>\n",
              "      <td>1977.0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>94</td>\n",
              "      <td>53</td>\n",
              "      <td>2923.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-20</th>\n",
              "      <td>1068.0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>61</td>\n",
              "      <td>57</td>\n",
              "      <td>1977.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-21</th>\n",
              "      <td>989.0</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>117</td>\n",
              "      <td>44</td>\n",
              "      <td>27</td>\n",
              "      <td>1068.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-22</th>\n",
              "      <td>2129.0</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>78</td>\n",
              "      <td>33</td>\n",
              "      <td>26</td>\n",
              "      <td>989.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-23</th>\n",
              "      <td>2500.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>56</td>\n",
              "      <td>30</td>\n",
              "      <td>2129.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-24</th>\n",
              "      <td>2429.0</td>\n",
              "      <td>71</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>117</td>\n",
              "      <td>61</td>\n",
              "      <td>21</td>\n",
              "      <td>2500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-25</th>\n",
              "      <td>2713.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>117</td>\n",
              "      <td>67</td>\n",
              "      <td>15</td>\n",
              "      <td>2429.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-26</th>\n",
              "      <td>2073.0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>72</td>\n",
              "      <td>25</td>\n",
              "      <td>2713.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-27</th>\n",
              "      <td>531.0</td>\n",
              "      <td>231</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>94</td>\n",
              "      <td>51</td>\n",
              "      <td>2073.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-28</th>\n",
              "      <td>1062.0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>100</td>\n",
              "      <td>38</td>\n",
              "      <td>531.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-29</th>\n",
              "      <td>2217.0</td>\n",
              "      <td>109</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>156</td>\n",
              "      <td>100</td>\n",
              "      <td>49</td>\n",
              "      <td>1062.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-30</th>\n",
              "      <td>1735.0</td>\n",
              "      <td>345</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>122</td>\n",
              "      <td>28</td>\n",
              "      <td>2217.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-10-31</th>\n",
              "      <td>1710.0</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>156</td>\n",
              "      <td>111</td>\n",
              "      <td>27</td>\n",
              "      <td>1735.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-11-01</th>\n",
              "      <td>2091.0</td>\n",
              "      <td>97</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>106</td>\n",
              "      <td>30</td>\n",
              "      <td>1710.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-11-02</th>\n",
              "      <td>2478.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>106</td>\n",
              "      <td>10</td>\n",
              "      <td>2091.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-04-25</th>\n",
              "      <td>1616.0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>133</td>\n",
              "      <td>56</td>\n",
              "      <td>30</td>\n",
              "      <td>2276.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-04-26</th>\n",
              "      <td>1566.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>156</td>\n",
              "      <td>44</td>\n",
              "      <td>27</td>\n",
              "      <td>1616.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-04-27</th>\n",
              "      <td>4140.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>250</td>\n",
              "      <td>106</td>\n",
              "      <td>23</td>\n",
              "      <td>1566.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-04-28</th>\n",
              "      <td>2745.0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>156</td>\n",
              "      <td>89</td>\n",
              "      <td>43</td>\n",
              "      <td>4140.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-04-29</th>\n",
              "      <td>3437.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>161</td>\n",
              "      <td>72</td>\n",
              "      <td>47</td>\n",
              "      <td>2745.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-04-30</th>\n",
              "      <td>3608.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>78</td>\n",
              "      <td>21</td>\n",
              "      <td>3437.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-01</th>\n",
              "      <td>4224.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>183</td>\n",
              "      <td>89</td>\n",
              "      <td>37</td>\n",
              "      <td>3608.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-02</th>\n",
              "      <td>2215.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>183</td>\n",
              "      <td>78</td>\n",
              "      <td>37</td>\n",
              "      <td>4224.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-03</th>\n",
              "      <td>2235.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>206</td>\n",
              "      <td>78</td>\n",
              "      <td>26</td>\n",
              "      <td>2215.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-04</th>\n",
              "      <td>4520.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>72</td>\n",
              "      <td>52</td>\n",
              "      <td>2235.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-05</th>\n",
              "      <td>2605.0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>72</td>\n",
              "      <td>51</td>\n",
              "      <td>4520.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-06</th>\n",
              "      <td>4231.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>167</td>\n",
              "      <td>72</td>\n",
              "      <td>26</td>\n",
              "      <td>2605.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-07</th>\n",
              "      <td>4916.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>206</td>\n",
              "      <td>61</td>\n",
              "      <td>30</td>\n",
              "      <td>4231.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-08</th>\n",
              "      <td>4452.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>239</td>\n",
              "      <td>83</td>\n",
              "      <td>30</td>\n",
              "      <td>4916.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-09</th>\n",
              "      <td>2753.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>267</td>\n",
              "      <td>94</td>\n",
              "      <td>26</td>\n",
              "      <td>4452.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-10</th>\n",
              "      <td>1958.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>194</td>\n",
              "      <td>111</td>\n",
              "      <td>28</td>\n",
              "      <td>2753.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-11</th>\n",
              "      <td>3676.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>139</td>\n",
              "      <td>100</td>\n",
              "      <td>25</td>\n",
              "      <td>1958.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-12</th>\n",
              "      <td>3653.0</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>156</td>\n",
              "      <td>106</td>\n",
              "      <td>33</td>\n",
              "      <td>3676.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-13</th>\n",
              "      <td>2707.0</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>122</td>\n",
              "      <td>100</td>\n",
              "      <td>28</td>\n",
              "      <td>3653.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-14</th>\n",
              "      <td>3811.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>178</td>\n",
              "      <td>94</td>\n",
              "      <td>20</td>\n",
              "      <td>2707.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-15</th>\n",
              "      <td>5053.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>200</td>\n",
              "      <td>94</td>\n",
              "      <td>28</td>\n",
              "      <td>3811.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-16</th>\n",
              "      <td>1655.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>156</td>\n",
              "      <td>111</td>\n",
              "      <td>30</td>\n",
              "      <td>5053.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-17</th>\n",
              "      <td>2011.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>194</td>\n",
              "      <td>106</td>\n",
              "      <td>21</td>\n",
              "      <td>1655.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-18</th>\n",
              "      <td>4845.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>256</td>\n",
              "      <td>122</td>\n",
              "      <td>30</td>\n",
              "      <td>2011.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-19</th>\n",
              "      <td>4872.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>217</td>\n",
              "      <td>117</td>\n",
              "      <td>26</td>\n",
              "      <td>4845.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-20</th>\n",
              "      <td>4580.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>233</td>\n",
              "      <td>106</td>\n",
              "      <td>18</td>\n",
              "      <td>4872.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-21</th>\n",
              "      <td>4768.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>256</td>\n",
              "      <td>117</td>\n",
              "      <td>21</td>\n",
              "      <td>4580.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-22</th>\n",
              "      <td>3668.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>167</td>\n",
              "      <td>117</td>\n",
              "      <td>37</td>\n",
              "      <td>4768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-23</th>\n",
              "      <td>1580.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>161</td>\n",
              "      <td>117</td>\n",
              "      <td>26</td>\n",
              "      <td>3668.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-05-24</th>\n",
              "      <td>1776.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>178</td>\n",
              "      <td>111</td>\n",
              "      <td>27</td>\n",
              "      <td>1580.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>963 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Total  PRCP  SNOW  SNWD  TMAX  TMIN  AWND  Total_yesterday\n",
              "2012-10-04  3475.0     0     0     0   189    83    65           3521.0\n",
              "2012-10-05  3148.0     0     0     0   217    89    57           3475.0\n",
              "2012-10-06  2006.0     0     0     0   239    78    51           3148.0\n",
              "2012-10-07  2142.0     0     0     0   239    78    13           2006.0\n",
              "2012-10-08  3537.0     0     0     0   211    78    19           2142.0\n",
              "2012-10-09  3501.0     0     0     0   161    89    16           3537.0\n",
              "2012-10-10  3235.0     0     0     0   122    83    14           3501.0\n",
              "2012-10-11  3047.0     0     0     0   139    72    13           3235.0\n",
              "2012-10-12  2011.0    20     0     0   139    89    46           3047.0\n",
              "2012-10-13   766.0    48     0     0   156   122    39           2011.0\n",
              "2012-10-14   698.0   165     0     0   178   133    34            766.0\n",
              "2012-10-15  2273.0    79     0     0   172   111    46            698.0\n",
              "2012-10-16  3036.0     0     0     0   161    83    55           2273.0\n",
              "2012-10-17  3243.0     0     0     0   144    61    16           3036.0\n",
              "2012-10-18  2923.0   208     0     0   178    67    20           3243.0\n",
              "2012-10-19  1977.0    48     0     0   150    94    53           2923.0\n",
              "2012-10-20  1068.0     5     0     0   111    61    57           1977.0\n",
              "2012-10-21   989.0    64     0     0   117    44    27           1068.0\n",
              "2012-10-22  2129.0    89     0     0    78    33    26            989.0\n",
              "2012-10-23  2500.0     0     0     0   111    56    30           2129.0\n",
              "2012-10-24  2429.0    71     0     0   117    61    21           2500.0\n",
              "2012-10-25  2713.0     0     0     0   117    67    15           2429.0\n",
              "2012-10-26  2073.0    15     0     0   111    72    25           2713.0\n",
              "2012-10-27   531.0   231     0     0   144    94    51           2073.0\n",
              "2012-10-28  1062.0    61     0     0   144   100    38            531.0\n",
              "2012-10-29  2217.0   109     0     0   156   100    49           1062.0\n",
              "2012-10-30  1735.0   345     0     0   150   122    28           2217.0\n",
              "2012-10-31  1710.0   145     0     0   156   111    27           1735.0\n",
              "2012-11-01  2091.0    97     0     0   150   106    30           1710.0\n",
              "2012-11-02  2478.0    56     0     0   150   106    10           2091.0\n",
              "...            ...   ...   ...   ...   ...   ...   ...              ...\n",
              "2015-04-25  1616.0    13     0     0   133    56    30           2276.0\n",
              "2015-04-26  1566.0     0     0     0   156    44    27           1616.0\n",
              "2015-04-27  4140.0     3     0     0   250   106    23           1566.0\n",
              "2015-04-28  2745.0    18     0     0   156    89    43           4140.0\n",
              "2015-04-29  3437.0     0     0     0   161    72    47           2745.0\n",
              "2015-04-30  3608.0     0     0     0   172    78    21           3437.0\n",
              "2015-05-01  4224.0     0     0     0   183    89    37           3608.0\n",
              "2015-05-02  2215.0     0     0     0   183    78    37           4224.0\n",
              "2015-05-03  2235.0     0     0     0   206    78    26           2215.0\n",
              "2015-05-04  4520.0     0     0     0   172    72    52           2235.0\n",
              "2015-05-05  2605.0    61     0     0   144    72    51           4520.0\n",
              "2015-05-06  4231.0     0     0     0   167    72    26           2605.0\n",
              "2015-05-07  4916.0     0     0     0   206    61    30           4231.0\n",
              "2015-05-08  4452.0     0     0     0   239    83    30           4916.0\n",
              "2015-05-09  2753.0     0     0     0   267    94    26           4452.0\n",
              "2015-05-10  1958.0     0     0     0   194   111    28           2753.0\n",
              "2015-05-11  3676.0     0     0     0   139   100    25           1958.0\n",
              "2015-05-12  3653.0    43     0     0   156   106    33           3676.0\n",
              "2015-05-13  2707.0    41     0     0   122   100    28           3653.0\n",
              "2015-05-14  3811.0     3     0     0   178    94    20           2707.0\n",
              "2015-05-15  5053.0     0     0     0   200    94    28           3811.0\n",
              "2015-05-16  1655.0     0     0     0   156   111    30           5053.0\n",
              "2015-05-17  2011.0     0     0     0   194   106    21           1655.0\n",
              "2015-05-18  4845.0     0     0     0   256   122    30           2011.0\n",
              "2015-05-19  4872.0     0     0     0   217   117    26           4845.0\n",
              "2015-05-20  4580.0     0     0     0   233   106    18           4872.0\n",
              "2015-05-21  4768.0     0     0     0   256   117    21           4580.0\n",
              "2015-05-22  3668.0     0     0     0   167   117    37           4768.0\n",
              "2015-05-23  1580.0     0     0     0   161   117    26           3668.0\n",
              "2015-05-24  1776.0     0     0     0   178   111    27           1580.0\n",
              "\n",
              "[963 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "XgMvCsaWJR7Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Target\n",
        "- Total : Daily total number of bicycle trips across Seattle's Fremont Bridge\n",
        "\n",
        "Features\n",
        "- Date (index) : from 2012-10-04 to 2015-09-01\n",
        "- Total_yesterday : Total trips yesterday\n",
        "- PRCP : Precipitation (1/10 mm)\n",
        "- SNOW : Snowfall (1/10 mm)\n",
        "- SNWD : Snow depth (1/10 mm)\n",
        "- TMAX : Maximum temperature (1/10 Celsius)\n",
        "- TMIN : Minimum temperature (1/10 Celsius)\n",
        "- AWND : Average daily wind speed (1/10 meters per second)"
      ]
    },
    {
      "metadata": {
        "id": "lenL-przSYCo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Choose how you’ll measure success on your problem.\n",
        "\n",
        "Which metrics will you monitor on your validation data?\n",
        "\n",
        "This is a regression problem, so we need to choose a regression [metric](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values).\n",
        "\n",
        "\n",
        "\n",
        "I'll choose mean absolute error.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1TqbomapSyRP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error as mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IRHrB3rsS5hF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Determine your evaluation protocol \n",
        "\n",
        "We're doing **model selection, hyperparameter optimization, and performance estimation.** So generally we have two ideal [options](https://sebastianraschka.com/images/blog/2018/model-evaluation-selection-part4/model-eval-conclusions.jpg) to choose from:\n",
        "\n",
        "- 3-way holdout method (train/validation/test split)\n",
        "- Cross-validation with independent test set\n",
        "\n",
        "I'll choose cross-validation with independent test set. Scikit-learn makes cross-validation convenient for us!\n",
        "\n",
        "Specifically, I will use random shuffled cross validation to train and validate, but I will hold out an \"out-of-time\" test set, from the last 100 days of data:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "A3xo6HgbPMFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = daily[:-100]\n",
        "test = daily[-100:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bW4zIi6Y1QLY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "70ee8e8d-892d-457f-f317-fcaa2fc8aa28"
      },
      "cell_type": "code",
      "source": [
        "train.index"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatetimeIndex(['2012-10-04', '2012-10-05', '2012-10-06', '2012-10-07',\n",
              "               '2012-10-08', '2012-10-09', '2012-10-10', '2012-10-11',\n",
              "               '2012-10-12', '2012-10-13',\n",
              "               ...\n",
              "               '2015-05-15', '2015-05-16', '2015-05-17', '2015-05-18',\n",
              "               '2015-05-19', '2015-05-20', '2015-05-21', '2015-05-22',\n",
              "               '2015-05-23', '2015-05-24'],\n",
              "              dtype='datetime64[ns]', length=963, freq=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "Oez7ah1j1Rn7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "1820368c-d9a5-4fc8-f973-f2dc1a6d2848"
      },
      "cell_type": "code",
      "source": [
        "test.index"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatetimeIndex(['2015-05-25', '2015-05-26', '2015-05-27', '2015-05-28',\n",
              "               '2015-05-29', '2015-05-30', '2015-05-31', '2015-06-01',\n",
              "               '2015-06-02', '2015-06-03', '2015-06-04', '2015-06-05',\n",
              "               '2015-06-06', '2015-06-07', '2015-06-08', '2015-06-09',\n",
              "               '2015-06-10', '2015-06-11', '2015-06-12', '2015-06-13',\n",
              "               '2015-06-14', '2015-06-15', '2015-06-16', '2015-06-17',\n",
              "               '2015-06-18', '2015-06-19', '2015-06-20', '2015-06-21',\n",
              "               '2015-06-22', '2015-06-23', '2015-06-24', '2015-06-25',\n",
              "               '2015-06-26', '2015-06-27', '2015-06-28', '2015-06-29',\n",
              "               '2015-06-30', '2015-07-01', '2015-07-02', '2015-07-03',\n",
              "               '2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07',\n",
              "               '2015-07-08', '2015-07-09', '2015-07-10', '2015-07-11',\n",
              "               '2015-07-12', '2015-07-13', '2015-07-14', '2015-07-15',\n",
              "               '2015-07-16', '2015-07-17', '2015-07-18', '2015-07-19',\n",
              "               '2015-07-20', '2015-07-21', '2015-07-22', '2015-07-23',\n",
              "               '2015-07-24', '2015-07-25', '2015-07-26', '2015-07-27',\n",
              "               '2015-07-28', '2015-07-29', '2015-07-30', '2015-07-31',\n",
              "               '2015-08-01', '2015-08-02', '2015-08-03', '2015-08-04',\n",
              "               '2015-08-05', '2015-08-06', '2015-08-07', '2015-08-08',\n",
              "               '2015-08-09', '2015-08-10', '2015-08-11', '2015-08-12',\n",
              "               '2015-08-13', '2015-08-14', '2015-08-15', '2015-08-16',\n",
              "               '2015-08-17', '2015-08-18', '2015-08-19', '2015-08-20',\n",
              "               '2015-08-21', '2015-08-22', '2015-08-23', '2015-08-24',\n",
              "               '2015-08-25', '2015-08-26', '2015-08-27', '2015-08-28',\n",
              "               '2015-08-29', '2015-08-30', '2015-08-31', '2015-09-01'],\n",
              "              dtype='datetime64[ns]', freq=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "G0G684NM1XDy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = train.drop(columns='Total')\n",
        "y_train = train.Total\n",
        "\n",
        "x_test = test.drop(columns='Total')\n",
        "y_test = test.Total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "moxYtD3b1g9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88c32005-3055-4cff-b3a9-7093066ef259"
      },
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((963, 7), (963,), (100, 7), (100,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "vH6IsORQTvTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Develop a first model that does better than a basic baseline"
      ]
    },
    {
      "metadata": {
        "id": "DJBs2nQkj7oB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Look at the target's distribution and descriptive stats"
      ]
    },
    {
      "metadata": {
        "id": "P5peakv9Zs71",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "e5144c4e-871a-4d24-823c-77ddaccc6390"
      },
      "cell_type": "code",
      "source": [
        "# understand what basic baseline is first\n",
        "# target is decently normally distributed\n",
        "\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.distplot(y_train);"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
            "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
            "  alternative=\"'density'\", removal=\"3.1\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEGCAYAAACHGfl5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "oKB0si_916N2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ec2f5860-9d57-4c35-a2ae-1504f4b7fb57"
      },
      "cell_type": "code",
      "source": [
        "y_train.describe()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     963.000000\n",
              "mean     2534.329180\n",
              "std      1224.065027\n",
              "min        98.000000\n",
              "25%      1755.000000\n",
              "50%      2381.000000\n",
              "75%      3317.500000\n",
              "max      6088.000000\n",
              "Name: Total, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "fEjxxgV9kExY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Basic baseline 1"
      ]
    },
    {
      "metadata": {
        "id": "6GepKdQjYcEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6bdf4095-2334-4e80-a9cf-c034e18c8448"
      },
      "cell_type": "code",
      "source": [
        "# what if predicted 2500 (mean) every time\n",
        "\n",
        "import numpy as np\n",
        "y_pred = np.full(shape=y_train.shape, fill_value=y_train.mean())\n",
        "mse(y_train, y_pred)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "980.8981106765484"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "KaXFkaV52mjg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "off by nearly 1000"
      ]
    },
    {
      "metadata": {
        "id": "tN2I_F3FkIHb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Basic baseline 2"
      ]
    },
    {
      "metadata": {
        "id": "ZW8bhZFtTunV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e53fe879-2497-42b2-e787-190e28b3cb47"
      },
      "cell_type": "code",
      "source": [
        "# guessing that today's trips will replicate yesterday's\n",
        "\n",
        "mse(y_train, X_train.Total_yesterday)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "708.061266874351"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "7PXASbp12xMt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "off by about 700 trips"
      ]
    },
    {
      "metadata": {
        "id": "Ggf3VpxwkJ0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### First model that does better than a basic baseline"
      ]
    },
    {
      "metadata": {
        "id": "KfaqL1Ezer2-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html"
      ]
    },
    {
      "metadata": {
        "id": "OeBtU68skfW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "7567d818-957e-4b7e-80ab-9b668acd77be"
      },
      "cell_type": "code",
      "source": [
        "# use negative mean absolute error because we want to MINIMIZE score,\n",
        "# and cross validate will try to maximize\n",
        "# cv = how many cross validations\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "scores = cross_validate(LinearRegression(), X_train, y_train,\n",
        "                       scoring='neg_mean_absolute_error', cv=3,\n",
        "                       return_train_score=True, return_estimator=True)\n",
        "\n",
        "pd.DataFrame(scores)#.rename(columns={'test_score': 'validation_score'})"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimator</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>score_time</th>\n",
              "      <th>test_score</th>\n",
              "      <th>train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LinearRegression(copy_X=True, fit_intercept=Tr...</td>\n",
              "      <td>0.004383</td>\n",
              "      <td>0.002264</td>\n",
              "      <td>-555.186275</td>\n",
              "      <td>-619.509206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LinearRegression(copy_X=True, fit_intercept=Tr...</td>\n",
              "      <td>0.002115</td>\n",
              "      <td>0.001688</td>\n",
              "      <td>-651.126513</td>\n",
              "      <td>-583.427702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LinearRegression(copy_X=True, fit_intercept=Tr...</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.001552</td>\n",
              "      <td>-615.965800</td>\n",
              "      <td>-589.341301</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           estimator  fit_time  score_time  \\\n",
              "0  LinearRegression(copy_X=True, fit_intercept=Tr...  0.004383    0.002264   \n",
              "1  LinearRegression(copy_X=True, fit_intercept=Tr...  0.002115    0.001688   \n",
              "2  LinearRegression(copy_X=True, fit_intercept=Tr...  0.001916    0.001552   \n",
              "\n",
              "   test_score  train_score  \n",
              "0 -555.186275  -619.509206  \n",
              "1 -651.126513  -583.427702  \n",
              "2 -615.965800  -589.341301  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "e7qKHica4EZ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- test_score = validation score, flip the negative sign\n",
        "  - we've already done better than baseline tests"
      ]
    },
    {
      "metadata": {
        "id": "vXtlYbDh4onX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "993a5d08-23d7-4e69-e86d-f9f7d0416ffa"
      },
      "cell_type": "code",
      "source": [
        "# single number summarization\n",
        "\n",
        "scores['test_score'].mean()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-607.4261958631806"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "56nwZNuJ5m1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40646c3b-cf3a-413f-c2eb-fb3eaf1f3eae"
      },
      "cell_type": "code",
      "source": [
        "scores['train_score'].mean()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-597.4260696722171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "fg1YI4X8n9nI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Develop a model that overfits. \n",
        "\n",
        "\"The universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\" —Chollet"
      ]
    },
    {
      "metadata": {
        "id": "lodd6UPOoy89",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-validation-curve.png\">\n",
        "\n",
        "Diagram source: https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#Validation-curves-in-Scikit-Learn"
      ]
    },
    {
      "metadata": {
        "id": "FrmQ3RM0w2JE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Polynomial Regression?"
      ]
    },
    {
      "metadata": {
        "id": "uctwo0X3pTw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copied from cell 10 at\n",
        "# https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#Validation-curves-in-Scikit-Learn\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def PolynomialRegression(degree=2, **kwargs):\n",
        "    return make_pipeline(PolynomialFeatures(degree),\n",
        "                         LinearRegression(**kwargs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wvY4HOXVw7Mj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "d6420efc-425e-4b7a-8326-55ae5e10f186"
      },
      "cell_type": "code",
      "source": [
        "# would never need in the real world\n",
        "# shows different features based on different polynomial degrees\n",
        "\n",
        "for degree in [0, 1, 2, 3]:\n",
        "    features = PolynomialFeatures(degree).fit(X_train).get_feature_names(X_train.columns)\n",
        "    print(f'{degree} degree polynomial has {len(features)} features')\n",
        "    print(features)\n",
        "    print('\\n')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 degree polynomial has 1 features\n",
            "['1']\n",
            "\n",
            "\n",
            "1 degree polynomial has 8 features\n",
            "['1', 'PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'AWND', 'Total_yesterday']\n",
            "\n",
            "\n",
            "2 degree polynomial has 36 features\n",
            "['1', 'PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'AWND', 'Total_yesterday', 'PRCP^2', 'PRCP SNOW', 'PRCP SNWD', 'PRCP TMAX', 'PRCP TMIN', 'PRCP AWND', 'PRCP Total_yesterday', 'SNOW^2', 'SNOW SNWD', 'SNOW TMAX', 'SNOW TMIN', 'SNOW AWND', 'SNOW Total_yesterday', 'SNWD^2', 'SNWD TMAX', 'SNWD TMIN', 'SNWD AWND', 'SNWD Total_yesterday', 'TMAX^2', 'TMAX TMIN', 'TMAX AWND', 'TMAX Total_yesterday', 'TMIN^2', 'TMIN AWND', 'TMIN Total_yesterday', 'AWND^2', 'AWND Total_yesterday', 'Total_yesterday^2']\n",
            "\n",
            "\n",
            "3 degree polynomial has 120 features\n",
            "['1', 'PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'AWND', 'Total_yesterday', 'PRCP^2', 'PRCP SNOW', 'PRCP SNWD', 'PRCP TMAX', 'PRCP TMIN', 'PRCP AWND', 'PRCP Total_yesterday', 'SNOW^2', 'SNOW SNWD', 'SNOW TMAX', 'SNOW TMIN', 'SNOW AWND', 'SNOW Total_yesterday', 'SNWD^2', 'SNWD TMAX', 'SNWD TMIN', 'SNWD AWND', 'SNWD Total_yesterday', 'TMAX^2', 'TMAX TMIN', 'TMAX AWND', 'TMAX Total_yesterday', 'TMIN^2', 'TMIN AWND', 'TMIN Total_yesterday', 'AWND^2', 'AWND Total_yesterday', 'Total_yesterday^2', 'PRCP^3', 'PRCP^2 SNOW', 'PRCP^2 SNWD', 'PRCP^2 TMAX', 'PRCP^2 TMIN', 'PRCP^2 AWND', 'PRCP^2 Total_yesterday', 'PRCP SNOW^2', 'PRCP SNOW SNWD', 'PRCP SNOW TMAX', 'PRCP SNOW TMIN', 'PRCP SNOW AWND', 'PRCP SNOW Total_yesterday', 'PRCP SNWD^2', 'PRCP SNWD TMAX', 'PRCP SNWD TMIN', 'PRCP SNWD AWND', 'PRCP SNWD Total_yesterday', 'PRCP TMAX^2', 'PRCP TMAX TMIN', 'PRCP TMAX AWND', 'PRCP TMAX Total_yesterday', 'PRCP TMIN^2', 'PRCP TMIN AWND', 'PRCP TMIN Total_yesterday', 'PRCP AWND^2', 'PRCP AWND Total_yesterday', 'PRCP Total_yesterday^2', 'SNOW^3', 'SNOW^2 SNWD', 'SNOW^2 TMAX', 'SNOW^2 TMIN', 'SNOW^2 AWND', 'SNOW^2 Total_yesterday', 'SNOW SNWD^2', 'SNOW SNWD TMAX', 'SNOW SNWD TMIN', 'SNOW SNWD AWND', 'SNOW SNWD Total_yesterday', 'SNOW TMAX^2', 'SNOW TMAX TMIN', 'SNOW TMAX AWND', 'SNOW TMAX Total_yesterday', 'SNOW TMIN^2', 'SNOW TMIN AWND', 'SNOW TMIN Total_yesterday', 'SNOW AWND^2', 'SNOW AWND Total_yesterday', 'SNOW Total_yesterday^2', 'SNWD^3', 'SNWD^2 TMAX', 'SNWD^2 TMIN', 'SNWD^2 AWND', 'SNWD^2 Total_yesterday', 'SNWD TMAX^2', 'SNWD TMAX TMIN', 'SNWD TMAX AWND', 'SNWD TMAX Total_yesterday', 'SNWD TMIN^2', 'SNWD TMIN AWND', 'SNWD TMIN Total_yesterday', 'SNWD AWND^2', 'SNWD AWND Total_yesterday', 'SNWD Total_yesterday^2', 'TMAX^3', 'TMAX^2 TMIN', 'TMAX^2 AWND', 'TMAX^2 Total_yesterday', 'TMAX TMIN^2', 'TMAX TMIN AWND', 'TMAX TMIN Total_yesterday', 'TMAX AWND^2', 'TMAX AWND Total_yesterday', 'TMAX Total_yesterday^2', 'TMIN^3', 'TMIN^2 AWND', 'TMIN^2 Total_yesterday', 'TMIN AWND^2', 'TMIN AWND Total_yesterday', 'TMIN Total_yesterday^2', 'AWND^3', 'AWND^2 Total_yesterday', 'AWND Total_yesterday^2', 'Total_yesterday^3']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XEUdG9-ktHoa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Validation curve (with Polynomial Regression)"
      ]
    },
    {
      "metadata": {
        "id": "_ryO1hVKr-6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html\n",
        "\n",
        "> Validation curve. Determine training and test scores for varying parameter values. This is similar to grid search with one parameter."
      ]
    },
    {
      "metadata": {
        "id": "znJgKqPcqBh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "76119f76-759f-4af4-cdd7-71210339f36e"
      },
      "cell_type": "code",
      "source": [
        "# Modified from cell 13 at\n",
        "# https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#Validation-curves-in-Scikit-Learn\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import validation_curve\n",
        "\n",
        "degree = [0, 1, 2]\n",
        "train_score, val_score = validation_curve(\n",
        "    PolynomialRegression(), X_train, y_train,\n",
        "    param_name='polynomialfeatures__degree', param_range=degree, \n",
        "    scoring='neg_mean_absolute_error', cv=3)\n",
        "\n",
        "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
        "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('degree');"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfGBkzSy2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "QCF5SVLeAlnE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "going to second degree polynomial significantly kills your validation score"
      ]
    },
    {
      "metadata": {
        "id": "HUdGHWhvtVnV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Grid Search (with Polynomial Regression)\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/grid_search.html"
      ]
    },
    {
      "metadata": {
        "id": "GziXyq8Is6dL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "04d0bc23-322f-47e3-e8a6-e2360604632e"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# specify the parameter that you want to try multiple values of\n",
        "# do cross validation for each one, then tell me which is best\n",
        "\n",
        "param_grid = {\n",
        "    'polynomialfeatures__degree': [0, 1, 2, 3]\n",
        "}\n",
        "\n",
        "gridsearch = GridSearchCV(PolynomialRegression(), param_grid=param_grid,\n",
        "                         scoring='neg_mean_absolute_error', cv=3,\n",
        "                         return_train_score=True, verbose=10)\n",
        "\n",
        "# like a for loop, repeatedly pulling new polynomial degrees input above\n",
        "# computationally expensive\n",
        "# cross validating each degree\n",
        "# spitting out best polynomial degree for overall fit at end\n",
        "\n",
        "gridsearch.fit(X_train, y_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "[CV] polynomialfeatures__degree=0 ....................................\n",
            "[CV]  polynomialfeatures__degree=0, score=-1026.3529857047195, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=0 ....................................\n",
            "[CV]  polynomialfeatures__degree=0, score=-1001.6149251268913, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=0 ....................................\n",
            "[CV]  polynomialfeatures__degree=0, score=-927.0707048650538, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=1 ....................................\n",
            "[CV]  polynomialfeatures__degree=1, score=-555.1862745374103, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=1 ....................................\n",
            "[CV]  polynomialfeatures__degree=1, score=-651.1265132746228, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=1 ....................................\n",
            "[CV]  polynomialfeatures__degree=1, score=-615.9657997775082, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=2 ....................................\n",
            "[CV]  polynomialfeatures__degree=2, score=-7553.67367810515, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=2 ....................................\n",
            "[CV]  polynomialfeatures__degree=2, score=-1439.191570208428, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=2 ....................................\n",
            "[CV]  polynomialfeatures__degree=2, score=-644.1228337571547, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=3 ....................................\n",
            "[CV]  polynomialfeatures__degree=3, score=-2487.2087241103723, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=3 ....................................\n",
            "[CV]  polynomialfeatures__degree=3, score=-97950.22398935535, total=   0.0s\n",
            "[CV] polynomialfeatures__degree=3 ...................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[CV]  polynomialfeatures__degree=3, score=-1835.2490429251202, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
              "       estimator=Pipeline(memory=None,\n",
              "     steps=[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
              "         normalize=False))]),\n",
              "       fit_params=None, iid='warn', n_jobs=None,\n",
              "       param_grid={'polynomialfeatures__degree': [0, 1, 2, 3]},\n",
              "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
              "       scoring='neg_mean_absolute_error', verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "avcAxNqhB_CK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- [0, 1, 2, 3] for a total of 4 candidates\n",
        "- cv = 3, therefore 12 fits"
      ]
    },
    {
      "metadata": {
        "id": "OSM_VNK-Cp0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "d081e221-258e-4902-cd42-7a67964ef992"
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(gridsearch.cv_results_).sort_values(by='rank_test_score')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>param_polynomialfeatures__degree</th>\n",
              "      <th>params</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.003062</td>\n",
              "      <td>0.001853</td>\n",
              "      <td>-607.426196</td>\n",
              "      <td>-597.426070</td>\n",
              "      <td>1</td>\n",
              "      <td>{'polynomialfeatures__degree': 1}</td>\n",
              "      <td>1</td>\n",
              "      <td>-555.186275</td>\n",
              "      <td>-619.509206</td>\n",
              "      <td>-651.126513</td>\n",
              "      <td>-583.427702</td>\n",
              "      <td>-615.965800</td>\n",
              "      <td>-589.341301</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>39.630174</td>\n",
              "      <td>15.800661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.003497</td>\n",
              "      <td>0.001915</td>\n",
              "      <td>-985.012872</td>\n",
              "      <td>-979.844161</td>\n",
              "      <td>0</td>\n",
              "      <td>{'polynomialfeatures__degree': 0}</td>\n",
              "      <td>2</td>\n",
              "      <td>-1026.352986</td>\n",
              "      <td>-968.880368</td>\n",
              "      <td>-1001.614925</td>\n",
              "      <td>-970.755413</td>\n",
              "      <td>-927.070705</td>\n",
              "      <td>-999.896701</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>42.197661</td>\n",
              "      <td>14.199935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.008731</td>\n",
              "      <td>0.004687</td>\n",
              "      <td>-3212.329361</td>\n",
              "      <td>-576.333816</td>\n",
              "      <td>2</td>\n",
              "      <td>{'polynomialfeatures__degree': 2}</td>\n",
              "      <td>3</td>\n",
              "      <td>-7553.673678</td>\n",
              "      <td>-595.089615</td>\n",
              "      <td>-1439.191570</td>\n",
              "      <td>-568.150803</td>\n",
              "      <td>-644.122834</td>\n",
              "      <td>-565.761032</td>\n",
              "      <td>0.002828</td>\n",
              "      <td>0.001034</td>\n",
              "      <td>3086.906373</td>\n",
              "      <td>13.298189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.028712</td>\n",
              "      <td>0.006014</td>\n",
              "      <td>-34090.893919</td>\n",
              "      <td>-601.797899</td>\n",
              "      <td>3</td>\n",
              "      <td>{'polynomialfeatures__degree': 3}</td>\n",
              "      <td>4</td>\n",
              "      <td>-2487.208724</td>\n",
              "      <td>-565.787908</td>\n",
              "      <td>-97950.223989</td>\n",
              "      <td>-663.520180</td>\n",
              "      <td>-1835.249043</td>\n",
              "      <td>-576.085609</td>\n",
              "      <td>0.001542</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>45156.149752</td>\n",
              "      <td>43.846251</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
              "1       0.003062         0.001853      -607.426196       -597.426070   \n",
              "0       0.003497         0.001915      -985.012872       -979.844161   \n",
              "2       0.008731         0.004687     -3212.329361       -576.333816   \n",
              "3       0.028712         0.006014    -34090.893919       -601.797899   \n",
              "\n",
              "  param_polynomialfeatures__degree                             params  \\\n",
              "1                                1  {'polynomialfeatures__degree': 1}   \n",
              "0                                0  {'polynomialfeatures__degree': 0}   \n",
              "2                                2  {'polynomialfeatures__degree': 2}   \n",
              "3                                3  {'polynomialfeatures__degree': 3}   \n",
              "\n",
              "   rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
              "1                1        -555.186275         -619.509206        -651.126513   \n",
              "0                2       -1026.352986         -968.880368       -1001.614925   \n",
              "2                3       -7553.673678         -595.089615       -1439.191570   \n",
              "3                4       -2487.208724         -565.787908      -97950.223989   \n",
              "\n",
              "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
              "1         -583.427702        -615.965800         -589.341301      0.000285   \n",
              "0         -970.755413        -927.070705         -999.896701      0.000829   \n",
              "2         -568.150803        -644.122834         -565.761032      0.002828   \n",
              "3         -663.520180       -1835.249043         -576.085609      0.001542   \n",
              "\n",
              "   std_score_time  std_test_score  std_train_score  \n",
              "1        0.000122       39.630174        15.800661  \n",
              "0        0.000428       42.197661        14.199935  \n",
              "2        0.001034     3086.906373        13.298189  \n",
              "3        0.000379    45156.149752        43.846251  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "Di5RrqszDAKS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- revalidates that one polynomial provides best fit (lowest mean squared error)"
      ]
    },
    {
      "metadata": {
        "id": "xj82P0VdwYlh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Random Forest?\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
      ]
    },
    {
      "metadata": {
        "id": "_yYXpk99C4cM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "92190f21-2891-4826-ec0c-4db82ed95323"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, max_depth=20)\n",
        "\n",
        "scores = cross_validate(model, X_train, y_train,\n",
        "                       scoring='neg_mean_absolute_error',\n",
        "                       cv=3, return_train_score=True,\n",
        "                       return_estimator=True)\n",
        "\n",
        "pd.DataFrame(scores)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimator</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>score_time</th>\n",
              "      <th>test_score</th>\n",
              "      <th>train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(DecisionTreeRegressor(criterion='mse', max_de...</td>\n",
              "      <td>0.210896</td>\n",
              "      <td>0.011860</td>\n",
              "      <td>-555.958378</td>\n",
              "      <td>-242.778097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(DecisionTreeRegressor(criterion='mse', max_de...</td>\n",
              "      <td>0.209944</td>\n",
              "      <td>0.012030</td>\n",
              "      <td>-634.617279</td>\n",
              "      <td>-225.874585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(DecisionTreeRegressor(criterion='mse', max_de...</td>\n",
              "      <td>0.211091</td>\n",
              "      <td>0.011698</td>\n",
              "      <td>-644.593704</td>\n",
              "      <td>-223.672344</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           estimator  fit_time  score_time  \\\n",
              "0  (DecisionTreeRegressor(criterion='mse', max_de...  0.210896    0.011860   \n",
              "1  (DecisionTreeRegressor(criterion='mse', max_de...  0.209944    0.012030   \n",
              "2  (DecisionTreeRegressor(criterion='mse', max_de...  0.211091    0.011698   \n",
              "\n",
              "   test_score  train_score  \n",
              "0 -555.958378  -242.778097  \n",
              "1 -634.617279  -225.874585  \n",
              "2 -644.593704  -223.672344  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "jIDnfDphEL1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51695058-1404-460e-d11c-12c58a5890e8"
      },
      "cell_type": "code",
      "source": [
        "scores['test_score'].mean()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-611.7231201066443"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "vofwgIpSweEb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Validation Curve (with Random Forest)"
      ]
    },
    {
      "metadata": {
        "id": "apKk4vKiwgtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2e34f561-aff0-474b-d81f-d9391b2069a7"
      },
      "cell_type": "code",
      "source": [
        "# Modified from cell 13 at\n",
        "# https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#Validation-curves-in-Scikit-Learn\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import validation_curve\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100)\n",
        "\n",
        "depth = [2, 3, 4, 5, 6]\n",
        "train_score, val_score = validation_curve(\n",
        "    model, X_train, y_train,\n",
        "    param_name='max_depth', param_range=depth, \n",
        "    scoring='neg_mean_absolute_error', cv=3)\n",
        "\n",
        "plt.plot(depth, np.median(train_score, 1), color='blue', label='training score')\n",
        "plt.plot(depth, np.median(val_score, 1), color='red', label='validation score')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('depth');"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DQoMvZ7-yCAQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Grid Search (with Random Forest)"
      ]
    },
    {
      "metadata": {
        "id": "bk_dX_mByKm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        },
        "outputId": "f0c11d0a-9de7-4525-86bd-9926ebf4a204"
      },
      "cell_type": "code",
      "source": [
        "# measure how long running this cell takes\n",
        "%%time\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [4, 5],\n",
        "    'criterion': ['mse', 'mae']\n",
        "}\n",
        "\n",
        "gridsearch = GridSearchCV(RandomForestRegressor(), param_grid=param_grid, \n",
        "                         scoring='neg_mean_absolute_error', cv=3, \n",
        "                         return_train_score=True, verbose=10)\n",
        "\n",
        "gridsearch.fit(X_train, y_train)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "[CV] criterion=mse, max_depth=4, n_estimators=100 ....................\n",
            "[CV]  criterion=mse, max_depth=4, n_estimators=100, score=-552.1947758708461, total=   0.1s\n",
            "[CV] criterion=mse, max_depth=4, n_estimators=100 ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  criterion=mse, max_depth=4, n_estimators=100, score=-643.5791139300968, total=   0.1s\n",
            "[CV] criterion=mse, max_depth=4, n_estimators=100 ....................\n",
            "[CV]  criterion=mse, max_depth=4, n_estimators=100, score=-622.6512860282103, total=   0.1s\n",
            "[CV] criterion=mse, max_depth=4, n_estimators=200 ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  criterion=mse, max_depth=4, n_estimators=200, score=-556.6201066282554, total=   0.2s\n",
            "[CV] criterion=mse, max_depth=4, n_estimators=200 ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.7s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  criterion=mse, max_depth=4, n_estimators=200, score=-639.9270697736678, total=   0.3s\n",
            "[CV] criterion=mse, max_depth=4, n_estimators=200 ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  criterion=mse, max_depth=4, n_estimators=200, score=-627.5859145277325, total=   0.2s\n",
            "[CV] criterion=mse, max_depth=5, n_estimators=100 ....................\n",
            "[CV]  criterion=mse, max_depth=5, n_estimators=100, score=-551.3048773567973, total=   0.1s\n",
            "[CV] criterion=mse, max_depth=5, n_estimators=100 ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    1.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    1.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  criterion=mse, max_depth=5, n_estimators=100, score=-634.6932277248943, total=   0.1s\n",
            "[CV] criterion=mse, max_depth=5, n_estimators=100 ....................\n",
            "[CV]  criterion=mse, max_depth=5, n_estimators=100, score=-628.7058394382674, total=   0.1s\n",
            "[CV] criterion=mse, max_depth=5, n_estimators=200 ....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    1.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.6s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  criterion=mse, max_depth=5, n_estimators=200, score=-544.3704764948251, total=   0.3s\n",
            "[CV] criterion=mse, max_depth=5, n_estimators=200 ....................\n",
            "[CV]  criterion=mse, max_depth=5, n_estimators=200, score=-640.2165835428887, total=   0.3s\n",
            "[CV] criterion=mse, max_depth=5, n_estimators=200 ....................\n",
            "[CV]  criterion=mse, max_depth=5, n_estimators=200, score=-628.372602086555, total=   0.3s\n",
            "[CV] criterion=mae, max_depth=4, n_estimators=100 ....................\n",
            "[CV]  criterion=mae, max_depth=4, n_estimators=100, score=-544.3893925233644, total=   0.7s\n",
            "[CV] criterion=mae, max_depth=4, n_estimators=100 ....................\n",
            "[CV]  criterion=mae, max_depth=4, n_estimators=100, score=-633.093800623053, total=   0.7s\n",
            "[CV] criterion=mae, max_depth=4, n_estimators=100 ....................\n",
            "[CV]  criterion=mae, max_depth=4, n_estimators=100, score=-615.6430218068535, total=   0.7s\n",
            "[CV] criterion=mae, max_depth=4, n_estimators=200 ....................\n",
            "[CV]  criterion=mae, max_depth=4, n_estimators=200, score=-545.5993613707166, total=   1.4s\n",
            "[CV] criterion=mae, max_depth=4, n_estimators=200 ....................\n",
            "[CV]  criterion=mae, max_depth=4, n_estimators=200, score=-636.4574766355139, total=   1.3s\n",
            "[CV] criterion=mae, max_depth=4, n_estimators=200 ....................\n",
            "[CV]  criterion=mae, max_depth=4, n_estimators=200, score=-612.9532866043614, total=   1.4s\n",
            "[CV] criterion=mae, max_depth=5, n_estimators=100 ....................\n",
            "[CV]  criterion=mae, max_depth=5, n_estimators=100, score=-537.0092679127725, total=   0.8s\n",
            "[CV] criterion=mae, max_depth=5, n_estimators=100 ....................\n",
            "[CV]  criterion=mae, max_depth=5, n_estimators=100, score=-635.4109034267913, total=   0.8s\n",
            "[CV] criterion=mae, max_depth=5, n_estimators=100 ....................\n",
            "[CV]  criterion=mae, max_depth=5, n_estimators=100, score=-615.5530685358256, total=   0.8s\n",
            "[CV] criterion=mae, max_depth=5, n_estimators=200 ....................\n",
            "[CV]  criterion=mae, max_depth=5, n_estimators=200, score=-537.562492211838, total=   1.6s\n",
            "[CV] criterion=mae, max_depth=5, n_estimators=200 ....................\n",
            "[CV]  criterion=mae, max_depth=5, n_estimators=200, score=-634.8904049844238, total=   1.5s\n",
            "[CV] criterion=mae, max_depth=5, n_estimators=200 ....................\n",
            "[CV]  criterion=mae, max_depth=5, n_estimators=200, score=-613.7594859813084, total=   1.5s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   15.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 18.3 s, sys: 23.9 ms, total: 18.3 s\n",
            "Wall time: 18.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bbyKc7AuGQVr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "outputId": "c50816fc-3667-445e-c6be-937ece780ffb"
      },
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(gridsearch.cv_results_)\n",
        "print(f'Best result from grid search of {len(results)} parameter combinations')\n",
        "results.sort_values(by='rank_test_score').head(8)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best result from grid search of 8 parameter combinations\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>param_criterion</th>\n",
              "      <th>param_max_depth</th>\n",
              "      <th>param_n_estimators</th>\n",
              "      <th>params</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.512501</td>\n",
              "      <td>0.012814</td>\n",
              "      <td>-595.404128</td>\n",
              "      <td>-493.663433</td>\n",
              "      <td>mae</td>\n",
              "      <td>5</td>\n",
              "      <td>200</td>\n",
              "      <td>{'criterion': 'mae', 'max_depth': 5, 'n_estima...</td>\n",
              "      <td>1</td>\n",
              "      <td>-537.562492</td>\n",
              "      <td>-517.593843</td>\n",
              "      <td>-634.890405</td>\n",
              "      <td>-478.298968</td>\n",
              "      <td>-613.759486</td>\n",
              "      <td>-485.097488</td>\n",
              "      <td>0.032527</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>41.800080</td>\n",
              "      <td>17.147466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.766772</td>\n",
              "      <td>0.007657</td>\n",
              "      <td>-595.991080</td>\n",
              "      <td>-493.285101</td>\n",
              "      <td>mae</td>\n",
              "      <td>5</td>\n",
              "      <td>100</td>\n",
              "      <td>{'criterion': 'mae', 'max_depth': 5, 'n_estima...</td>\n",
              "      <td>2</td>\n",
              "      <td>-537.009268</td>\n",
              "      <td>-515.863995</td>\n",
              "      <td>-635.410903</td>\n",
              "      <td>-478.111947</td>\n",
              "      <td>-615.553069</td>\n",
              "      <td>-485.879361</td>\n",
              "      <td>0.009773</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>42.487049</td>\n",
              "      <td>16.277552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.671211</td>\n",
              "      <td>0.007251</td>\n",
              "      <td>-597.708738</td>\n",
              "      <td>-531.377139</td>\n",
              "      <td>mae</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>{'criterion': 'mae', 'max_depth': 4, 'n_estima...</td>\n",
              "      <td>3</td>\n",
              "      <td>-544.389393</td>\n",
              "      <td>-555.393505</td>\n",
              "      <td>-633.093801</td>\n",
              "      <td>-512.857531</td>\n",
              "      <td>-615.643022</td>\n",
              "      <td>-525.880382</td>\n",
              "      <td>0.010439</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>38.369666</td>\n",
              "      <td>17.794906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.352008</td>\n",
              "      <td>0.012211</td>\n",
              "      <td>-598.336708</td>\n",
              "      <td>-530.883882</td>\n",
              "      <td>mae</td>\n",
              "      <td>4</td>\n",
              "      <td>200</td>\n",
              "      <td>{'criterion': 'mae', 'max_depth': 4, 'n_estima...</td>\n",
              "      <td>4</td>\n",
              "      <td>-545.599361</td>\n",
              "      <td>-554.812212</td>\n",
              "      <td>-636.457477</td>\n",
              "      <td>-514.902461</td>\n",
              "      <td>-612.953287</td>\n",
              "      <td>-522.936974</td>\n",
              "      <td>0.024565</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>38.505693</td>\n",
              "      <td>17.234888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.257765</td>\n",
              "      <td>0.013260</td>\n",
              "      <td>-604.319887</td>\n",
              "      <td>-501.071325</td>\n",
              "      <td>mse</td>\n",
              "      <td>5</td>\n",
              "      <td>200</td>\n",
              "      <td>{'criterion': 'mse', 'max_depth': 5, 'n_estima...</td>\n",
              "      <td>5</td>\n",
              "      <td>-544.370476</td>\n",
              "      <td>-523.310473</td>\n",
              "      <td>-640.216584</td>\n",
              "      <td>-490.369122</td>\n",
              "      <td>-628.372602</td>\n",
              "      <td>-489.534382</td>\n",
              "      <td>0.003530</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>42.665512</td>\n",
              "      <td>15.729144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.131946</td>\n",
              "      <td>0.007664</td>\n",
              "      <td>-604.901315</td>\n",
              "      <td>-501.394361</td>\n",
              "      <td>mse</td>\n",
              "      <td>5</td>\n",
              "      <td>100</td>\n",
              "      <td>{'criterion': 'mse', 'max_depth': 5, 'n_estima...</td>\n",
              "      <td>6</td>\n",
              "      <td>-551.304877</td>\n",
              "      <td>-523.341420</td>\n",
              "      <td>-634.693228</td>\n",
              "      <td>-491.733040</td>\n",
              "      <td>-628.705839</td>\n",
              "      <td>-489.108622</td>\n",
              "      <td>0.003947</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>37.977149</td>\n",
              "      <td>15.555855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.119402</td>\n",
              "      <td>0.007677</td>\n",
              "      <td>-606.141725</td>\n",
              "      <td>-545.443420</td>\n",
              "      <td>mse</td>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>{'criterion': 'mse', 'max_depth': 4, 'n_estima...</td>\n",
              "      <td>7</td>\n",
              "      <td>-552.194776</td>\n",
              "      <td>-572.443898</td>\n",
              "      <td>-643.579114</td>\n",
              "      <td>-529.576329</td>\n",
              "      <td>-622.651286</td>\n",
              "      <td>-534.310034</td>\n",
              "      <td>0.001065</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>39.091333</td>\n",
              "      <td>19.189777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.234941</td>\n",
              "      <td>0.012235</td>\n",
              "      <td>-608.044364</td>\n",
              "      <td>-544.599921</td>\n",
              "      <td>mse</td>\n",
              "      <td>4</td>\n",
              "      <td>200</td>\n",
              "      <td>{'criterion': 'mse', 'max_depth': 4, 'n_estima...</td>\n",
              "      <td>8</td>\n",
              "      <td>-556.620107</td>\n",
              "      <td>-569.935974</td>\n",
              "      <td>-639.927070</td>\n",
              "      <td>-529.704035</td>\n",
              "      <td>-627.585915</td>\n",
              "      <td>-534.159753</td>\n",
              "      <td>0.002694</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>36.709823</td>\n",
              "      <td>18.007407</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
              "7       1.512501         0.012814      -595.404128       -493.663433   \n",
              "6       0.766772         0.007657      -595.991080       -493.285101   \n",
              "4       0.671211         0.007251      -597.708738       -531.377139   \n",
              "5       1.352008         0.012211      -598.336708       -530.883882   \n",
              "3       0.257765         0.013260      -604.319887       -501.071325   \n",
              "2       0.131946         0.007664      -604.901315       -501.394361   \n",
              "0       0.119402         0.007677      -606.141725       -545.443420   \n",
              "1       0.234941         0.012235      -608.044364       -544.599921   \n",
              "\n",
              "  param_criterion param_max_depth param_n_estimators  \\\n",
              "7             mae               5                200   \n",
              "6             mae               5                100   \n",
              "4             mae               4                100   \n",
              "5             mae               4                200   \n",
              "3             mse               5                200   \n",
              "2             mse               5                100   \n",
              "0             mse               4                100   \n",
              "1             mse               4                200   \n",
              "\n",
              "                                              params  rank_test_score  \\\n",
              "7  {'criterion': 'mae', 'max_depth': 5, 'n_estima...                1   \n",
              "6  {'criterion': 'mae', 'max_depth': 5, 'n_estima...                2   \n",
              "4  {'criterion': 'mae', 'max_depth': 4, 'n_estima...                3   \n",
              "5  {'criterion': 'mae', 'max_depth': 4, 'n_estima...                4   \n",
              "3  {'criterion': 'mse', 'max_depth': 5, 'n_estima...                5   \n",
              "2  {'criterion': 'mse', 'max_depth': 5, 'n_estima...                6   \n",
              "0  {'criterion': 'mse', 'max_depth': 4, 'n_estima...                7   \n",
              "1  {'criterion': 'mse', 'max_depth': 4, 'n_estima...                8   \n",
              "\n",
              "   split0_test_score  split0_train_score  split1_test_score  \\\n",
              "7        -537.562492         -517.593843        -634.890405   \n",
              "6        -537.009268         -515.863995        -635.410903   \n",
              "4        -544.389393         -555.393505        -633.093801   \n",
              "5        -545.599361         -554.812212        -636.457477   \n",
              "3        -544.370476         -523.310473        -640.216584   \n",
              "2        -551.304877         -523.341420        -634.693228   \n",
              "0        -552.194776         -572.443898        -643.579114   \n",
              "1        -556.620107         -569.935974        -639.927070   \n",
              "\n",
              "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
              "7         -478.298968        -613.759486         -485.097488      0.032527   \n",
              "6         -478.111947        -615.553069         -485.879361      0.009773   \n",
              "4         -512.857531        -615.643022         -525.880382      0.010439   \n",
              "5         -514.902461        -612.953287         -522.936974      0.024565   \n",
              "3         -490.369122        -628.372602         -489.534382      0.003530   \n",
              "2         -491.733040        -628.705839         -489.108622      0.003947   \n",
              "0         -529.576329        -622.651286         -534.310034      0.001065   \n",
              "1         -529.704035        -627.585915         -534.159753      0.002694   \n",
              "\n",
              "   std_score_time  std_test_score  std_train_score  \n",
              "7        0.000088       41.800080        17.147466  \n",
              "6        0.000044       42.487049        16.277552  \n",
              "4        0.000084       38.369666        17.794906  \n",
              "5        0.000177       38.505693        17.234888  \n",
              "3        0.000257       42.665512        15.729144  \n",
              "2        0.000074       37.977149        15.555855  \n",
              "0        0.000455       39.091333        19.189777  \n",
              "1        0.000051       36.709823        18.007407  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "uCtH8VuDGvOj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- not improving our validation score"
      ]
    },
    {
      "metadata": {
        "id": "ZW5HfYtU0GW2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FEATURE ENGINEERING!"
      ]
    },
    {
      "metadata": {
        "id": "0ms-eoOHFvPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Jake VanderPlas demonstrates this feature engineering: \n",
        "https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Example:-Predicting-Bicycle-Traffic"
      ]
    },
    {
      "metadata": {
        "id": "sEwME8wR3A5g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Modified from code cells 17-21 at\n",
        "# https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Example:-Predicting-Bicycle-Traffic\n",
        "\n",
        "\n",
        "# patterns of use generally vary from day to day; \n",
        "# let's add binary columns that indicate the day of the week:\n",
        "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "for i, day in enumerate(days):\n",
        "    X_train[day] = (X_train.index.dayofweek == i).astype(float)\n",
        "\n",
        "\n",
        "    \n",
        "# we might expect riders to behave differently on holidays; \n",
        "# let's add an indicator of this as well:\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "cal = USFederalHolidayCalendar()\n",
        "holidays = cal.holidays('2012', '2016')\n",
        "X_train = X_train.join(pd.Series(1, index=holidays, name='holiday'))\n",
        "X_train['holiday'].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# We also might suspect that the hours of daylight would affect \n",
        "# how many people ride; let's use the standard astronomical calculation \n",
        "# to add this information:\n",
        "def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
        "    \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
        "    days = (date - pd.datetime(2000, 12, 21)).days\n",
        "    m = (1. - np.tan(np.radians(latitude))\n",
        "         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
        "    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
        "\n",
        "X_train['daylight_hrs'] = list(map(hours_of_daylight, X_train.index))\n",
        "\n",
        "\n",
        "\n",
        "# temperatures are in 1/10 deg C; convert to C\n",
        "X_train['TMIN'] /= 10\n",
        "X_train['TMAX'] /= 10\n",
        "\n",
        "# We can also calcuate the average temperature.\n",
        "X_train['Temp (C)'] = 0.5 * (X_train['TMIN'] + X_train['TMAX'])\n",
        "\n",
        "\n",
        "\n",
        "# precip is in 1/10 mm; convert to inches\n",
        "X_train['PRCP'] /= 254\n",
        "\n",
        "# In addition to the inches of precipitation, let's add a flag that \n",
        "# indicates whether a day is dry (has zero precipitation):\n",
        "X_train['dry day'] = (X_train['PRCP'] == 0).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "# Let's add a counter that increases from day 1, and measures how many \n",
        "# years have passed. This will let us measure any observed annual increase \n",
        "# or decrease in daily crossings:\n",
        "X_train['annual'] = (X_train.index - X_train.index[0]).days / 365."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dDGkAv813Wtj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Linear Regression (with new features)"
      ]
    },
    {
      "metadata": {
        "id": "cj3HTM6p5F1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "59304b56-d787-4317-ec9e-913383248f21"
      },
      "cell_type": "code",
      "source": [
        "scores = cross_validate(LinearRegression(), X_train, y_train, \n",
        "                       scoring='neg_mean_absolute_error', cv=3, \n",
        "                       return_train_score=True, return_estimator=True)\n",
        "\n",
        "pd.DataFrame(scores)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimator</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>score_time</th>\n",
              "      <th>test_score</th>\n",
              "      <th>train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LinearRegression(copy_X=True, fit_intercept=Tr...</td>\n",
              "      <td>0.012487</td>\n",
              "      <td>0.002552</td>\n",
              "      <td>-297.692524</td>\n",
              "      <td>-294.532315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LinearRegression(copy_X=True, fit_intercept=Tr...</td>\n",
              "      <td>0.008034</td>\n",
              "      <td>0.002384</td>\n",
              "      <td>-300.419037</td>\n",
              "      <td>-283.779461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LinearRegression(copy_X=True, fit_intercept=Tr...</td>\n",
              "      <td>0.010442</td>\n",
              "      <td>0.003759</td>\n",
              "      <td>-322.640378</td>\n",
              "      <td>-283.509114</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           estimator  fit_time  score_time  \\\n",
              "0  LinearRegression(copy_X=True, fit_intercept=Tr...  0.012487    0.002552   \n",
              "1  LinearRegression(copy_X=True, fit_intercept=Tr...  0.008034    0.002384   \n",
              "2  LinearRegression(copy_X=True, fit_intercept=Tr...  0.010442    0.003759   \n",
              "\n",
              "   test_score  train_score  \n",
              "0 -297.692524  -294.532315  \n",
              "1 -300.419037  -283.779461  \n",
              "2 -322.640378  -283.509114  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "Guxao-tbIbiq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- significant decrease in test score"
      ]
    },
    {
      "metadata": {
        "id": "b6zxN2xB3bX_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Random Forest (with new features)"
      ]
    },
    {
      "metadata": {
        "id": "3sWUDZIz1-kk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100], \n",
        "    'max_depth': [5, 10, 15], \n",
        "    'criterion': ['mae']\n",
        "}\n",
        "\n",
        "# TODO\n",
        "# instantiate and fit a gridseachcv and look at best results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0QEBUVR13kcb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression (with new features)\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
      ]
    },
    {
      "metadata": {
        "id": "4voLbIxU8r6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "410ddc71-88fb-4e5a-ffa6-ad976a067056"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 1.0, 10.]\n",
        "}\n",
        "\n",
        "gridsearch = GridSearchCV(Ridge(), param_grid=param_grid, \n",
        "                         scoring='neg_mean_absolute_error', cv=3, \n",
        "                         return_train_score=True)\n",
        "\n",
        "gridsearch.fit(X_train, y_train)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
              "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
              "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
              "       fit_params=None, iid='warn', n_jobs=None,\n",
              "       param_grid={'alpha': [0.1, 1.0, 10.0]}, pre_dispatch='2*n_jobs',\n",
              "       refit=True, return_train_score=True,\n",
              "       scoring='neg_mean_absolute_error', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "XHYo_wrSJbE-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Look at best results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lHTMKgeXJI_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb1d198a-b607-429f-9fda-71477bc15074"
      },
      "cell_type": "code",
      "source": [
        "model = gridsearch.best_estimator_\n",
        "\n",
        "type(model)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sklearn.linear_model.ridge.Ridge"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "VnbCLrIsJimQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "37e34aba-e723-4395-ba5c-09794b356c00"
      },
      "cell_type": "code",
      "source": [
        "coefficients = model.coef_\n",
        "intercept = model.intercept_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "print('Best model from grid search cross validation')\n",
        "print('Intercept', intercept)\n",
        "print(pd.Series(coefficients, feature_names).to_string())"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best model from grid search cross validation\n",
            "Intercept 33.741779570641484\n",
            "PRCP               -553.070741\n",
            "SNOW                 -0.002829\n",
            "SNWD                 -1.877519\n",
            "TMAX                 63.833062\n",
            "TMIN                -37.450291\n",
            "AWND                 -1.900084\n",
            "Total_yesterday       0.296029\n",
            "Mon                 779.221395\n",
            "Tue                 432.700039\n",
            "Wed                 368.367626\n",
            "Thu                 274.054021\n",
            "Fri                  47.251356\n",
            "Sat               -1099.692199\n",
            "Sun                -801.902237\n",
            "holiday            -939.301546\n",
            "daylight_hrs         70.256463\n",
            "Temp (C)             13.191386\n",
            "dry day             298.475434\n",
            "annual               44.518889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dofdwyTf3pm0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compare to statsmodels"
      ]
    },
    {
      "metadata": {
        "id": "i-Qt4mDk_yBY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Same as before"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edpJ87A8A8sd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Feature engineering, explained by Francois Chollet\n",
        "\n",
        "> _Feature engineering_ is the process of using your own knowledge about the data and about the machine learning algorithm at hand to make the algorithm work better by applying hardcoded (nonlearned) transformations to the data before it goes into the model. In many cases, it isn’t reasonable to expect a machine-learning model to be able to learn from completely arbitrary data. The data needs to be presented to the model in a way that will make the model’s job easier.\n",
        "\n",
        "> Let’s look at an intuitive example. Suppose you’re trying to develop a model that can take as input an image of a clock and can output the time of day.\n",
        "\n",
        "> If you choose to use the raw pixels of the image as input data, then you have a difficult machine-learning problem on your hands. You’ll need a convolutional neural network to solve it, and you’ll have to expend quite a bit of computational resources to train the network.\n",
        "\n",
        "> But if you already understand the problem at a high level (you understand how humans read time on a clock face), then you can come up with much better input features for a machine-learning algorithm: for instance, write a Python script to follow the black pixels of the clock hands and output the (x, y) coordinates of the tip of each hand. Then a simple machine-learning algorithm can learn to associate these coordinates with the appropriate time of day.\n",
        "\n",
        "> You can go even further: do a coordinate change, and express the (x, y) coordinates as polar coordinates with regard to the center of the image. Your input will become the angle theta of each clock hand. At this point, your features are making the problem so easy that no machine learning is required; a simple rounding operation and dictionary lookup are enough to recover the approximate time of day.\n",
        "\n",
        "> That’s the essence of feature engineering: making a problem easier by expressing it in a simpler way. It usually requires understanding the problem in depth.\n",
        "\n",
        "> Before convolutional neural networks became successful on the MNIST digit-classification problem, solutions were typically based on hardcoded features such as the number of loops in a digit image, the height of each digit in an image, a histogram of pixel values, and so on.\n",
        "\n",
        "> Neural networks are capable of automatically extracting useful features from raw data. Does this mean you don’t have to worry about feature engineering as long as you’re using deep neural networks? No, for two reasons:\n",
        "\n",
        "> - Good features still allow you to solve problems more elegantly while using fewer resources. For instance, it would be ridiculous to solve the problem of reading a clock face using a convolutional neural network.\n",
        "> - Good features let you solve a problem with far less data. The ability of deep-learning models to learn features on their own relies on having lots of training data available; if you have only a few samples, then the information value in their features becomes critical.\n"
      ]
    },
    {
      "metadata": {
        "id": "oux-dd-5FD6p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT\n",
        "\n",
        "### Core assignment\n",
        "\n",
        "Complete the notebook cells that were originally commented **`TODO`**. \n",
        "\n",
        "Then, focus on feature engineering to improve your cross validation scores. Collaborate with your cohort on Slack. You could start with the ideas [Jake VanderPlas suggests:](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Example:-Predicting-Bicycle-Traffic)\n",
        "\n",
        "> Our model is almost certainly missing some relevant information. For example, nonlinear effects (such as effects of precipitation and cold temperature) and nonlinear trends within each variable (such as disinclination to ride at very cold and very hot temperatures) cannot be accounted for in this model. Additionally, we have thrown away some of the finer-grained information (such as the difference between a rainy morning and a rainy afternoon), and we have ignored correlations between days (such as the possible effect of a rainy Tuesday on Wednesday's numbers, or the effect of an unexpected sunny day after a streak of rainy days). These are all potentially interesting effects, and you now have the tools to begin exploring them if you wish!\n",
        "\n",
        "At the end of the day, take the last step in the \"universal workflow of machine learning\" — \"You can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set.\"\n",
        "\n",
        "See the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) documentation for the `refit` parameter, `best_estimator_` attribute, and `predict` method:\n",
        "\n",
        "> **refit : boolean, or string, default=True**\n",
        "\n",
        "> Refit an estimator using the best found parameters on the whole dataset.\n",
        "\n",
        "> The refitted estimator is made available at the `best_estimator_` attribute and permits using `predict` directly on this `GridSearchCV` instance.\n",
        "\n",
        "### More options\n",
        "\n",
        "**A.** Apply this lesson to other datasets.\n",
        "\n",
        "**B.** We predicted the number of bicycle trips based on that day's weather. But imagine you were asked to predict trips at the beginning of each day, based only on data known at the time of prediction or before — so you cannot use the current day's weather. How would you wrangle the features to handle this new requirement? How does this impact the predictive accuracy and coefficients of your models?\n",
        "\n",
        "**C.** In additon to `GridSearchCV`, scikit-learn has [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html), which is sometimes even better. Another library called scikit-optimize has [`BayesSearchCV`](https://scikit-optimize.github.io/notebooks/sklearn-gridsearchcv-replacement.html). Experiment with these alternatives.\n",
        "\n",
        "**D.** _[Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do)_ discusses options for \"Grid-Searching Which Model To Use\" in Chapter 6:\n",
        "\n",
        "> You can even go further in combining GridSearchCV and Pipeline: it is also possible to search over the actual steps being performed in the pipeline (say whether to use StandardScaler or MinMaxScaler). This leads to an even bigger search space and should be considered carefully. Trying all possible solutions is usually not a viable machine learning strategy. However, here is an example comparing a RandomForestClassifier and an SVC ...\n",
        "\n",
        "The example is shown in [the accompanying notebook](https://github.com/amueller/introduction_to_ml_with_python/blob/master/06-algorithm-chains-and-pipelines.ipynb), code cells 35-37. Could you apply this concept to your own pipelines?\n",
        "\n",
        "\n"
      ]
    }
  ]
}