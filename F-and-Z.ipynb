{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>lcp</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "      <th>lpsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.579818</td>\n",
       "      <td>2.769459</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.430783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.994252</td>\n",
       "      <td>3.319626</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.510826</td>\n",
       "      <td>2.691243</td>\n",
       "      <td>74</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.203973</td>\n",
       "      <td>3.282789</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.751416</td>\n",
       "      <td>3.432373</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.371564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45      lpsa\n",
       "1 -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0 -0.430783\n",
       "2 -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0 -0.162519\n",
       "3 -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20 -0.162519\n",
       "4 -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0 -0.162519\n",
       "5  0.751416  3.432373   62 -1.386294    0 -1.386294        6      0  0.371564"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "##\n",
    "### https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "'''Prostate data info\n",
    "\n",
    "Predictors (columns 1--8)\n",
    "\n",
    "lcavol\n",
    "lweight\n",
    "age\n",
    "lbph\n",
    "svi\n",
    "lcp\n",
    "gleason\n",
    "pgg45\n",
    "\n",
    "outcome (column 9)\n",
    "\n",
    "lpsa\n",
    "\n",
    "train/test indicator (column 10)\n",
    "\n",
    "This last column indicates which 67 observations were used as the \n",
    "\"training set\" and which 30 as the test set, as described on page 48\n",
    "in the book.\n",
    "\n",
    "There was an error in these data in the first edition of this\n",
    "book. Subject 32 had a value of 6.1 for lweight, which translates to a\n",
    "449 gm prostate! The correct value is 44.9 gm. We are grateful to\n",
    "Prof. Stephen W. Link for alerting us to this error.\n",
    "\n",
    "The features must first be scaled to have mean zero and  variance 96 (=n)\n",
    "before the analyses in Tables 3.1 and beyond.  That is, if x is the  96 by 8 matrix\n",
    "of features, we compute xp <- scale(x,TRUE,TRUE)\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "def repeatedly(f, n): \n",
    "    def composition(f,g):\n",
    "        # \"f after g\" \n",
    "        return lambda x: f(g(x))\n",
    "    return reduce(lambda ma, mma: composition(ma, mma), [f]*n)\n",
    "\n",
    "df_unnormalized = pd.read_csv(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data\", \n",
    "                 delim_whitespace=True).select_dtypes(include=['float', 'int'])\n",
    "\n",
    "#df_unnormalized['ones'] = np.ones(df_unnormalized.shape[0])\n",
    "print(df_unnormalized.shape)\n",
    "df_unnormalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grokking the f-statistic and z-score\n",
    "\n",
    "Do a linear regression with `p` features. drop a feature (which is similar to requiring that one of the coefficients is zero), and train another model. \n",
    "\n",
    "the _F-statistic_ measures the difference between these two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS: \n",
    "    def __init__(self, dat, dependent): \n",
    "        self.X, self.y = self.X_from_df(dat, dependent)\n",
    "        self.names = self.X.columns\n",
    "        self.N = self.X.shape[0]\n",
    "        assert self.N==self.y.shape[0]\n",
    "        self.XtX_inv = self.XtX_inv(self.X)\n",
    "        self.beta_hat = self.beta_hat()\n",
    "        for feat in self.X.drop('ones', axis=1).columns: \n",
    "            assert_almost_equal(self.X[feat].mean(), 0)\n",
    "            assert self.X[feat].isna().sum()==0\n",
    "        self.vs = np.diag(self.XtX_inv)\n",
    "        self.p = self.X.shape[1]\n",
    "        self.vari_esti = np.divide(1, self.N - self.p - 2) * self.RSS(self.beta_hat)\n",
    "        self.zs = np.divide(self.beta_hat, self.vari_esti * np.sqrt(self.vs))\n",
    "\n",
    "    def X_from_df(self, dat, dependent): \n",
    "        '''takes: dat and the name of the y feature\n",
    "    \n",
    "        returns: centered dataframe with ones\n",
    "        '''\n",
    "        ones = pd.DataFrame(np.ones(dat.shape[0]), columns=['ones']).shift(1)\n",
    "        X = np.divide(dat.drop(dependent, axis=1) - dat.drop(dependent, axis=1).mean(),\n",
    "                  dat.drop(dependent, axis=1).var())\n",
    "\n",
    "        df = pd.concat([ones, X], axis=1).drop(0, axis=0)\n",
    "        df.ones.iloc[df.shape[0]-1] = 1\n",
    "        return df, dat[dependent]    \n",
    "        \n",
    "    def XtX_inv(self, X): \n",
    "        XtX = np.matmul(X.T, X)\n",
    "        return np.linalg.inv(XtX)\n",
    "    \n",
    "    def beta_hat(self):\n",
    "        '''analytic solution to OLS'''\n",
    "        left_fact = np.matmul(self.XtX_inv, self.X.T)\n",
    "        return np.matmul(left_fact, self.y)\n",
    "\n",
    "    def RSS(self, beta):#, X=self.X, y=self.y): \n",
    "        '''residual sum squared'''\n",
    "        resid_mat = self.y - np.matmul(self.X, beta)\n",
    "        return np.matmul(resid_mat.T, resid_mat)\n",
    "\n",
    "    def drop_one(self, i): \n",
    "        to_drop = self.names[i]\n",
    "        return X.drop(to_drop, axis=1)\n",
    "    \n",
    "    def report(self): \n",
    "        rprt_dict = {k: (self.beta_hat[k], self.zs[k]) for k in range(self.p)}\n",
    "        s = ''\n",
    "        for k in range(self.p):\n",
    "            s += f'Coeff #{k} is {rprt_dict[k][0]:.3}, garnering a z-score of {rprt_dict[k][1]:.3}\\n'\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff #0 is 2.48, garnering a z-score of 48.8\n",
      "Coeff #1 is 0.784, garnering a z-score of 8.98\n",
      "Coeff #2 is 0.114, garnering a z-score of 4.33\n",
      "Coeff #3 is -1.18, garnering a z-score of -2.68\n",
      "Coeff #4 is 0.204, garnering a z-score of 2.33\n",
      "Coeff #5 is 0.131, garnering a z-score of 4.41\n",
      "Coeff #6 is -0.207, garnering a z-score of -1.65\n",
      "Coeff #7 is 0.0257, garnering a z-score of 0.443\n",
      "Coeff #8 is 3.55, garnering a z-score of 1.43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ols = OLS(df_unnormalized, 'lpsa')\n",
    "\n",
    "print(ols.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to be continued.... the F-score function\n",
    "\n",
    "### it basically looks like this: \n",
    "\n",
    "$$F = \\frac{(RSS_0 - RSS_1) * (N - p_1 - 1)}{(p_1 - p_0) * RSS_1} $$ \n",
    "\n",
    "where $RSS_1$ is the loss of a _larger_ model, i.e., a model with $p_1$ coefficients; and $RSS_0$ is a _smaller_ model with $p_0$ coefficients (we require $p_1 > p_0$ )\n",
    "\n",
    "\n",
    "## _Exercise 3.1_ states that $z_i ^ 2 == F$ when $RSS_1$ is the full model and $RSS_0$ is the model having dropped feature #$i$ (or required that it's coefficient is exactly $0$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
