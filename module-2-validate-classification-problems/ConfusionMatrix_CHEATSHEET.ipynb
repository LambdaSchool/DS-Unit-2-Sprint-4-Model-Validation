{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $TruePositives$ (`TP`): These are cases in which we predicted yes (they have the disease), and they do have the disease.\n",
    "2. $TrueNegatives$ (`TN`): We predicted no, and they don't have the disease.\n",
    "3. $FalsePositives$ (`FP`): We predicted yes, but they don't actually have the disease. (Also known as a `Type I error`)\n",
    "4. $FalseNegatives$ (`FN`): We predicted no, but they actually do have the disease. (Also known as a `Type II error`.)\n",
    "\n",
    "\n",
    "1. _Accuracy_: Overall, how often is the classifier correct?\n",
    "  * $\\frac{TruePositive + TrueNegative}{Total}$\n",
    "2. _Misclassification Rate_: Overall, how often is it wrong?\n",
    "  * $\\frac{FalsePositive + FalseNegative}{Total}$, equivalent to `1 minus Accuracy`, also known as `Error Rate`\n",
    "3. _True Positive Rate_: When it's actually yes, how often does it predict yes?\n",
    "  * $\\frac{TruePositive}{ActuallyYes}$, also known as `Sensitivity` or `Recall`\n",
    "  - \"How complete are the results?\" \n",
    "  - measures completeness or quantity\n",
    "  - high recall => algorithm returned most relevant results \n",
    "  - missing recall is `type 2 error`\n",
    "4. _False Positive Rate_: When it's actually no, how often does it predict yes?\n",
    "  * $\\frac{FalsePositive}{ActuallyNo}$\n",
    "5. _True Negative Rate_: When it's actually no, how often does it predict no?\n",
    "  * $\\frac{TrueNegative}{ActuallyNo}$, equivalent to `1 minus False Positive Rate` also known as `Specificity`\n",
    "6. _Precision_: When it predicts yes, how often is it correct? \n",
    "\n",
    "  * $\\frac{TruePositive}{PredictedYes}$\n",
    "  - high precision => algorithm returned more relevant results than irrelevant\n",
    "  - missing precision is `type 1 error`\n",
    "  - positive predicted value\n",
    "  - “how useful are the results”\n",
    "  - measure of exactness or quality\n",
    "\n",
    "7. _Prevalence_: How often does the yes condition actually occur in our sample?\n",
    "  * $\\frac{ActuallyYes}{Total}$\n",
    "\n",
    "8. $f_1 = 2 (\\frac{precision * recall}{precision + recall})$\n",
    "\n",
    "\n",
    "in the `class_weight` parameter of sklearn's logistic regression, the `class_weight` dict `{0:1, 1:100}` says \"**each false positive costs 1 dollar, each false negative costs 100 dollars**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thanks Chad for adding to this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
