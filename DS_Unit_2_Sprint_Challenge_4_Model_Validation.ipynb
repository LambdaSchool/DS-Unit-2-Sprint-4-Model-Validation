{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PC9RfopIWrc9"
   },
   "source": [
    " # Data Science Unit 2 Sprint Challenge 4 — Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UV7ArLFQN84W"
   },
   "source": [
    "Follow the instructions for each numbered part to earn a score of 2. See the bottom of the notebook for a list of ways you can earn a score of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAZcbTtiUlkI"
   },
   "source": [
    "## Predicting Blood Donations\n",
    "\n",
    "Our dataset is from a mobile blood donation vehicle in Taiwan. The Blood Transfusion Service Center drives to different universities and collects blood as part of a blood drive.\n",
    "\n",
    "The goal is to predict the last column, whether the donor made a donation in March 2007, using information about each donor's history. We'll measure success using recall score as the model evaluation metric.\n",
    "\n",
    "Good data-driven systems for tracking and predicting donations and supply needs can improve the entire supply chain, making sure that more patients get the blood transfusions they need.\n",
    "\n",
    "#### Run this cell to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvV9VORbxyvu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data')\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'Recency (months)': 'months_since_last_donation', \n",
    "    'Frequency (times)': 'number_of_donations', \n",
    "    'Monetary (c.c. blood)': 'total_volume_donated', \n",
    "    'Time (months)': 'months_since_first_donation', \n",
    "    'whether he/she donated blood in March 2007': 'made_donation_in_march_2007'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_since_last_donation</th>\n",
       "      <th>number_of_donations</th>\n",
       "      <th>total_volume_donated</th>\n",
       "      <th>months_since_first_donation</th>\n",
       "      <th>made_donation_in_march_2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   months_since_last_donation  number_of_donations  total_volume_donated  \\\n",
       "0                           2                   50                 12500   \n",
       "1                           0                   13                  3250   \n",
       "2                           1                   16                  4000   \n",
       "3                           2                   20                  5000   \n",
       "4                           1                   24                  6000   \n",
       "\n",
       "   months_since_first_donation  made_donation_in_march_2007  \n",
       "0                           98                            1  \n",
       "1                           28                            1  \n",
       "2                           35                            1  \n",
       "3                           45                            1  \n",
       "4                           77                            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxKfgx4ycb3c"
   },
   "source": [
    "## Part 1.1 — Begin with baselines\n",
    "\n",
    "What **accuracy score** would you get here with a **\"majority class baseline\"?** \n",
    " \n",
    "(You don't need to split the data into train and test sets yet. You can answer this question either with a scikit-learn function or with a pandas function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3oo31Remcq-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Class Baseline Accuracy: 0.7620320855614974\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df.drop(columns='made_donation_in_march_2007')\n",
    "y = df['made_donation_in_march_2007']\n",
    "majority_class = y.mode()[0]\n",
    "y_pred = np.full(shape=y.shape, fill_value=majority_class)\n",
    "\n",
    "print('Majority Class Baseline Accuracy:', accuracy_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_KdxE1TrcriI"
   },
   "source": [
    "What **recall score** would you get here with a **majority class baseline?**\n",
    "\n",
    "(You can answer this question either with a scikit-learn function or with no code, just your understanding of recall.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILS0fN0Cctyc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Class Baseline Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print('Majority Class Baseline Recall:', recall_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqYNDtwKYhji"
   },
   "source": [
    "## Part 1.2 — Split data\n",
    "\n",
    "In this Sprint Challenge, you will use \"Cross-Validation with Independent Test Set\" for your model evaluation protocol.\n",
    "\n",
    "First, **split the data into `X_train, X_test, y_train, y_test`**, with random shuffle. (You can include 75% of the data in the train set, and hold out 25% for the test set.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPKf86yDYf0t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((561, 4), (187, 4), (561,), (187,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, train_size=0.75, test_size=0.25, random_state=None, shuffle=True)\n",
    "    \n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:     made_donation_in_march_2007   R-squared:                       0.129\n",
      "Model:                                     OLS   Adj. R-squared:                  0.124\n",
      "Method:                          Least Squares   F-statistic:                     27.48\n",
      "Date:                         Fri, 01 Feb 2019   Prob (F-statistic):           1.38e-16\n",
      "Time:                                 10:56:32   Log-Likelihood:                -280.53\n",
      "No. Observations:                          561   AIC:                             569.1\n",
      "Df Residuals:                              557   BIC:                             586.4\n",
      "Df Model:                                    3                                         \n",
      "Covariance Type:                     nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "const                           0.3127      0.035      9.062      0.000       0.245       0.380\n",
      "months_since_last_donation     -0.0078      0.002     -3.502      0.000      -0.012      -0.003\n",
      "number_of_donations          4.292e-07   6.79e-08      6.319      0.000    2.96e-07    5.63e-07\n",
      "total_volume_donated            0.0001    1.7e-05      6.319      0.000    7.39e-05       0.000\n",
      "months_since_first_donation    -0.0043      0.001     -4.490      0.000      -0.006      -0.002\n",
      "==============================================================================\n",
      "Omnibus:                       80.649   Durbin-Watson:                   2.178\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              108.829\n",
      "Skew:                           1.059   Prob(JB):                     2.33e-24\n",
      "Kurtosis:                       2.589   Cond. No.                     1.16e+18\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.62e-27. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y_train, sm.add_constant(X_train))\n",
    "print(model.fit().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 degree polynomial has 1 features\n",
      "['1']\n",
      "\n",
      "\n",
      "1 degree polynomial has 5 features\n",
      "['1', 'months_since_last_donation', 'number_of_donations', 'total_volume_donated', 'months_since_first_donation']\n",
      "\n",
      "\n",
      "2 degree polynomial has 15 features\n",
      "['1', 'months_since_last_donation', 'number_of_donations', 'total_volume_donated', 'months_since_first_donation', 'months_since_last_donation^2', 'months_since_last_donation number_of_donations', 'months_since_last_donation total_volume_donated', 'months_since_last_donation months_since_first_donation', 'number_of_donations^2', 'number_of_donations total_volume_donated', 'number_of_donations months_since_first_donation', 'total_volume_donated^2', 'total_volume_donated months_since_first_donation', 'months_since_first_donation^2']\n",
      "\n",
      "\n",
      "3 degree polynomial has 35 features\n",
      "['1', 'months_since_last_donation', 'number_of_donations', 'total_volume_donated', 'months_since_first_donation', 'months_since_last_donation^2', 'months_since_last_donation number_of_donations', 'months_since_last_donation total_volume_donated', 'months_since_last_donation months_since_first_donation', 'number_of_donations^2', 'number_of_donations total_volume_donated', 'number_of_donations months_since_first_donation', 'total_volume_donated^2', 'total_volume_donated months_since_first_donation', 'months_since_first_donation^2', 'months_since_last_donation^3', 'months_since_last_donation^2 number_of_donations', 'months_since_last_donation^2 total_volume_donated', 'months_since_last_donation^2 months_since_first_donation', 'months_since_last_donation number_of_donations^2', 'months_since_last_donation number_of_donations total_volume_donated', 'months_since_last_donation number_of_donations months_since_first_donation', 'months_since_last_donation total_volume_donated^2', 'months_since_last_donation total_volume_donated months_since_first_donation', 'months_since_last_donation months_since_first_donation^2', 'number_of_donations^3', 'number_of_donations^2 total_volume_donated', 'number_of_donations^2 months_since_first_donation', 'number_of_donations total_volume_donated^2', 'number_of_donations total_volume_donated months_since_first_donation', 'number_of_donations months_since_first_donation^2', 'total_volume_donated^3', 'total_volume_donated^2 months_since_first_donation', 'total_volume_donated months_since_first_donation^2', 'months_since_first_donation^3']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "for degree in [0, 1, 2, 3]:\n",
    "    features = PolynomialFeatures(degree).fit(X_train).get_feature_names(X_train.columns)\n",
    "    print(f'{degree} degree polynomial has {len(features)} features')\n",
    "    print(features)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LogisticRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "degree = [0, 1, 2, 3]\n",
    "train_score, val_score = validation_curve(\n",
    "    PolynomialRegression(), X_train, y_train,\n",
    "    param_name='polynomialfeatures__degree', param_range=degree, \n",
    "    scoring='recall_macro', cv=3)\n",
    "\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('degree');\n",
    "\n",
    "#1st degree (original features have best validation score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_ATNJdqTCuZ"
   },
   "source": [
    "## Part 2.1 — Make a pipeline\n",
    "\n",
    "Make a **pipeline** which includes:\n",
    "- Preprocessing with any scikit-learn [**Scaler**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "- Feature selection with **[`SelectKBest`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)([`f_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html))**\n",
    "- Classification with [**`LogisticRegression`**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DRrVU5n5_Jw"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    RobustScaler(), \n",
    "    SelectKBest(f_classif), \n",
    "    LogisticRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vRkQHatglMG"
   },
   "source": [
    "## Part 2.2 — Do Grid Search Cross-Validation\n",
    "\n",
    "Do [**GridSearchCV**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) with your pipeline. Use **5 folds** and **recall score**.\n",
    "\n",
    "Include these **parameters for your grid:**\n",
    "\n",
    "#### `SelectKBest`\n",
    "- `k : 1, 2, 3, 4`\n",
    "\n",
    "#### `LogisticRegression`\n",
    "- `class_weight : None, 'balanced'`\n",
    "- `C : .0001, .001, .01, .1, 1.0, 10.0, 100.00, 1000.0, 10000.0`\n",
    "\n",
    "\n",
    "**Fit** on the appropriate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wgN8kG0ogBMH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_logisticregression__C</th>\n",
       "      <th>param_logisticregression__class_weight</th>\n",
       "      <th>param_selectkbest__k</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>2.319929e-05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>0.617140</td>\n",
       "      <td>0.634205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679134</td>\n",
       "      <td>0.054888</td>\n",
       "      <td>1</td>\n",
       "      <td>0.692048</td>\n",
       "      <td>0.710003</td>\n",
       "      <td>0.682687</td>\n",
       "      <td>0.689245</td>\n",
       "      <td>0.646804</td>\n",
       "      <td>0.684157</td>\n",
       "      <td>0.020754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>5.794307e-06</td>\n",
       "      <td>10000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logisticregression__C': 10000.0, 'logisticre...</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>0.634205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678823</td>\n",
       "      <td>0.051817</td>\n",
       "      <td>2</td>\n",
       "      <td>0.706754</td>\n",
       "      <td>0.715638</td>\n",
       "      <td>0.689788</td>\n",
       "      <td>0.696345</td>\n",
       "      <td>0.666327</td>\n",
       "      <td>0.694970</td>\n",
       "      <td>0.016824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>7.240392e-06</td>\n",
       "      <td>10</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logisticregression__C': 10.0, 'logisticregre...</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>0.634205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678823</td>\n",
       "      <td>0.051817</td>\n",
       "      <td>2</td>\n",
       "      <td>0.706754</td>\n",
       "      <td>0.715638</td>\n",
       "      <td>0.688321</td>\n",
       "      <td>0.696345</td>\n",
       "      <td>0.666327</td>\n",
       "      <td>0.694677</td>\n",
       "      <td>0.016924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>1.458841e-06</td>\n",
       "      <td>100</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logisticregression__C': 100.0, 'logisticregr...</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>0.634205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678823</td>\n",
       "      <td>0.051817</td>\n",
       "      <td>2</td>\n",
       "      <td>0.706754</td>\n",
       "      <td>0.715638</td>\n",
       "      <td>0.689788</td>\n",
       "      <td>0.696345</td>\n",
       "      <td>0.666327</td>\n",
       "      <td>0.694970</td>\n",
       "      <td>0.016824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>1.294620e-04</td>\n",
       "      <td>1000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logisticregression__C': 1000.0, 'logisticreg...</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>0.634205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678823</td>\n",
       "      <td>0.051817</td>\n",
       "      <td>2</td>\n",
       "      <td>0.706754</td>\n",
       "      <td>0.715638</td>\n",
       "      <td>0.689788</td>\n",
       "      <td>0.696345</td>\n",
       "      <td>0.666327</td>\n",
       "      <td>0.694970</td>\n",
       "      <td>0.016824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>2.113922e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'logisticregres...</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>0.634205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676474</td>\n",
       "      <td>0.051613</td>\n",
       "      <td>6</td>\n",
       "      <td>0.703813</td>\n",
       "      <td>0.718801</td>\n",
       "      <td>0.685389</td>\n",
       "      <td>0.694879</td>\n",
       "      <td>0.666327</td>\n",
       "      <td>0.693842</td>\n",
       "      <td>0.017618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>2.351296e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'logisticregres...</td>\n",
       "      <td>0.593885</td>\n",
       "      <td>0.640959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665925</td>\n",
       "      <td>0.057180</td>\n",
       "      <td>7</td>\n",
       "      <td>0.671242</td>\n",
       "      <td>0.678438</td>\n",
       "      <td>0.642324</td>\n",
       "      <td>0.670875</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.660563</td>\n",
       "      <td>0.016113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>1.893326e-05</td>\n",
       "      <td>10000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 10000.0, 'logisticre...</td>\n",
       "      <td>0.625108</td>\n",
       "      <td>0.635076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664646</td>\n",
       "      <td>0.059451</td>\n",
       "      <td>8</td>\n",
       "      <td>0.674619</td>\n",
       "      <td>0.672111</td>\n",
       "      <td>0.644795</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.649424</td>\n",
       "      <td>0.664711</td>\n",
       "      <td>0.014856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>3.410636e-06</td>\n",
       "      <td>100</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 100.0, 'logisticregr...</td>\n",
       "      <td>0.625108</td>\n",
       "      <td>0.635076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664646</td>\n",
       "      <td>0.059451</td>\n",
       "      <td>8</td>\n",
       "      <td>0.674619</td>\n",
       "      <td>0.672111</td>\n",
       "      <td>0.644795</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.649424</td>\n",
       "      <td>0.664711</td>\n",
       "      <td>0.014856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>4.452750e-06</td>\n",
       "      <td>1000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 1000.0, 'logisticreg...</td>\n",
       "      <td>0.625108</td>\n",
       "      <td>0.635076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664646</td>\n",
       "      <td>0.059451</td>\n",
       "      <td>8</td>\n",
       "      <td>0.674619</td>\n",
       "      <td>0.672111</td>\n",
       "      <td>0.644795</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.649424</td>\n",
       "      <td>0.664711</td>\n",
       "      <td>0.014856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>4.794526e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'logisticregres...</td>\n",
       "      <td>0.625108</td>\n",
       "      <td>0.635076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664646</td>\n",
       "      <td>0.059451</td>\n",
       "      <td>8</td>\n",
       "      <td>0.674619</td>\n",
       "      <td>0.672111</td>\n",
       "      <td>0.644795</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.649424</td>\n",
       "      <td>0.664711</td>\n",
       "      <td>0.014856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>3.664518e-06</td>\n",
       "      <td>10</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 10.0, 'logisticregre...</td>\n",
       "      <td>0.625108</td>\n",
       "      <td>0.635076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664646</td>\n",
       "      <td>0.059451</td>\n",
       "      <td>8</td>\n",
       "      <td>0.674619</td>\n",
       "      <td>0.672111</td>\n",
       "      <td>0.644795</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.649424</td>\n",
       "      <td>0.664711</td>\n",
       "      <td>0.014856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>4.438429e-06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>0.625108</td>\n",
       "      <td>0.635076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664646</td>\n",
       "      <td>0.059451</td>\n",
       "      <td>8</td>\n",
       "      <td>0.674619</td>\n",
       "      <td>0.672111</td>\n",
       "      <td>0.644795</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.649424</td>\n",
       "      <td>0.664711</td>\n",
       "      <td>0.014856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>3.429563e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 0.01, 'logisticregre...</td>\n",
       "      <td>0.625108</td>\n",
       "      <td>0.635076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664472</td>\n",
       "      <td>0.058984</td>\n",
       "      <td>14</td>\n",
       "      <td>0.678813</td>\n",
       "      <td>0.676279</td>\n",
       "      <td>0.650660</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.652126</td>\n",
       "      <td>0.668097</td>\n",
       "      <td>0.013794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>3.744310e-06</td>\n",
       "      <td>100</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 100.0, 'logisticregr...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.640959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>15</td>\n",
       "      <td>0.681155</td>\n",
       "      <td>0.673808</td>\n",
       "      <td>0.640857</td>\n",
       "      <td>0.670875</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.661326</td>\n",
       "      <td>0.017417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>1.129913e-05</td>\n",
       "      <td>10</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 10.0, 'logisticregre...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.640959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>15</td>\n",
       "      <td>0.679684</td>\n",
       "      <td>0.678438</td>\n",
       "      <td>0.640857</td>\n",
       "      <td>0.670875</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.661958</td>\n",
       "      <td>0.017864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>3.036820e-06</td>\n",
       "      <td>10000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 10000.0, 'logisticre...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.640959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>15</td>\n",
       "      <td>0.681155</td>\n",
       "      <td>0.673808</td>\n",
       "      <td>0.640857</td>\n",
       "      <td>0.670875</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.661326</td>\n",
       "      <td>0.017417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>1.994753e-06</td>\n",
       "      <td>100</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 100.0, 'logisticregr...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.640959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>15</td>\n",
       "      <td>0.681155</td>\n",
       "      <td>0.673808</td>\n",
       "      <td>0.640857</td>\n",
       "      <td>0.670875</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.661326</td>\n",
       "      <td>0.017417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>1.774795e-05</td>\n",
       "      <td>10</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 10.0, 'logisticregre...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.640959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>15</td>\n",
       "      <td>0.679684</td>\n",
       "      <td>0.678438</td>\n",
       "      <td>0.640857</td>\n",
       "      <td>0.670875</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.661958</td>\n",
       "      <td>0.017864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>1.683336e-05</td>\n",
       "      <td>1000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 1000.0, 'logisticreg...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.640959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>15</td>\n",
       "      <td>0.681155</td>\n",
       "      <td>0.673808</td>\n",
       "      <td>0.640857</td>\n",
       "      <td>0.670875</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.661326</td>\n",
       "      <td>0.017417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>6.143617e-07</td>\n",
       "      <td>1000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 1000.0, 'logisticreg...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.640959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>15</td>\n",
       "      <td>0.681155</td>\n",
       "      <td>0.673808</td>\n",
       "      <td>0.640857</td>\n",
       "      <td>0.670875</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.661326</td>\n",
       "      <td>0.017417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>4.139430e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'logisticregres...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.640959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>15</td>\n",
       "      <td>0.679684</td>\n",
       "      <td>0.678438</td>\n",
       "      <td>0.642324</td>\n",
       "      <td>0.670875</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.662251</td>\n",
       "      <td>0.017524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>7.528735e-05</td>\n",
       "      <td>10000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 10000.0, 'logisticre...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.640959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>15</td>\n",
       "      <td>0.681155</td>\n",
       "      <td>0.673808</td>\n",
       "      <td>0.640857</td>\n",
       "      <td>0.670875</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.661326</td>\n",
       "      <td>0.017417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>1.350973e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 0.01, 'logisticregre...</td>\n",
       "      <td>0.614987</td>\n",
       "      <td>0.681264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663390</td>\n",
       "      <td>0.071688</td>\n",
       "      <td>24</td>\n",
       "      <td>0.685893</td>\n",
       "      <td>0.662471</td>\n",
       "      <td>0.646193</td>\n",
       "      <td>0.689788</td>\n",
       "      <td>0.642337</td>\n",
       "      <td>0.665337</td>\n",
       "      <td>0.019617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>2.434206e-06</td>\n",
       "      <td>0.001</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 0.001, 'logisticregr...</td>\n",
       "      <td>0.606589</td>\n",
       "      <td>0.622440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662743</td>\n",
       "      <td>0.061457</td>\n",
       "      <td>25</td>\n",
       "      <td>0.678595</td>\n",
       "      <td>0.674582</td>\n",
       "      <td>0.645569</td>\n",
       "      <td>0.682375</td>\n",
       "      <td>0.639473</td>\n",
       "      <td>0.664119</td>\n",
       "      <td>0.017910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>8.265391e-06</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 0.0001, 'logisticreg...</td>\n",
       "      <td>0.606589</td>\n",
       "      <td>0.622440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662743</td>\n",
       "      <td>0.061457</td>\n",
       "      <td>25</td>\n",
       "      <td>0.678595</td>\n",
       "      <td>0.674582</td>\n",
       "      <td>0.645569</td>\n",
       "      <td>0.682375</td>\n",
       "      <td>0.639473</td>\n",
       "      <td>0.664119</td>\n",
       "      <td>0.017910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>5.666142e-06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>0.598622</td>\n",
       "      <td>0.640087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.655440</td>\n",
       "      <td>0.057742</td>\n",
       "      <td>27</td>\n",
       "      <td>0.664924</td>\n",
       "      <td>0.659688</td>\n",
       "      <td>0.631449</td>\n",
       "      <td>0.668717</td>\n",
       "      <td>0.634843</td>\n",
       "      <td>0.651924</td>\n",
       "      <td>0.015635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>2.574921e-06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>balanced</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logisticregression__C': 0.01, 'logisticregre...</td>\n",
       "      <td>0.596469</td>\n",
       "      <td>0.650980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653962</td>\n",
       "      <td>0.037526</td>\n",
       "      <td>28</td>\n",
       "      <td>0.688399</td>\n",
       "      <td>0.661698</td>\n",
       "      <td>0.659770</td>\n",
       "      <td>0.663164</td>\n",
       "      <td>0.660856</td>\n",
       "      <td>0.666777</td>\n",
       "      <td>0.010868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>2.075332e-05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>0.598622</td>\n",
       "      <td>0.629194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653265</td>\n",
       "      <td>0.055729</td>\n",
       "      <td>29</td>\n",
       "      <td>0.670806</td>\n",
       "      <td>0.671106</td>\n",
       "      <td>0.632372</td>\n",
       "      <td>0.670183</td>\n",
       "      <td>0.636309</td>\n",
       "      <td>0.656155</td>\n",
       "      <td>0.017858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>8.514573e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>balanced</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 0.001, 'logisticregr...</td>\n",
       "      <td>0.573213</td>\n",
       "      <td>0.698911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650931</td>\n",
       "      <td>0.063730</td>\n",
       "      <td>30</td>\n",
       "      <td>0.669063</td>\n",
       "      <td>0.659077</td>\n",
       "      <td>0.627675</td>\n",
       "      <td>0.683461</td>\n",
       "      <td>0.632304</td>\n",
       "      <td>0.654316</td>\n",
       "      <td>0.021372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>2.003851e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'logisticregres...</td>\n",
       "      <td>0.580965</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535142</td>\n",
       "      <td>0.032274</td>\n",
       "      <td>43</td>\n",
       "      <td>0.529902</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.532638</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.536025</td>\n",
       "      <td>0.005194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>3.380786e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'logisticregres...</td>\n",
       "      <td>0.580965</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535142</td>\n",
       "      <td>0.032274</td>\n",
       "      <td>43</td>\n",
       "      <td>0.529902</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.532638</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.536025</td>\n",
       "      <td>0.005194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>2.023577e-05</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 10.0, 'logisticregre...</td>\n",
       "      <td>0.575151</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533971</td>\n",
       "      <td>0.030655</td>\n",
       "      <td>45</td>\n",
       "      <td>0.529902</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.531172</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.535732</td>\n",
       "      <td>0.005414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>8.637731e-06</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 1000.0, 'logisticreg...</td>\n",
       "      <td>0.575151</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533971</td>\n",
       "      <td>0.030655</td>\n",
       "      <td>45</td>\n",
       "      <td>0.529902</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.531172</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.535732</td>\n",
       "      <td>0.005414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>2.170264e-05</td>\n",
       "      <td>10000</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 10000.0, 'logisticre...</td>\n",
       "      <td>0.575151</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533971</td>\n",
       "      <td>0.030655</td>\n",
       "      <td>45</td>\n",
       "      <td>0.529902</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.531172</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.535732</td>\n",
       "      <td>0.005414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>5.567539e-05</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 1000.0, 'logisticreg...</td>\n",
       "      <td>0.575151</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533971</td>\n",
       "      <td>0.030655</td>\n",
       "      <td>45</td>\n",
       "      <td>0.529902</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.531172</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.535732</td>\n",
       "      <td>0.005414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>7.920887e-05</td>\n",
       "      <td>100</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 100.0, 'logisticregr...</td>\n",
       "      <td>0.575151</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533971</td>\n",
       "      <td>0.030655</td>\n",
       "      <td>45</td>\n",
       "      <td>0.529902</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.531172</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.535732</td>\n",
       "      <td>0.005414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>2.469510e-05</td>\n",
       "      <td>100</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 100.0, 'logisticregr...</td>\n",
       "      <td>0.575151</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533971</td>\n",
       "      <td>0.030655</td>\n",
       "      <td>45</td>\n",
       "      <td>0.529902</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.531172</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.535732</td>\n",
       "      <td>0.005414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>6.603119e-06</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 10.0, 'logisticregre...</td>\n",
       "      <td>0.575151</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533971</td>\n",
       "      <td>0.030655</td>\n",
       "      <td>45</td>\n",
       "      <td>0.529902</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.531172</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.535732</td>\n",
       "      <td>0.005414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>2.077993e-05</td>\n",
       "      <td>10000</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 10000.0, 'logisticre...</td>\n",
       "      <td>0.575151</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533971</td>\n",
       "      <td>0.030655</td>\n",
       "      <td>45</td>\n",
       "      <td>0.529902</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.531172</td>\n",
       "      <td>0.542359</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.535732</td>\n",
       "      <td>0.005414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>2.640318e-06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>0.543928</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532379</td>\n",
       "      <td>0.017335</td>\n",
       "      <td>53</td>\n",
       "      <td>0.532625</td>\n",
       "      <td>0.534104</td>\n",
       "      <td>0.517052</td>\n",
       "      <td>0.535802</td>\n",
       "      <td>0.538734</td>\n",
       "      <td>0.531664</td>\n",
       "      <td>0.007583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>1.080055e-05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>0.543928</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531205</td>\n",
       "      <td>0.017576</td>\n",
       "      <td>54</td>\n",
       "      <td>0.532843</td>\n",
       "      <td>0.538734</td>\n",
       "      <td>0.530941</td>\n",
       "      <td>0.537499</td>\n",
       "      <td>0.534335</td>\n",
       "      <td>0.534870</td>\n",
       "      <td>0.002887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>2.294127e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logisticregression__C': 0.01, 'logisticregre...</td>\n",
       "      <td>0.531223</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530994</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>55</td>\n",
       "      <td>0.530937</td>\n",
       "      <td>0.530941</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.529475</td>\n",
       "      <td>0.534104</td>\n",
       "      <td>0.530647</td>\n",
       "      <td>0.002085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>2.810161e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 0.01, 'logisticregre...</td>\n",
       "      <td>0.531223</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528472</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>56</td>\n",
       "      <td>0.530937</td>\n",
       "      <td>0.523148</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>0.520216</td>\n",
       "      <td>0.534104</td>\n",
       "      <td>0.523533</td>\n",
       "      <td>0.008735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>4.604874e-06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 0.01, 'logisticregre...</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>57</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.523148</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.523148</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.005072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>2.646270e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 0.001, 'logisticregr...</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514821</td>\n",
       "      <td>0.007402</td>\n",
       "      <td>58</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.524845</td>\n",
       "      <td>0.517006</td>\n",
       "      <td>0.005211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>1.753633e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logisticregression__C': 0.001, 'logisticregr...</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511124</td>\n",
       "      <td>0.009069</td>\n",
       "      <td>59</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.504630</td>\n",
       "      <td>0.504630</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>0.530941</td>\n",
       "      <td>0.513596</td>\n",
       "      <td>0.010047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>2.322936e-05</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logisticregression__C': 0.0001, 'logisticreg...</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511124</td>\n",
       "      <td>0.009069</td>\n",
       "      <td>59</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>0.504630</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.524845</td>\n",
       "      <td>0.515154</td>\n",
       "      <td>0.007240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>1.040161e-05</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logisticregression__C': 0.0001, 'logisticreg...</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507427</td>\n",
       "      <td>0.009076</td>\n",
       "      <td>61</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.523148</td>\n",
       "      <td>0.506481</td>\n",
       "      <td>0.009072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>2.953312e-06</td>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 0.001, 'logisticregr...</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507427</td>\n",
       "      <td>0.009076</td>\n",
       "      <td>61</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>0.504630</td>\n",
       "      <td>0.005856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>5.689101e-05</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logisticregression__C': 0.0001, 'logisticreg...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.504630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>0.503704</td>\n",
       "      <td>0.005399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.001511</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>1.530343e-06</td>\n",
       "      <td>10000</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 10000.0, 'logisticre...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>2.934002e-06</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 1000.0, 'logisticreg...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>4.544371e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 0.01, 'logisticregre...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>4.768372e-07</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 10.0, 'logisticregre...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>8.286546e-06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>1.506051e-05</td>\n",
       "      <td>100</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 100.0, 'logisticregr...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>1.091530e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'logisticregres...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>2.070813e-06</td>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 0.001, 'logisticregr...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>2.355913e-05</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logisticregression__C': 0.0001, 'logisticreg...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "31       0.001727      0.000092         0.000474    2.319929e-05   \n",
       "71       0.001669      0.000058         0.000443    5.794307e-06   \n",
       "47       0.001676      0.000028         0.000453    7.240392e-06   \n",
       "55       0.001679      0.000064         0.000441    1.458841e-06   \n",
       "63       0.001718      0.000059         0.000509    1.294620e-04   \n",
       "39       0.001671      0.000023         0.000453    2.113922e-05   \n",
       "38       0.001656      0.000093         0.000453    2.351296e-05   \n",
       "68       0.001602      0.000028         0.000458    1.893326e-05   \n",
       "52       0.001621      0.000051         0.000443    3.410636e-06   \n",
       "60       0.001569      0.000003         0.000442    4.452750e-06   \n",
       "36       0.001587      0.000033         0.000445    4.794526e-06   \n",
       "44       0.001621      0.000105         0.000440    3.664518e-06   \n",
       "28       0.001601      0.000058         0.000449    4.438429e-06   \n",
       "20       0.001687      0.000113         0.000468    3.429563e-05   \n",
       "53       0.001598      0.000028         0.000445    3.744310e-06   \n",
       "46       0.001698      0.000125         0.000452    1.129913e-05   \n",
       "69       0.001642      0.000040         0.000452    3.036820e-06   \n",
       "54       0.001602      0.000006         0.000440    1.994753e-06   \n",
       "45       0.001637      0.000053         0.000459    1.774795e-05   \n",
       "61       0.001637      0.000038         0.000457    1.683336e-05   \n",
       "62       0.001631      0.000013         0.000445    6.143617e-07   \n",
       "37       0.001592      0.000005         0.000446    4.139430e-06   \n",
       "70       0.001803      0.000118         0.000510    7.528735e-05   \n",
       "21       0.001594      0.000024         0.000451    1.350973e-05   \n",
       "12       0.001550      0.000014         0.000439    2.434206e-06   \n",
       "4        0.001551      0.000026         0.000449    8.265391e-06   \n",
       "30       0.001629      0.000029         0.000447    5.666142e-06   \n",
       "23       0.001635      0.000042         0.000447    2.574921e-06   \n",
       "29       0.001692      0.000139         0.000455    2.075332e-05   \n",
       "13       0.001642      0.000154         0.000488    8.514573e-05   \n",
       "..            ...           ...              ...             ...   \n",
       "34       0.001547      0.000027         0.000441    2.003851e-06   \n",
       "33       0.001545      0.000051         0.000460    3.380786e-05   \n",
       "41       0.001524      0.000011         0.000457    2.023577e-05   \n",
       "57       0.001589      0.000054         0.000446    8.637731e-06   \n",
       "66       0.001590      0.000098         0.000449    2.170264e-05   \n",
       "58       0.001572      0.000040         0.000479    5.567539e-05   \n",
       "49       0.001639      0.000074         0.000545    7.920887e-05   \n",
       "50       0.001608      0.000049         0.000478    2.469510e-05   \n",
       "42       0.001572      0.000040         0.000445    6.603119e-06   \n",
       "65       0.001540      0.000031         0.000452    2.077993e-05   \n",
       "25       0.001519      0.000007         0.000444    2.640318e-06   \n",
       "26       0.001597      0.000092         0.000453    1.080055e-05   \n",
       "19       0.001752      0.000298         0.000472    2.294127e-05   \n",
       "18       0.001551      0.000067         0.000465    2.810161e-05   \n",
       "17       0.001519      0.000024         0.000444    4.604874e-06   \n",
       "10       0.001556      0.000038         0.000469    2.646270e-05   \n",
       "11       0.001509      0.000021         0.000455    1.753633e-05   \n",
       "2        0.001830      0.000141         0.000545    2.322936e-05   \n",
       "3        0.001703      0.000064         0.000495    1.040161e-05   \n",
       "9        0.001518      0.000014         0.000455    2.953312e-06   \n",
       "1        0.002062      0.000091         0.000640    5.689101e-05   \n",
       "64       0.001511      0.000028         0.000441    1.530343e-06   \n",
       "56       0.001530      0.000044         0.000443    2.934002e-06   \n",
       "16       0.001500      0.000016         0.000491    4.544371e-05   \n",
       "40       0.001515      0.000022         0.000443    4.768372e-07   \n",
       "24       0.001541      0.000067         0.000446    8.286546e-06   \n",
       "48       0.001535      0.000034         0.000461    1.506051e-05   \n",
       "32       0.001493      0.000006         0.000441    1.091530e-06   \n",
       "8        0.001489      0.000016         0.000450    2.070813e-06   \n",
       "0        0.002343      0.000203         0.000650    2.355913e-05   \n",
       "\n",
       "   param_logisticregression__C param_logisticregression__class_weight  \\\n",
       "31                         0.1                               balanced   \n",
       "71                       10000                               balanced   \n",
       "47                          10                               balanced   \n",
       "55                         100                               balanced   \n",
       "63                        1000                               balanced   \n",
       "39                           1                               balanced   \n",
       "38                           1                               balanced   \n",
       "68                       10000                               balanced   \n",
       "52                         100                               balanced   \n",
       "60                        1000                               balanced   \n",
       "36                           1                               balanced   \n",
       "44                          10                               balanced   \n",
       "28                         0.1                               balanced   \n",
       "20                        0.01                               balanced   \n",
       "53                         100                               balanced   \n",
       "46                          10                               balanced   \n",
       "69                       10000                               balanced   \n",
       "54                         100                               balanced   \n",
       "45                          10                               balanced   \n",
       "61                        1000                               balanced   \n",
       "62                        1000                               balanced   \n",
       "37                           1                               balanced   \n",
       "70                       10000                               balanced   \n",
       "21                        0.01                               balanced   \n",
       "12                       0.001                               balanced   \n",
       "4                       0.0001                               balanced   \n",
       "30                         0.1                               balanced   \n",
       "23                        0.01                               balanced   \n",
       "29                         0.1                               balanced   \n",
       "13                       0.001                               balanced   \n",
       "..                         ...                                    ...   \n",
       "34                           1                                   None   \n",
       "33                           1                                   None   \n",
       "41                          10                                   None   \n",
       "57                        1000                                   None   \n",
       "66                       10000                                   None   \n",
       "58                        1000                                   None   \n",
       "49                         100                                   None   \n",
       "50                         100                                   None   \n",
       "42                          10                                   None   \n",
       "65                       10000                                   None   \n",
       "25                         0.1                                   None   \n",
       "26                         0.1                                   None   \n",
       "19                        0.01                                   None   \n",
       "18                        0.01                                   None   \n",
       "17                        0.01                                   None   \n",
       "10                       0.001                                   None   \n",
       "11                       0.001                                   None   \n",
       "2                       0.0001                                   None   \n",
       "3                       0.0001                                   None   \n",
       "9                        0.001                                   None   \n",
       "1                       0.0001                                   None   \n",
       "64                       10000                                   None   \n",
       "56                        1000                                   None   \n",
       "16                        0.01                                   None   \n",
       "40                          10                                   None   \n",
       "24                         0.1                                   None   \n",
       "48                         100                                   None   \n",
       "32                           1                                   None   \n",
       "8                        0.001                                   None   \n",
       "0                       0.0001                                   None   \n",
       "\n",
       "   param_selectkbest__k                                             params  \\\n",
       "31                    4  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "71                    4  {'logisticregression__C': 10000.0, 'logisticre...   \n",
       "47                    4  {'logisticregression__C': 10.0, 'logisticregre...   \n",
       "55                    4  {'logisticregression__C': 100.0, 'logisticregr...   \n",
       "63                    4  {'logisticregression__C': 1000.0, 'logisticreg...   \n",
       "39                    4  {'logisticregression__C': 1.0, 'logisticregres...   \n",
       "38                    3  {'logisticregression__C': 1.0, 'logisticregres...   \n",
       "68                    1  {'logisticregression__C': 10000.0, 'logisticre...   \n",
       "52                    1  {'logisticregression__C': 100.0, 'logisticregr...   \n",
       "60                    1  {'logisticregression__C': 1000.0, 'logisticreg...   \n",
       "36                    1  {'logisticregression__C': 1.0, 'logisticregres...   \n",
       "44                    1  {'logisticregression__C': 10.0, 'logisticregre...   \n",
       "28                    1  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "20                    1  {'logisticregression__C': 0.01, 'logisticregre...   \n",
       "53                    2  {'logisticregression__C': 100.0, 'logisticregr...   \n",
       "46                    3  {'logisticregression__C': 10.0, 'logisticregre...   \n",
       "69                    2  {'logisticregression__C': 10000.0, 'logisticre...   \n",
       "54                    3  {'logisticregression__C': 100.0, 'logisticregr...   \n",
       "45                    2  {'logisticregression__C': 10.0, 'logisticregre...   \n",
       "61                    2  {'logisticregression__C': 1000.0, 'logisticreg...   \n",
       "62                    3  {'logisticregression__C': 1000.0, 'logisticreg...   \n",
       "37                    2  {'logisticregression__C': 1.0, 'logisticregres...   \n",
       "70                    3  {'logisticregression__C': 10000.0, 'logisticre...   \n",
       "21                    2  {'logisticregression__C': 0.01, 'logisticregre...   \n",
       "12                    1  {'logisticregression__C': 0.001, 'logisticregr...   \n",
       "4                     1  {'logisticregression__C': 0.0001, 'logisticreg...   \n",
       "30                    3  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "23                    4  {'logisticregression__C': 0.01, 'logisticregre...   \n",
       "29                    2  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "13                    2  {'logisticregression__C': 0.001, 'logisticregr...   \n",
       "..                  ...                                                ...   \n",
       "34                    3  {'logisticregression__C': 1.0, 'logisticregres...   \n",
       "33                    2  {'logisticregression__C': 1.0, 'logisticregres...   \n",
       "41                    2  {'logisticregression__C': 10.0, 'logisticregre...   \n",
       "57                    2  {'logisticregression__C': 1000.0, 'logisticreg...   \n",
       "66                    3  {'logisticregression__C': 10000.0, 'logisticre...   \n",
       "58                    3  {'logisticregression__C': 1000.0, 'logisticreg...   \n",
       "49                    2  {'logisticregression__C': 100.0, 'logisticregr...   \n",
       "50                    3  {'logisticregression__C': 100.0, 'logisticregr...   \n",
       "42                    3  {'logisticregression__C': 10.0, 'logisticregre...   \n",
       "65                    2  {'logisticregression__C': 10000.0, 'logisticre...   \n",
       "25                    2  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "26                    3  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "19                    4  {'logisticregression__C': 0.01, 'logisticregre...   \n",
       "18                    3  {'logisticregression__C': 0.01, 'logisticregre...   \n",
       "17                    2  {'logisticregression__C': 0.01, 'logisticregre...   \n",
       "10                    3  {'logisticregression__C': 0.001, 'logisticregr...   \n",
       "11                    4  {'logisticregression__C': 0.001, 'logisticregr...   \n",
       "2                     3  {'logisticregression__C': 0.0001, 'logisticreg...   \n",
       "3                     4  {'logisticregression__C': 0.0001, 'logisticreg...   \n",
       "9                     2  {'logisticregression__C': 0.001, 'logisticregr...   \n",
       "1                     2  {'logisticregression__C': 0.0001, 'logisticreg...   \n",
       "64                    1  {'logisticregression__C': 10000.0, 'logisticre...   \n",
       "56                    1  {'logisticregression__C': 1000.0, 'logisticreg...   \n",
       "16                    1  {'logisticregression__C': 0.01, 'logisticregre...   \n",
       "40                    1  {'logisticregression__C': 10.0, 'logisticregre...   \n",
       "24                    1  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "48                    1  {'logisticregression__C': 100.0, 'logisticregr...   \n",
       "32                    1  {'logisticregression__C': 1.0, 'logisticregres...   \n",
       "8                     1  {'logisticregression__C': 0.001, 'logisticregr...   \n",
       "0                     1  {'logisticregression__C': 0.0001, 'logisticreg...   \n",
       "\n",
       "    split0_test_score  split1_test_score       ...         mean_test_score  \\\n",
       "31           0.617140           0.634205       ...                0.679134   \n",
       "71           0.629845           0.634205       ...                0.678823   \n",
       "47           0.629845           0.634205       ...                0.678823   \n",
       "55           0.629845           0.634205       ...                0.678823   \n",
       "63           0.629845           0.634205       ...                0.678823   \n",
       "39           0.629845           0.634205       ...                0.676474   \n",
       "38           0.593885           0.640959       ...                0.665925   \n",
       "68           0.625108           0.635076       ...                0.664646   \n",
       "52           0.625108           0.635076       ...                0.664646   \n",
       "60           0.625108           0.635076       ...                0.664646   \n",
       "36           0.625108           0.635076       ...                0.664646   \n",
       "44           0.625108           0.635076       ...                0.664646   \n",
       "28           0.625108           0.635076       ...                0.664646   \n",
       "20           0.625108           0.635076       ...                0.664472   \n",
       "53           0.583333           0.640959       ...                0.663800   \n",
       "46           0.583333           0.640959       ...                0.663800   \n",
       "69           0.583333           0.640959       ...                0.663800   \n",
       "54           0.583333           0.640959       ...                0.663800   \n",
       "45           0.583333           0.640959       ...                0.663800   \n",
       "61           0.583333           0.640959       ...                0.663800   \n",
       "62           0.583333           0.640959       ...                0.663800   \n",
       "37           0.583333           0.640959       ...                0.663800   \n",
       "70           0.583333           0.640959       ...                0.663800   \n",
       "21           0.614987           0.681264       ...                0.663390   \n",
       "12           0.606589           0.622440       ...                0.662743   \n",
       "4            0.606589           0.622440       ...                0.662743   \n",
       "30           0.598622           0.640087       ...                0.655440   \n",
       "23           0.596469           0.650980       ...                0.653962   \n",
       "29           0.598622           0.629194       ...                0.653265   \n",
       "13           0.573213           0.698911       ...                0.650931   \n",
       "..                ...                ...       ...                     ...   \n",
       "34           0.580965           0.482353       ...                0.535142   \n",
       "33           0.580965           0.482353       ...                0.535142   \n",
       "41           0.575151           0.482353       ...                0.533971   \n",
       "57           0.575151           0.482353       ...                0.533971   \n",
       "66           0.575151           0.482353       ...                0.533971   \n",
       "58           0.575151           0.482353       ...                0.533971   \n",
       "49           0.575151           0.482353       ...                0.533971   \n",
       "50           0.575151           0.482353       ...                0.533971   \n",
       "42           0.575151           0.482353       ...                0.533971   \n",
       "65           0.575151           0.482353       ...                0.533971   \n",
       "25           0.543928           0.500000       ...                0.532379   \n",
       "26           0.543928           0.500000       ...                0.531205   \n",
       "19           0.531223           0.518519       ...                0.530994   \n",
       "18           0.531223           0.500000       ...                0.528472   \n",
       "17           0.518519           0.500000       ...                0.518519   \n",
       "10           0.518519           0.500000       ...                0.514821   \n",
       "11           0.518519           0.500000       ...                0.511124   \n",
       "2            0.518519           0.500000       ...                0.511124   \n",
       "3            0.518519           0.500000       ...                0.507427   \n",
       "9            0.518519           0.500000       ...                0.507427   \n",
       "1            0.500000           0.500000       ...                0.500000   \n",
       "64           0.500000           0.500000       ...                0.500000   \n",
       "56           0.500000           0.500000       ...                0.500000   \n",
       "16           0.500000           0.500000       ...                0.500000   \n",
       "40           0.500000           0.500000       ...                0.500000   \n",
       "24           0.500000           0.500000       ...                0.500000   \n",
       "48           0.500000           0.500000       ...                0.500000   \n",
       "32           0.500000           0.500000       ...                0.500000   \n",
       "8            0.500000           0.500000       ...                0.500000   \n",
       "0            0.500000           0.500000       ...                0.500000   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "31        0.054888                1            0.692048            0.710003   \n",
       "71        0.051817                2            0.706754            0.715638   \n",
       "47        0.051817                2            0.706754            0.715638   \n",
       "55        0.051817                2            0.706754            0.715638   \n",
       "63        0.051817                2            0.706754            0.715638   \n",
       "39        0.051613                6            0.703813            0.718801   \n",
       "38        0.057180                7            0.671242            0.678438   \n",
       "68        0.059451                8            0.674619            0.672111   \n",
       "52        0.059451                8            0.674619            0.672111   \n",
       "60        0.059451                8            0.674619            0.672111   \n",
       "36        0.059451                8            0.674619            0.672111   \n",
       "44        0.059451                8            0.674619            0.672111   \n",
       "28        0.059451                8            0.674619            0.672111   \n",
       "20        0.058984               14            0.678813            0.676279   \n",
       "53        0.059948               15            0.681155            0.673808   \n",
       "46        0.059948               15            0.679684            0.678438   \n",
       "69        0.059948               15            0.681155            0.673808   \n",
       "54        0.059948               15            0.681155            0.673808   \n",
       "45        0.059948               15            0.679684            0.678438   \n",
       "61        0.059948               15            0.681155            0.673808   \n",
       "62        0.059948               15            0.681155            0.673808   \n",
       "37        0.059948               15            0.679684            0.678438   \n",
       "70        0.059948               15            0.681155            0.673808   \n",
       "21        0.071688               24            0.685893            0.662471   \n",
       "12        0.061457               25            0.678595            0.674582   \n",
       "4         0.061457               25            0.678595            0.674582   \n",
       "30        0.057742               27            0.664924            0.659688   \n",
       "23        0.037526               28            0.688399            0.661698   \n",
       "29        0.055729               29            0.670806            0.671106   \n",
       "13        0.063730               30            0.669063            0.659077   \n",
       "..             ...              ...                 ...                 ...   \n",
       "34        0.032274               43            0.529902            0.542128   \n",
       "33        0.032274               43            0.529902            0.542128   \n",
       "41        0.030655               45            0.529902            0.542128   \n",
       "57        0.030655               45            0.529902            0.542128   \n",
       "66        0.030655               45            0.529902            0.542128   \n",
       "58        0.030655               45            0.529902            0.542128   \n",
       "49        0.030655               45            0.529902            0.542128   \n",
       "50        0.030655               45            0.529902            0.542128   \n",
       "42        0.030655               45            0.529902            0.542128   \n",
       "65        0.030655               45            0.529902            0.542128   \n",
       "25        0.017335               53            0.532625            0.534104   \n",
       "26        0.017576               54            0.532843            0.538734   \n",
       "19        0.011794               55            0.530937            0.530941   \n",
       "18        0.018554               56            0.530937            0.523148   \n",
       "17        0.020268               57            0.518519            0.523148   \n",
       "10        0.007402               58            0.518519            0.513889   \n",
       "11        0.009069               59            0.518519            0.504630   \n",
       "2         0.009069               59            0.518519            0.509259   \n",
       "3         0.009076               61            0.509259            0.500000   \n",
       "9         0.009076               61            0.509259            0.500000   \n",
       "1         0.000000               63            0.504630            0.500000   \n",
       "64        0.000000               63            0.500000            0.500000   \n",
       "56        0.000000               63            0.500000            0.500000   \n",
       "16        0.000000               63            0.500000            0.500000   \n",
       "40        0.000000               63            0.500000            0.500000   \n",
       "24        0.000000               63            0.500000            0.500000   \n",
       "48        0.000000               63            0.500000            0.500000   \n",
       "32        0.000000               63            0.500000            0.500000   \n",
       "8         0.000000               63            0.500000            0.500000   \n",
       "0         0.000000               63            0.500000            0.500000   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "31            0.682687            0.689245            0.646804   \n",
       "71            0.689788            0.696345            0.666327   \n",
       "47            0.688321            0.696345            0.666327   \n",
       "55            0.689788            0.696345            0.666327   \n",
       "63            0.689788            0.696345            0.666327   \n",
       "39            0.685389            0.694879            0.666327   \n",
       "38            0.642324            0.670875            0.639934   \n",
       "68            0.644795            0.682606            0.649424   \n",
       "52            0.644795            0.682606            0.649424   \n",
       "60            0.644795            0.682606            0.649424   \n",
       "36            0.644795            0.682606            0.649424   \n",
       "44            0.644795            0.682606            0.649424   \n",
       "28            0.644795            0.682606            0.649424   \n",
       "20            0.650660            0.682606            0.652126   \n",
       "53            0.640857            0.670875            0.639934   \n",
       "46            0.640857            0.670875            0.639934   \n",
       "69            0.640857            0.670875            0.639934   \n",
       "54            0.640857            0.670875            0.639934   \n",
       "45            0.640857            0.670875            0.639934   \n",
       "61            0.640857            0.670875            0.639934   \n",
       "62            0.640857            0.670875            0.639934   \n",
       "37            0.642324            0.670875            0.639934   \n",
       "70            0.640857            0.670875            0.639934   \n",
       "21            0.646193            0.689788            0.642337   \n",
       "12            0.645569            0.682375            0.639473   \n",
       "4             0.645569            0.682375            0.639473   \n",
       "30            0.631449            0.668717            0.634843   \n",
       "23            0.659770            0.663164            0.660856   \n",
       "29            0.632372            0.670183            0.636309   \n",
       "13            0.627675            0.683461            0.632304   \n",
       "..                 ...                 ...                 ...   \n",
       "34            0.532638            0.542359            0.533100   \n",
       "33            0.532638            0.542359            0.533100   \n",
       "41            0.531172            0.542359            0.533100   \n",
       "57            0.531172            0.542359            0.533100   \n",
       "66            0.531172            0.542359            0.533100   \n",
       "58            0.531172            0.542359            0.533100   \n",
       "49            0.531172            0.542359            0.533100   \n",
       "50            0.531172            0.542359            0.533100   \n",
       "42            0.531172            0.542359            0.533100   \n",
       "65            0.531172            0.542359            0.533100   \n",
       "25            0.517052            0.535802            0.538734   \n",
       "26            0.530941            0.537499            0.534335   \n",
       "19            0.527778            0.529475            0.534104   \n",
       "18            0.509259            0.520216            0.534104   \n",
       "17            0.509259            0.518519            0.523148   \n",
       "10            0.509259            0.518519            0.524845   \n",
       "11            0.504630            0.509259            0.530941   \n",
       "2             0.504630            0.518519            0.524845   \n",
       "3             0.500000            0.500000            0.523148   \n",
       "9             0.500000            0.500000            0.513889   \n",
       "1             0.500000            0.500000            0.513889   \n",
       "64            0.500000            0.500000            0.500000   \n",
       "56            0.500000            0.500000            0.500000   \n",
       "16            0.500000            0.500000            0.500000   \n",
       "40            0.500000            0.500000            0.500000   \n",
       "24            0.500000            0.500000            0.500000   \n",
       "48            0.500000            0.500000            0.500000   \n",
       "32            0.500000            0.500000            0.500000   \n",
       "8             0.500000            0.500000            0.500000   \n",
       "0             0.500000            0.500000            0.500000   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "31          0.684157         0.020754  \n",
       "71          0.694970         0.016824  \n",
       "47          0.694677         0.016924  \n",
       "55          0.694970         0.016824  \n",
       "63          0.694970         0.016824  \n",
       "39          0.693842         0.017618  \n",
       "38          0.660563         0.016113  \n",
       "68          0.664711         0.014856  \n",
       "52          0.664711         0.014856  \n",
       "60          0.664711         0.014856  \n",
       "36          0.664711         0.014856  \n",
       "44          0.664711         0.014856  \n",
       "28          0.664711         0.014856  \n",
       "20          0.668097         0.013794  \n",
       "53          0.661326         0.017417  \n",
       "46          0.661958         0.017864  \n",
       "69          0.661326         0.017417  \n",
       "54          0.661326         0.017417  \n",
       "45          0.661958         0.017864  \n",
       "61          0.661326         0.017417  \n",
       "62          0.661326         0.017417  \n",
       "37          0.662251         0.017524  \n",
       "70          0.661326         0.017417  \n",
       "21          0.665337         0.019617  \n",
       "12          0.664119         0.017910  \n",
       "4           0.664119         0.017910  \n",
       "30          0.651924         0.015635  \n",
       "23          0.666777         0.010868  \n",
       "29          0.656155         0.017858  \n",
       "13          0.654316         0.021372  \n",
       "..               ...              ...  \n",
       "34          0.536025         0.005194  \n",
       "33          0.536025         0.005194  \n",
       "41          0.535732         0.005414  \n",
       "57          0.535732         0.005414  \n",
       "66          0.535732         0.005414  \n",
       "58          0.535732         0.005414  \n",
       "49          0.535732         0.005414  \n",
       "50          0.535732         0.005414  \n",
       "42          0.535732         0.005414  \n",
       "65          0.535732         0.005414  \n",
       "25          0.531664         0.007583  \n",
       "26          0.534870         0.002887  \n",
       "19          0.530647         0.002085  \n",
       "18          0.523533         0.008735  \n",
       "17          0.518519         0.005072  \n",
       "10          0.517006         0.005211  \n",
       "11          0.513596         0.010047  \n",
       "2           0.515154         0.007240  \n",
       "3           0.506481         0.009072  \n",
       "9           0.504630         0.005856  \n",
       "1           0.503704         0.005399  \n",
       "64          0.500000         0.000000  \n",
       "56          0.500000         0.000000  \n",
       "16          0.500000         0.000000  \n",
       "40          0.500000         0.000000  \n",
       "24          0.500000         0.000000  \n",
       "48          0.500000         0.000000  \n",
       "32          0.500000         0.000000  \n",
       "8           0.500000         0.000000  \n",
       "0           0.500000         0.000000  \n",
       "\n",
       "[72 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'selectkbest__k': [1,2,3,4], \n",
    "    'logisticregression__class_weight': [None,'balanced'],\n",
    "    'logisticregression__C': [.0001, .001, .01, .1, 1.0, 10.0, 100.00, 1000.0, 10000.0]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,param_grid=param_grid, cv=5,\n",
    "                 scoring='recall_macro')\n",
    "\n",
    "gs.fit(X_train,y_train)\n",
    "\n",
    "pd.DataFrame(gs.cv_results_).sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "urY_Wp3AiF83"
   },
   "source": [
    "## Part 3 — Show best score and parameters\n",
    "\n",
    "Display your **best cross-validation score**, and the **best parameters** (the values of `k, class_weight, C`) from the grid search.\n",
    "\n",
    "(You're not evaluated here on how good your score is, or which parameters you find. You're only evaluated on being able to display the information. There are several ways you can get the information, and any way is acceptable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAxxkjG7gACP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6791343691829194\n",
      "Parameters for best score: {'logisticregression__C': 0.1, 'logisticregression__class_weight': 'balanced', 'selectkbest__k': 4}\n"
     ]
    }
   ],
   "source": [
    "print('Best score:',gs.best_score_)\n",
    "print('Parameters for best score:', gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected:\n",
      "months_since_last_donation\n",
      "number_of_donations\n",
      "total_volume_donated\n",
      "months_since_first_donation\n",
      "\n",
      "Features not selected:\n"
     ]
    }
   ],
   "source": [
    "# Which features were selected?\n",
    "selector = gs.best_estimator_.named_steps['selectkbest']\n",
    "all_names = X_train.columns\n",
    "selected_mask = selector.get_support()\n",
    "selected_names = all_names[selected_mask]\n",
    "unselected_names = all_names[~selected_mask]\n",
    "\n",
    "print('Features selected:')\n",
    "for name in selected_names:\n",
    "    print(name)\n",
    "\n",
    "print()\n",
    "print('Features not selected:')\n",
    "for name in unselected_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.7088985788113695\n"
     ]
    }
   ],
   "source": [
    "#Testing model on Test data\n",
    "\n",
    "test_score = gs.score(X_test, y_test)\n",
    "print('Test Score:', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkyHoRIbEgRR"
   },
   "source": [
    "## Part 4 — Calculate classification metrics from a confusion matrix\n",
    "\n",
    "Suppose this is the confusion matrix for your binary classification model:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" rowspan=\"2\"></th>\n",
    "    <th colspan=\"2\">Predicted</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Negative</th>\n",
    "    <th>Positive</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Actual</th>\n",
    "    <th>Negative</th>\n",
    "    <td>85</td>\n",
    "    <td>58</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Positive</th>\n",
    "    <td>8</td>\n",
    "    <td>36</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LhyMM5H-JpVB"
   },
   "source": [
    "Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZPwqdh2KUcB"
   },
   "outputs": [],
   "source": [
    "fn = 8\n",
    "fp = 58\n",
    "tn = 85\n",
    "tp = 36\n",
    "\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "print('Accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRWLfGcGKeQw"
   },
   "source": [
    "Calculate precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-FEZ4i_Kf_n"
   },
   "outputs": [],
   "source": [
    "precision = tp / (tp+fp)\n",
    "print('Precision:',precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_mH2NYDKi2C"
   },
   "source": [
    "Calculate recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4_wJGyjKkXJ"
   },
   "outputs": [],
   "source": [
    "recall = tp / (tp+fn)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = (2*tp) / ((2*tp)+fp+fn)\n",
    "print('F1 Score:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = fn / (fn+tp)\n",
    "print('False Positive Rate:',fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KEaWsk5Kk9W"
   },
   "source": [
    "## BONUS — How you can earn a score of 3\n",
    "\n",
    "### Part 1\n",
    "Do feature engineering, to try improving your cross-validation score.\n",
    "\n",
    "### Part 2\n",
    "Add transformations in your pipeline and parameters in your grid, to try improving your cross-validation score.\n",
    "\n",
    "### Part 3\n",
    "Show names of selected features. Then do a final evaluation on the test set — what is the test score?\n",
    "\n",
    "### Part 4\n",
    "Calculate F1 score and False Positive Rate. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS_Unit_2_Sprint_Challenge_4_Model_Validation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
