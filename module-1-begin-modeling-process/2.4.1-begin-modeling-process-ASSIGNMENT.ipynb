{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT options\n",
    "\n",
    "- Replicate the lesson code. [Do it \"the hard way\" or with the \"Benjamin Franklin method.\"](https://docs.google.com/document/d/1ubOw9B3Hfip27hF2ZFnW3a3z9xAgrUDRReOEo-FHCVs/edit)\n",
    "- Apply the lesson to other datasets you've worked with before, and compare results.\n",
    "- Choose how to split the Bank Marketing dataset. Train and validate baseline models.\n",
    "- Get weather data for your own area and calculate both baselines.  _\"One (persistence) predicts that the weather tomorrow is going to be whatever it was today. The other (climatology) predicts whatever the average historical weather has been on this day from prior years.\"_ What is the mean absolute error for each baseline? What if you average the two together? \n",
    "- When would this notebook's pipelines fail? How could you fix them? Add more [preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) and [imputation](https://scikit-learn.org/stable/modules/impute.html) to your [pipelines](https://scikit-learn.org/stable/modules/compose.html) with scikit-learn.\n",
    "- [This example from scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html) demonstrates its improved `OneHotEncoder` and new `ColumnTransformer` objects, which can replace functionality from third-party libraries like category_encoders and sklearn-pandas. Adapt this example, which uses Titanic data, to work with another dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, log_loss\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "import category_encoders as ce\n",
    "from numpy.testing import assert_almost_equal\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank = pd.read_csv('bank-additional/bank-additional-full.csv', sep=';')\n",
    "\n",
    "# X = bank.drop(columns='y')\n",
    "# y = bank['y'] == 'yes'\n",
    "\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # There is much to one-hot. \n",
    "# # we're gonna get that done automatically, however, in our pipeline. \n",
    "\n",
    "# pipeline = make_pipeline(\n",
    "#     ce.OneHotEncoder(use_cat_names=True),\n",
    "#     StandardScaler(), \n",
    "#     LogisticRegression()\n",
    "# )\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = cross_val_score(pipeline, X_train, y_train, cv=10) \n",
    "#scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1776) (751, 1776) (3000,) (751,)\n"
     ]
    }
   ],
   "source": [
    "## KAGGLE bioresponse https://www.kaggle.com/c/bioresponse#Evaluation\n",
    "\n",
    "train_url = 'kag-bioresponse/train.csv'\n",
    "test_url = 'kag-bioresponse/test.csv' ## iggnoring-- it doesn't have 'Activity' \n",
    "\n",
    "df_ = pd.read_csv(train_url)\n",
    "#df_test = pd.read_csv(test_url) # doesn't have 'Activity'\n",
    "assert all([x==0 for x in df_.isna().sum().values])\n",
    "assert all([pd.api.types.is_numeric_dtype(df_[feat]) for feat in df_.columns])\n",
    "dependent='Activity'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_.drop(dependent, axis=1), \n",
    "                                                    df_[dependent], \n",
    "                                                    train_size=0.8, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# df_.dtypes.value_counts()\n",
    "\n",
    "# #df_.describe()\n",
    "\n",
    "# df_.mean().array.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5885119410403075"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pipeline1 = make_pipeline(\n",
    "    #ce.OneHotEncoder(use_cat_names=True), # no categoricals. \n",
    "    #StandardScaler(), # means are very much near zero anyway, we don't really need this. \n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "\n",
    "pipeline1.fit(X_train, y_train)\n",
    "y_pred = pipeline1.predict(X_test)\n",
    "#accuracy_score(y_test, y_pred)\n",
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quinn/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/quinn/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/quinn/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/quinn/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/quinn/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/quinn/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/quinn/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/quinn/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/quinn/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/quinn/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.59238295, -0.64245771, -0.61530716, -0.71322103, -0.71528252,\n",
       "       -0.60554242, -0.63123414, -0.68629595, -0.67113009, -0.68777146])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline1, X_train, y_train, cv=10, scoring='neg_log_loss') \n",
    "\n",
    "scores#.mean() # don't run this a lot because it is EXPENSIVE\n",
    "\n",
    "# when test_size=0.15, scores.mean is better than accuracy_score... but when test_size=0.2, scores.mena is WORSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # We create the preprocessing pipelines for both numeric and categorical data.\n",
    "# numeric_features = ['age', 'fare']\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler())])\n",
    "\n",
    "# categorical_features = ['embarked', 'sex', 'pclass']\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# # Append classifier to preprocessing pipeline.\n",
    "# # Now we have a full prediction pipeline.\n",
    "# clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                       ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08813569643625678"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets dimension-reduction\n",
    "# we'll split the data into two different things, ints and floats. \n",
    "\n",
    "# a commenter here https://stats.stackexchange.com/questions/159705/would-pca-work-for-boolean-binary-data-types \n",
    "# said that a \"cosine\"ish version of PCA is more appropriate for 0,1. \n",
    "\n",
    "# We're gonna use the heuristic that, for N=#observations and M=#features, N > 5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3751, 834) (3751, 942)\n"
     ]
    }
   ],
   "source": [
    "ints = df_.drop(dependent, axis=1).select_dtypes(include='int')\n",
    "floats = df_.drop(dependent, axis=1).select_dtypes(include='float')\n",
    "\n",
    "print(ints.shape, floats.shape)\n",
    "\n",
    "P = 5.15\n",
    "n = int(np.divide(df_.shape[0], P)/ 2)\n",
    "\n",
    "LogiR = ('logistic_regression', SGDClassifier(loss='log', tol=np.exp(-P/2), max_iter=1234))\n",
    "\n",
    "estimators_c = [('PCA', PCA(n_components=n)), \n",
    "                LogiR]\n",
    "\n",
    "pipe_c = Pipeline(steps=estimators_c)\n",
    "\n",
    "estimators_d = [('FA', FeatureAgglomeration(n_clusters=n, affinity='cosine', linkage='complete')), \n",
    "                LogiR]\n",
    "\n",
    "pipe_d = Pipeline(steps=estimators_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 834) (751, 834) (3000, 942) (751, 942)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.89458424, 0.10541576],\n",
       "        [0.12677808, 0.87322192],\n",
       "        [0.97135242, 0.02864758],\n",
       "        ...,\n",
       "        [0.01759458, 0.98240542],\n",
       "        [0.99018714, 0.00981286],\n",
       "        [0.07848058, 0.92151942]]), array([[9.96911189e-01, 3.08881077e-03],\n",
       "        [1.65793779e-02, 9.83420622e-01],\n",
       "        [8.14470414e-01, 1.85529586e-01],\n",
       "        ...,\n",
       "        [7.04256899e-05, 9.99929574e-01],\n",
       "        [9.99988555e-01, 1.14454296e-05],\n",
       "        [1.93662214e-08, 9.99999981e-01]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ints_train = X_train.select_dtypes(include='int')\n",
    "ints_test = X_test.select_dtypes(include='int')\n",
    "floats_train = X_train.select_dtypes(include='float')\n",
    "floats_test = X_test.select_dtypes(include='float')\n",
    "\n",
    "print(ints_train.shape, ints_test.shape, floats_train.shape, floats_test.shape)\n",
    "\n",
    "pipe_c.fit(floats_train, y_train)\n",
    "y_pred_c = pipe_c.predict_proba(floats_test)\n",
    "\n",
    "pipe_d.fit(ints_train, y_train)\n",
    "y_pred_d = pipe_d.predict_proba(ints_test)\n",
    "\n",
    "\n",
    "# log_loss(y_test, y_pred_c), log_loss(y_test, y_pred_d)\n",
    "\n",
    "y_pred_d.shape, y_pred_c.shape, y_test.shape, df_.shape\n",
    "\n",
    "y_pred_c, y_pred_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5982614362581822 1.7561588475122367\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_test, y_pred_c), log_loss(y_test, y_pred_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66115672, 0.63633499, 0.69340228, 0.75470902, 0.76206379,\n",
       "       0.67908168, 0.74948106, 0.70610053, 0.6469453 , 0.71163227])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_c = cross_val_score(pipe_c, floats_train, y_train, cv=10, scoring='neg_log_loss') \n",
    "\n",
    "#scores_d = cross_val_score(pipe_d, ints_train, y_train, cv=5, scoring='neg_log_loss') \n",
    "\n",
    "-scores_c\n",
    "\n",
    "## ok so when I \n",
    "## get rid of the discretes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
