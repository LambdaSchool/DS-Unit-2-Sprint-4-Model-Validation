{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_241_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwardauron/DS-Unit-2-Sprint-4-Model-Validation/blob/master/LS_DS_241_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZEKrufFaG63H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "40a0e288-2ece-4043-89fe-548a7e3e3303"
      },
      "cell_type": "code",
      "source": [
        "!pip install seaborn==0.9.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn==0.9.0 in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (1.14.6)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (0.22.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (3.0.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn==0.9.0) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn==0.9.0) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.15.2->seaborn==0.9.0) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn==0.9.0) (40.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CLq-aJ0cGwtn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import urllib\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup, element\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQN0nkQ7G362",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "6affd7df-28f9-477f-e83e-8e7558727863"
      },
      "cell_type": "code",
      "source": [
        "pages = 20\n",
        "sleep_seconds = 15\n",
        "rec_count = 0\n",
        "rank = []\n",
        "gname = []\n",
        "platform = []\n",
        "year = []\n",
        "genre = []\n",
        "critic_score = []\n",
        "user_score = []\n",
        "publisher = []\n",
        "developer = []\n",
        "sales_na = []\n",
        "sales_pal = []\n",
        "sales_jp = []\n",
        "sales_ot = []\n",
        "sales_gl = []\n",
        "\n",
        "urlhead = 'http://www.vgchartz.com/gamedb/?page='\n",
        "urltail = '&console=&region=All&developer=&publisher=&genre=&boxart=Both&ownership=Both'\n",
        "urltail += '&results=1000&order=Sales&showtotalsales=0&showtotalsales=1&showpublisher=0'\n",
        "urltail += '&showpublisher=1&showvgchartzscore=0&shownasales=1&showdeveloper=1&showcriticscore=1'\n",
        "urltail += '&showpalsales=0&showpalsales=1&showreleasedate=1&showuserscore=1&showjapansales=1'\n",
        "urltail += '&showlastupdate=0&showothersales=1&showgenre=1&sort=GL'\n",
        "\n",
        "for page in range(1, pages):\n",
        "    surl = urlhead + str(page) + urltail\n",
        "    r = urllib.request.urlopen(surl).read()\n",
        "    soup = BeautifulSoup(r)\n",
        "    print(f\"Page: {page}\")\n",
        "\n",
        "    # vgchartz website is really weird so we have to search for\n",
        "    # <a> tags with game urls\n",
        "    game_tags = list(filter(\n",
        "        lambda x: x.attrs['href'].startswith('http://www.vgchartz.com/game/'),\n",
        "        # discard the first 10 elements because those\n",
        "        # links are in the navigation bar\n",
        "        soup.find_all(\"a\")\n",
        "    ))[10:]\n",
        "\n",
        "    for tag in game_tags:\n",
        "        # sleep to circumvent 429 error\n",
        "        time.sleep(sleep_seconds)\n",
        "        # add name to list\n",
        "        gname.append(\" \".join(tag.string.split()))\n",
        "        print(f\"{rec_count + 1} Fetch data for game {gname[-1]}\")\n",
        "        # get different attributes\n",
        "        # traverse up the DOM tree\n",
        "        data = tag.parent.parent.find_all(\"td\")\n",
        "        rank.append(np.int32(data[0].string))\n",
        "        platform.append(data[3].find('img').attrs['alt'])\n",
        "        publisher.append(data[4].string)\n",
        "        developer.append(data[5].string)\n",
        "        critic_score.append(\n",
        "            float(data[6].string) if\n",
        "            not data[6].string.startswith(\"N/A\") else np.nan)\n",
        "        user_score.append(\n",
        "            float(data[7].string) if\n",
        "            not data[7].string.startswith(\"N/A\") else np.nan)\n",
        "        sales_na.append(\n",
        "            float(data[9].string[:-1]) if\n",
        "            not data[9].string.startswith(\"N/A\") else np.nan)\n",
        "        sales_pal.append(\n",
        "            float(data[10].string[:-1]) if\n",
        "            not data[10].string.startswith(\"N/A\") else np.nan)\n",
        "        sales_jp.append(\n",
        "            float(data[11].string[:-1]) if\n",
        "            not data[11].string.startswith(\"N/A\") else np.nan)\n",
        "        sales_ot.append(\n",
        "            float(data[12].string[:-1]) if\n",
        "            not data[12].string.startswith(\"N/A\") else np.nan)\n",
        "        sales_gl.append(\n",
        "            float(data[8].string[:-1]) if\n",
        "            not data[8].string.startswith(\"N/A\") else np.nan)\n",
        "        release_year = data[13].string.split()[-1]\n",
        "        # different format for year\n",
        "        if release_year.startswith('N/A'):\n",
        "            year.append('N/A')\n",
        "        else:\n",
        "            if int(release_year) >= 80:\n",
        "                year_to_add = np.int32(\"19\" + release_year)\n",
        "            else:\n",
        "                year_to_add = np.int32(\"20\" + release_year)\n",
        "            year.append(year_to_add)\n",
        "\n",
        "        # go to every individual website to get genre info\n",
        "        url_to_game = tag.attrs['href']\n",
        "        site_raw = urllib.request.urlopen(url_to_game).read()\n",
        "        sub_soup = BeautifulSoup(site_raw, \"html.parser\")\n",
        "        # again, the info box is inconsistent among games so we\n",
        "        # have to find all the h2 and traverse from that to the genre name\n",
        "        h2s = sub_soup.find(\"div\", {\"id\": \"gameGenInfoBox\"}).find_all('h2')\n",
        "        # make a temporary tag here to search for the one that contains\n",
        "        # the word \"Genre\"\n",
        "        temp_tag = element.Tag\n",
        "        for h2 in h2s:\n",
        "            if h2.string == 'Genre':\n",
        "                temp_tag = h2\n",
        "        genre.append(temp_tag.next_sibling.string)\n",
        "\n",
        "        rec_count += 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Page: 1\n",
            "1 Fetch data for game Wii Sports\n",
            "2 Fetch data for game Super Mario Bros.\n",
            "3 Fetch data for game Mario Kart Wii\n",
            "4 Fetch data for game Wii Sports Resort\n",
            "5 Fetch data for game Pok√©mon Red / Green / Blue Version\n",
            "6 Fetch data for game New Super Mario Bros.\n",
            "7 Fetch data for game Tetris\n",
            "8 Fetch data for game New Super Mario Bros. Wii\n",
            "9 Fetch data for game Minecraft\n",
            "10 Fetch data for game Duck Hunt\n",
            "11 Fetch data for game Wii Play\n",
            "12 Fetch data for game Kinect Adventures!\n",
            "13 Fetch data for game PlayerUnknown's Battlegrounds\n",
            "14 Fetch data for game Nintendogs\n",
            "15 Fetch data for game Mario Kart DS\n",
            "16 Fetch data for game Pok√©mon Gold / Silver Version\n",
            "17 Fetch data for game Wii Fit\n",
            "18 Fetch data for game Wii Fit Plus\n",
            "19 Fetch data for game Super Mario World\n",
            "20 Fetch data for game Grand Theft Auto V\n",
            "21 Fetch data for game Grand Theft Auto V\n",
            "22 Fetch data for game Brain Age: Train Your Brain in Minutes a Day\n",
            "23 Fetch data for game Super Mario Land\n",
            "24 Fetch data for game Pok√©mon Diamond / Pearl Version\n",
            "25 Fetch data for game Mario Kart 7\n",
            "26 Fetch data for game Grand Theft Auto: San Andreas\n",
            "27 Fetch data for game Super Mario Bros. 3\n",
            "28 Fetch data for game Pok√©mon X/Y\n",
            "29 Fetch data for game Pok√©mon Ruby / Sapphire Version\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "au44s3g7ctwq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "columns = {\n",
        "    'Rank': rank,\n",
        "    'Name': gname,\n",
        "    'Platform': platform,\n",
        "    'Year': year,\n",
        "    'Genre': genre,\n",
        "    'Critic_Score': critic_score,\n",
        "    'User_Score': user_score,\n",
        "    'Publisher': publisher,\n",
        "    'Developer': developer,\n",
        "    'NA_Sales': sales_na,\n",
        "    'PAL_Sales': sales_pal,\n",
        "    'JP_Sales': sales_jp,\n",
        "    'Other_Sales': sales_ot,\n",
        "    'Global_Sales': sales_gl\n",
        "}\n",
        "print(rec_count)\n",
        "df = pd.DataFrame(columns)\n",
        "print(df.columns)\n",
        "df = df[[\n",
        "    'Rank', 'Name', 'Platform', 'Year', 'Genre',\n",
        "    'Publisher', 'Developer', 'Critic_Score', 'User_Score',\n",
        "    'NA_Sales', 'PAL_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']]\n",
        "df.to_csv(\"vgsales.csv\", sep=\",\", encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}