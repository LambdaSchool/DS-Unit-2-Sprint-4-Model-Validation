{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_244_Feature_Selection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/audreyakwenye/DS-Unit-2-Sprint-4-Model-Validation/blob/master/LS_DS_244_Feature_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "EktuEhKabPw1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_Lambda School Data Science - Model Validation_\n",
        "\n",
        "\n",
        "# Feature Selection\n",
        "\n",
        "Objectives:\n",
        "* Feature importance\n",
        "* Feature selection "
      ]
    },
    {
      "metadata": {
        "id": "rqC17z_cbPw5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Yesterday we saw that...\n",
        "\n",
        "## Less isn't always more (but sometimes it is)\n",
        "\n",
        "## More isn't always better (but sometimes it is)\n",
        "\n",
        "\n",
        "![Image of Terry Crews](https://media.giphy.com/media/b8kHKZq3YFfnq/giphy.gif)"
      ]
    },
    {
      "metadata": {
        "id": "90pyhgLYbPw8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Saavas, Ando [Feature Selection (4 parts)](https://blog.datadive.net/selecting-good-features-part-i-univariate-selection/)\n",
        "\n",
        ">There are in general two reasons why feature selection is used:\n",
        "1. Reducing the number of features, to reduce overfitting and improve the generalization of models.\n",
        "2. To gain a better understanding of the features and their relationship to the response variables.\n",
        "\n",
        ">These two goals are often at odds with each other and thus require different approaches: depending on the data at hand a feature selection method that is good for goal (1) isn’t necessarily good for goal (2) and vice versa. What seems to happen often though is that people use their favourite method (or whatever is most conveniently accessible from their tool of choice) indiscriminately, especially methods more suitable for (1) for achieving (2)."
      ]
    },
    {
      "metadata": {
        "id": "SkCG574vbPxB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "While they are not always mutually exclusive, here's a little bit about what's going on with these two goals"
      ]
    },
    {
      "metadata": {
        "id": "BJhlNZEEbPxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Goal 1: Reducing Features, Reducing Overfitting, Improving Generalization of Models\n",
        "\n",
        "This is when you're actually trying to engineer a packaged, machine learning pipeline that is streamlined and highly generalizable to novel data as more is collected, and you don't really care \"how\" it works as long as it does work. \n",
        "\n",
        "Approaches that are good at this tend to fail at Goal 2 because they handle multicollinearity by (sometime randomly) choosing/indicating just one of a group of strongly correlated features. This is good to reduce redundancy, but bad if you want to interpret the data."
      ]
    },
    {
      "metadata": {
        "id": "tfvPQq4ebPxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Goal 2: Gaining a Better Understanding of the Features and their Relationships\n",
        "\n",
        "This is when you want a good, interpretable model or you're doing data science more for analysis than engineering. Company asks you \"How do we increase X?\" and you can tell them all the factors that correlate to it and their predictive power.\n",
        "\n",
        "Approaches that are good at this tend to fail at Goal 1 because, well, they *don't* handle the multicollinearity problem. If three features are all strongly correlated to each other as well as the output, they will all have high scores. But including all three features in a model is redundant."
      ]
    },
    {
      "metadata": {
        "id": "UfJnm9g_bPxL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Each part in Saavas's Blog series describes an increasingly complex (and computationally costly) set of methods for feature selection and interpretation.\n",
        "\n",
        "The ultimate comparison is completed using an adaptation of a dataset called Friedman's 1 regression dataset from Friedman, Jerome H.'s '[Multivariate Adaptive Regression Splines](http://www.stat.ucla.edu/~cocteau/stat204/readings/mars.pdf).\n",
        ">The data is generated according to formula $y=10sin(πX_1X_2)+20(X_3–0.5)^2+10X_4+5X_5+ϵ$, where the $X_1$ to $X_5$ are drawn from uniform distribution and ϵ is the standard normal deviate N(0,1). Additionally, the original dataset had five noise variables $X_6,…,X_{10}$, independent of the response variable. We will increase the number of variables further and add four variables $X_{11},…,X_{14}$ each of which are very strongly correlated with $X_1,…,X_4$, respectively, generated by $f(x)=x+N(0,0.01)$. This yields a correlation coefficient of more than 0.999 between the variables. This will illustrate how different feature ranking methods deal with correlations in the data.\n",
        "\n",
        "**Okay, that's a lot--here's what you need to know:**\n",
        "1.   $X_1$ and $X_2$ have the same non-linear relationship to $Y$ -- though together they do have a not-quite-linear relationship to $Y$ (with sinusoidal noise--but the range of the values doesn't let it get negative)\n",
        "2.   $X_3$ has a quadratic relationship with $Y$\n",
        "3.   $X_4$ and $X_5$ have linear relationships to $Y$, with $X_4$ being weighted twice as heavily as $X_5$\n",
        "4.   $X_6$ through $X_{10}$ are random and have NO relationship to $Y$\n",
        "5.   $X_{11}$ through $X_{14}$ correlate strongly to $X_1$ through $X_4$ respectively (and thus have the same respective relationships with $Y$)\n",
        "\n",
        "\n",
        "This will help us see the difference between the models in selecting features and interpreting features\n",
        "* how well they deal with multicollinearity (#5)\n",
        "* how well they identify noise (#4)\n",
        "* how well they identify different kinds of relationships\n",
        "* how well they identify/interpret predictive power of individual variables."
      ]
    },
    {
      "metadata": {
        "id": "Xvs96UWD4XQs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import\n",
        "import numpy as np\n",
        "\n",
        "# Create the dataset\n",
        "# from https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "size = 1500 # I increased the size from what's given in the link\n",
        "Xs = np.random.uniform(0, 1, (size, 14)) \n",
        "# Changed variable name to Xs to use X later\n",
        " \n",
        "#\"Friedamn #1” regression problem\n",
        "Y = (10 * np.sin(np.pi*Xs[:,0]*Xs[:,1]) + 20*(Xs[:,2] - .5)**2 +\n",
        "     10*Xs[:,3] + 5*Xs[:,4] + np.random.normal(0,1))\n",
        "#Add 4 additional correlated variables (correlated with X1-X4)\n",
        "Xs[:,10:] = Xs[:,:4] + np.random.normal(0, .025, (size,4))\n",
        " \n",
        "names = [\"X%s\" % i for i in range(1,15)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7qjVVgwKB37G",
        "colab_type": "code",
        "outputId": "0ae420e1-51d3-4260-def8-25c669f19dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Putting it into pandas--because... I like pandas. And usually you'll be\n",
        "# working with dataframes not arrays (you'll care what the column titles are)\n",
        "import pandas as pd\n",
        "\n",
        "friedmanX = pd.DataFrame(data=Xs, columns=names)\n",
        "friedmanY = pd.Series(data=Y, name='Y')\n",
        "\n",
        "friedman = friedmanX.join(friedmanY)\n",
        "\n",
        "friedman.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.374540</td>\n",
              "      <td>0.950714</td>\n",
              "      <td>0.731994</td>\n",
              "      <td>0.598658</td>\n",
              "      <td>0.156019</td>\n",
              "      <td>0.155995</td>\n",
              "      <td>0.058084</td>\n",
              "      <td>0.866176</td>\n",
              "      <td>0.601115</td>\n",
              "      <td>0.708073</td>\n",
              "      <td>0.413325</td>\n",
              "      <td>0.956360</td>\n",
              "      <td>0.698205</td>\n",
              "      <td>0.568729</td>\n",
              "      <td>16.730915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.181825</td>\n",
              "      <td>0.183405</td>\n",
              "      <td>0.304242</td>\n",
              "      <td>0.524756</td>\n",
              "      <td>0.431945</td>\n",
              "      <td>0.291229</td>\n",
              "      <td>0.611853</td>\n",
              "      <td>0.139494</td>\n",
              "      <td>0.292145</td>\n",
              "      <td>0.366362</td>\n",
              "      <td>0.223650</td>\n",
              "      <td>0.167876</td>\n",
              "      <td>0.280668</td>\n",
              "      <td>0.561691</td>\n",
              "      <td>9.112092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.592415</td>\n",
              "      <td>0.046450</td>\n",
              "      <td>0.607545</td>\n",
              "      <td>0.170524</td>\n",
              "      <td>0.065052</td>\n",
              "      <td>0.948886</td>\n",
              "      <td>0.965632</td>\n",
              "      <td>0.808397</td>\n",
              "      <td>0.304614</td>\n",
              "      <td>0.097672</td>\n",
              "      <td>0.617692</td>\n",
              "      <td>0.078302</td>\n",
              "      <td>0.583794</td>\n",
              "      <td>0.132396</td>\n",
              "      <td>3.017894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.034389</td>\n",
              "      <td>0.909320</td>\n",
              "      <td>0.258780</td>\n",
              "      <td>0.662522</td>\n",
              "      <td>0.311711</td>\n",
              "      <td>0.520068</td>\n",
              "      <td>0.546710</td>\n",
              "      <td>0.184854</td>\n",
              "      <td>0.969585</td>\n",
              "      <td>0.775133</td>\n",
              "      <td>0.037718</td>\n",
              "      <td>0.917699</td>\n",
              "      <td>0.267895</td>\n",
              "      <td>0.699477</td>\n",
              "      <td>10.220976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.088493</td>\n",
              "      <td>0.195983</td>\n",
              "      <td>0.045227</td>\n",
              "      <td>0.325330</td>\n",
              "      <td>0.388677</td>\n",
              "      <td>0.271349</td>\n",
              "      <td>0.828738</td>\n",
              "      <td>0.356753</td>\n",
              "      <td>0.280935</td>\n",
              "      <td>0.542696</td>\n",
              "      <td>0.045148</td>\n",
              "      <td>0.217475</td>\n",
              "      <td>0.017812</td>\n",
              "      <td>0.346724</td>\n",
              "      <td>9.770285</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         X1        X2        X3        X4        X5        X6        X7  \\\n",
              "0  0.374540  0.950714  0.731994  0.598658  0.156019  0.155995  0.058084   \n",
              "1  0.181825  0.183405  0.304242  0.524756  0.431945  0.291229  0.611853   \n",
              "2  0.592415  0.046450  0.607545  0.170524  0.065052  0.948886  0.965632   \n",
              "3  0.034389  0.909320  0.258780  0.662522  0.311711  0.520068  0.546710   \n",
              "4  0.088493  0.195983  0.045227  0.325330  0.388677  0.271349  0.828738   \n",
              "\n",
              "         X8        X9       X10       X11       X12       X13       X14  \\\n",
              "0  0.866176  0.601115  0.708073  0.413325  0.956360  0.698205  0.568729   \n",
              "1  0.139494  0.292145  0.366362  0.223650  0.167876  0.280668  0.561691   \n",
              "2  0.808397  0.304614  0.097672  0.617692  0.078302  0.583794  0.132396   \n",
              "3  0.184854  0.969585  0.775133  0.037718  0.917699  0.267895  0.699477   \n",
              "4  0.356753  0.280935  0.542696  0.045148  0.217475  0.017812  0.346724   \n",
              "\n",
              "           Y  \n",
              "0  16.730915  \n",
              "1   9.112092  \n",
              "2   3.017894  \n",
              "3  10.220976  \n",
              "4   9.770285  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "uhDRfR-eF7pY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We want to be able to look at classification problems too, so let's bin the Y values to create a categorical feature from the Y values. It should have *roughly* similar relationships to the X features as Y does."
      ]
    },
    {
      "metadata": {
        "id": "-V1FuPAaFoUV",
        "colab_type": "code",
        "outputId": "c849e0ac-0605-474c-a5b1-565906682554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "cell_type": "code",
      "source": [
        "# First, let's take a look at what Y looks like\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.distplot(friedmanY);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
            "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
            "  alternative=\"'density'\", removal=\"3.1\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFYCAYAAABpkTT0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xt41eWd7/33OuS8Vs5rJSEhJAQx\nIYCIAkJUtIJWenraakm70d3Lbp/pVfXpzGCnHdop7EekW6fbGbVcte3o7NmPMzatZSzd04qjBQ8Q\niCAQCGcIIeeslfPKeR2eP4AICiQhh986fF7X5SUrv6ysz81a/L6579/9u29TIBAIICIiIkHDbHQA\nERERuZyKs4iISJBRcRYREQkyKs4iIiJBRsVZREQkyKg4i4iIBBmr0QEucrm6jY5w3VJS4mlv7zU6\nxpRRe8NbJLU3ktoKam+wcTjsVz2mnvMEsFotRkeYUmpveIuk9kZSW0HtDSWj6jlv2rSJgwcPYjKZ\nWLduHfPnzx8+tmvXLp577jksFgt33nknjz32GD09PXz/+9+ns7OToaEhHnvsMe64445Ja4SIiEg4\nGbE4V1RUUFNTQ1lZGadPn2bdunWUlZUNH9+4cSMvv/wyGRkZrFmzhvvuu4/du3eTn5/P2rVraW5u\n5r/+1//Km2++OakNERERCRcjDmuXl5ezYsUKAAoKCujs7MTj8QBQW1tLUlISWVlZmM1mli9fTnl5\nOSkpKXR0dADQ1dVFSkrKJDZBREQkvIxYnN1u92XFNTU1FZfLBYDL5SI1NfVTxz73uc/R0NDAypUr\nWbNmDd///vcnIbqIiEh4GvNs7dHsk/H73/+eadOm8fLLL3Ps2DHWrVvHli1brvmclJT4kL54f61Z\nd+FI7Q1vkdTeSGorqL2hYsTi7HQ6cbvdw49bWlpwOBxXPNbc3IzT6eSjjz7i9ttvB6CwsJCWlhZ8\nPh8Wy9WLbzBPdx+Jw2EP6VvBxkrtDW+R1N5IaiuovcFmXLdSlZSUsG3bNgCqqqpwOp3YbDYAcnJy\n8Hg81NXV4fV62b59OyUlJcyYMYODBw8CUF9fT0JCwjULs4iIiHxsxJ7zwoULKS4uprS0FJPJxPr1\n69myZQt2u52VK1eyYcMG1q5dC8CqVavIz8/H6XSybt061qxZg9frZcOGDZPdDhERkbBhCozmIvIU\nCOahh5EE+9DJRFN7w1sktTeS2gpqb7DRCmEiIiIhRMVZREQkyKg4i4iIBJmg2ZVKRK7fjgP11/3c\nuxZkT2ASEZkI6jmLiIgEGRVnERGRIKPiLCIiEmRUnEVERIKMirOIiEiQUXEWEREJMirOIiIiQUbF\nWUREJMioOIuIiAQZFWcREZEgo+IsIiISZFScRUREgoyKs4iISJBRcRYREQkyKs4iIiJBRsVZREQk\nyKg4i0SIQCCA3x8wOoaIjILV6AAiMjn6B72cqu/C1d6Hp2+I7t5BfP4A9rgokmwxpNhjmDkt0eiY\nInIFKs4iYaatq5+jNe1UN3YP95StFhP2+GisFhOdPYN0tXiobfFQebqVMw1drLglh+L8VEwmk8Hp\nRQRUnEXChtfn58BJN0fOtgNgj4/ixtxk8rMSiY22DBfeQCBA/6CPptZejp1rp/J0K5WnWynMTeab\n9xficNiNbIaIoOIsEhZa2nvZeaiJ7t4h7PFRLCpykp2ecMWesMlkIi7GSv60RPKnJTIjw87WD6o5\neLqVH79cwZr7i1hW5MRsVi9axCiaECYSwgKBAG/uOce2PbV09w4xJy+FL5TkkeOwjXqIOj8rkf/n\ngfl8+0vFxERbeOUPVTz72n66egYnOb2IXI2Ks0iI8vr8/PMfj/Gb7aeIjbFy35Lp3FroxGoZ+z9r\nk8nE4qIMnn70NpbNz+JEbQf/7798SE1T9yQkF5GRaFhbZILtOFB/3c+9a0H2qL6vu3eQzVsOcaKu\nk7xMO7cWOoiPjbru173IFhfFDx5exP/aeph/f+8MP3l1H498rojFRRnj/tkiMnqjKs6bNm3i4MGD\nmEwm1q1bx/z584eP7dq1i+eeew6LxcKdd97JY489xm9/+1u2bt06/D2HDx9m//79E59eJAK5O/t4\nruwgTW29LCp08sjniiivapqwn28ymfj8sjxynDZ+ubWKl35fRXfvEPfckjNhryEi1zZica6oqKCm\npoaysjJOnz7NunXrKCsrGz6+ceNGXn75ZTIyMlizZg333XcfDz74IA8++ODw8//0pz9NXgtEIkhd\ni4fnfnOADs8g9y/J5at3FWCepNufFsxKZ92aW/ifZQf41/88QU/fEF8oybvsWvb1jhKMdoRAJFKN\neHGqvLycFStWAFBQUEBnZycejweA2tpakpKSyMrKwmw2s3z5csrLyy97/ubNm/nOd74zCdFFIsvx\nc+385F8/osMzSOlnZvHg3bMmrTBflOO08bdrFpKeFMsbH1Tz2jsnCQS0ypjIZBuxOLvdblJSUoYf\np6am4nK5AHC5XKSmpl7xGEBlZSVZWVk4HI6JzCwScfYdd/E/yw4yOOTj0S/M4d7FuVP22s6UeP52\nzS1kpyfw9t46/u1tFWiRyTbmCWFj+Uf5+uuv8+Uvf3lU35uSEo/VahlrnKARaQs3qL1XZ7fFTujr\nvFl+lp+/cYjoKAt/+80lLLzROemv+cmvORx2nnniDtb9fCfv7KvDbovhkS8UX/frBtPnJ5iyTAW1\nNzSMWJydTidut3v4cUtLy3BP+JPHmpubcTo/PnHs2bOHH/3oR6MK0t7eO+rQwcbhsONyRc4tJ2rv\ntXV7+q/7tS59nUAgwO8/qGbrzrPY4qL4q6/dxPTUuCtmGc9r/vY/j1322G6LverPW1LkpLN7gDfe\nPc3Jc+0snJ1+XUt+BsvnR5/l8Bbs7b3WLw4jDmuXlJSwbds2AKqqqnA6ndhsNgBycnLweDzU1dXh\n9XrZvn07JSUlwPlCnZCQQHR09ES0QSSiDA75+MXWKrbuPEt6UizrHrqF/CzjN6mIi7GyctF0EuOj\nqKpu4/CZNqMjiYSlEXvOCxcupLi4mNLSUkwmE+vXr2fLli3Y7XZWrlzJhg0bWLt2LQCrVq0iPz8f\n+PT1aBEZnQ7PAC/+rpLqxm5m5STx+JfnkZgQPL/kxsdaWbl4On/afY79J93ExViZlZNkdCyRsGIK\nBMnMjmAeehhJsA+dTDS199rGswhJZko8v/xDFR2eQUrmZvLwZwuJso684td4XvOTrjWsfakOzwBv\n7jnHkNfP3Tdnk+O0jfo1guVWKn2Ww1uwt3dcw9oiMvn8/gD7T7r5+9f209UzxNfunsUjnysaVWE2\nSrIthntuycFsMvHugQZcHX1GRxIJG8H7L18kQnj6hthWcY5Dp1tJS4rlb9cs5LNLckNib2VHchzL\nF0zDHwiw/aN6PH1DRkcSCQtaW1vEQDVN3ZQfbmLQ6ycv085txRnUujzUujxGRxu1HKeNRYVOKo62\n8Od9ddx/24yg7vGLhAL9CxIxgNfnp/xwE+8eaMAfCLB0biZ33JRFdFRo3utfOCOFG3OT6fAM8v7B\n820SkeunnrPIFGvvHuC9Aw109gySYo/hzpuySLLFGB1r3BYVOunqGaTO1cP+Ey5uucJiKSIyOuo5\ni0yRQCDAsXPt/Ed5DZ09gxTOSGbVbblhUZgBzGYTyxdMIzEhmqrqds5qL2iR66aes8hVXLw9abS3\nFl2L1+dn16EmzjZ1ExNlYdmCTKaP4dajUBEdZeGum6fxx/Iadh1qJNkWTXKY/PIhMpXUcxaZZL39\nXrZV1HK2qRtHchxfKJkRloX5omRbDMvmZeH1Bdixv4FBr8/oSCIhR8VZZBK1dvXzx/IaWjv7KZiW\nyL2Lc4iPjTI61qTLy7QzJy+Frp5Bdh1q0i5WImOk4iwySVo7+3mropbeAS8LZ6ezbF4mFnPk/JNb\nONtBRkoc55o9nKjtNDqOSEiJnDOFyBRq7x7g7b11DHn93D4/i7kz00JiUZGJZDabuP2mLKKjzOw9\n1kJ794DRkURChoqzyATr6hnkPz+sZWDIx9K5mcycZvxuUkZJiI1i2dxMfP4A7x9swOvzGx1JJCSo\nOItMoP5BL//5YS39gz4WFzm5Qbs1kZthH16gZO8xl9FxREKCirPIBAkEAuysbKKn38v8gjQKZ6QY\nHSlo3HKjg2RbNCdqOzjXrPufRUai4iwyQQ6faaPe3cO09HhumpVmdJygYrWYuXPBNCxmE7sON9Ha\nOb77xkXCnYqzyARobuvlwEk38TFWbp+fFXGTv0Yj2RbDoiIng0N+fvWHKnx+XX8WuRoVZ5Fx6h/0\n8d7BBjDBHQuyiI3WwntXc0NOEjMybJyo6+T/7KoxOo5I0FJxFhmn/Sdc9A34WDArnYyUeKPjBDWT\nycRtczNJS4xh685qTtZ1GB1JJCipOIuMg7ujj5N1nSTboinOTzU6TkiIibLw6BeKAfjVH47QN+A1\nOJFI8FFxFrlO/kCA3UeaAVg8JwOzWdeZR2v29GRW3TYDd2c/r71z0ug4IkFHxVnkOp2o7aCta4CZ\n0xLJTNVw9lh96fZ8cjNsfFDZyEcndP+zyKVUnEWuQ9+AlwMn3ERZzdxyo8PoOCHJajHz6BeKsVrM\n/K8/HaOzZ9DoSCJBQ8VZ5DocOtPKoNfPghvSiYvR7OzrlZ2ewIN3FeDpG+Kf/3hUu1eJXKDiLDJG\nvf1DnKjtxBYXxezpyUbHCXn33JpD0YwUKk+38u7BBqPjiAQFFWeRMTp0pg2/P8C8malYNAls3Mwm\nE9/6XBHxMVZ+/c5Jmtt7jY4kYjgVZ5Ex6Okf4uSFXnNBtja1mCipibE8dN+NF1YPO6LVwyTiqTiL\njMHhM234AwHmFaTp1qkJtmROBrfNyeBMQxf/Ua7VwySyqTiLjFJP3yW95gjeo3ky/Zd7Z5Nij2Hr\nB2epbuwyOo6IYUZVnDdt2sTq1aspLS2lsrLysmO7du3igQceYPXq1WzevHn461u3buWLX/wiX/nK\nV9ixY8eEhhYxQlX1+V7zfPWaJ01CbBTf+lwR/kCAX/7hCANDPqMjiRhixOJcUVFBTU0NZWVlPP30\n0zz99NOXHd+4cSMvvvgir732Gjt37uTUqVO0t7ezefNm/u3f/o2XXnqJd955Z9IaIDIVBod8nKrv\nJD7Wykz1mifVnLxU7l00nea2Xn67/ZTRcUQMMeINmuXl5axYsQKAgoICOjs78Xg82Gw2amtrSUpK\nIisrC4Dly5dTXl5OWloaS5cuxWazYbPZeOqppya3FSKT7GRdJ15fgPkFyeo1T4GvLp/J4eo2/vxR\nPTfNSmfezI/3x95xoP66fuZdC7InKp7IpBuxOLvdboqLi4cfp6am4nK5sNlsuFwuUlNTLztWW1tL\nX18f/f39fPvb36arq4snnniCpUuXXvN1UlLisVot42iKsRwOu9ERplQktNduiwXA7w9worYDq8XE\nzYUZEbEl5MW2T5bRfH6+//Ai1j7/Lv/y5jE2f+8z2OKjx5Xtaq8ZCZ/lS6m9oWHMZ5nRruDT0dHB\nz372MxoaGnj44YfZvn37NTegbw/hexsdDjsuV7fRMaZMpLS329MPQEtnP929Q8yensTQoJehwfDe\nRcluix1u+2QZzefHHm3miyX5bHnvDC/8ej+PfmEOwHVnu9JrRspn+SK1N7hc6xeHEa85O51O3G73\n8OOWlhYcDscVjzU3N+N0OklLS+Pmm2/GarWSm5tLQkICbW1t42mDiGEOnjz/GS+ckWJwkshz/225\n5GXaKa9qYr82x5AIMmJxLikpYdu2bQBUVVXhdDqx2WwA5OTk4PF4qKurw+v1sn37dkpKSrj99tvZ\nvXs3fr+f9vZ2ent7SUnRiU1CT2tnP43uHqalx5NsizE6TsSxmM186/NzsFpM/Mu243j6hoyOJDIl\nRhzWXrhwIcXFxZSWlmIymVi/fj1btmzBbrezcuVKNmzYwNq1awFYtWoV+fn5ANx333187WtfA+BH\nP/oRZrNuqZbQc7SmHYCiGakjfKdMluz0BL58x0x+u+M0r751XCMYEhFGdc35ySefvOxxYWHh8J8X\nLVpEWVnZp55TWlpKaWnpOOOJGGdgyMfZpm6SbNFMS9d+zUa6b3EuH51wUXG0BVtcFDlOm9GRRCaV\nurMiV3GmoQu/P8Cc/LRrTmaUyWc2m/jm/YVYzCb2HGlmyKu1tyW8qTiLXEEgEOBkbQcmkyaCBYts\nh437b8ulp9/LgZPukZ8gEsJUnEWuoLqxmw7PINOdNuJjo4yOIxd8fmke9vgojtW009o5ubd7iRhJ\nxVnkCt47eH4Vqhtykg1OIpeKjrJwW3EGAaC8qgm/f3TrLoiEGhVnkU/oG/Cy50gLCbFWTQQLQllp\nCRRMS6Sta4ATdR1GxxGZFCrOIp9QcbSZgSEfN+QkaSJYkFp4o4Moi5mDJ1sZ1M5VEoZUnEU+4b2D\njZhMUJCTZHQUuYq4GCvzClIZGPJRebrV6DgiE07FWeQS9e4eqhu7mDczjQRNBAtqRTNSsMWdnxzW\n1TNodByRCaXiLHKJXYcbASiZl2VwEhmJxWLmlhsd+AOw77jW3ZbwouIscoHfH2B3VTNxMVYWzEob\n+QliuNwMG86UOGpbPDS1hu7OdiKfpOIscsHRc+20dw+wuMhJVAjvLR5JTCYTtxY6Adh/0j3qLW1F\ngl347xovMkq7Dp0f0l42N9PgJOFvx4H6CftZ6Umx5Dht1LV4aGztZVp6woT9bBGjqOcswvl7m/ed\ncOFMjmNWtmZph5qbCs5fhjh4Sr1nCQ8qziLARydcDA75WTY3U/c2h6C0pFimO224Ovpp1LVnCQMq\nziLAzgtD2rdpSDtkzZ+l3rOEDxVniXhtXf0cO9fB7JwknMlxRseR65SW+HHvucGt3rOENhVniXh7\njjYD6jWHg5su9J4Pn9GqYRLaVJwl4u2pasZiNnHrjU6jo8g4pSbGkpUWT3N7n7aUlJCm4iwRrcHd\nw7kWD3PzU7HFabnOcDAnLwWAozXtBicRuX4qzhLR9hw5P6S9ZE6GwUlkokxLTyApIZrqxi56+71G\nxxG5LirOErECgQB7jjYTHWVmwQ3pRseRCWIymSjKSyEQgOPn1HuW0KTiLBHrbFM3Le19LJiVTmy0\nFssLJzOnJRITZeF4bQden9/oOCJjpuIsEUtD2uHLajEze3oSg0N+ztR3GR1HZMxUnCUi+f0BKo42\nkxBrZd5M7UAVjm7MTcFsOr+hiRYlkVCj4iwR6URtBx2eQW650YHVon8G4Sg+1kpuhp1OzyCujj6j\n44iMic5KEpF2XxzSLtKQdji7Yfr5TUxO1nYanERkbFScJeJ4fX72HW8hyRbNjbkpRseRSZSZGo8t\nLoqzTd309g8ZHUdk1EZVnDdt2sTq1aspLS2lsrLysmO7du3igQceYPXq1WzevBmAPXv2cNttt/HQ\nQw/x0EMP8dRTT018cpHrdPhMGz39XhYXZmA2aweqcGYymbghJwmfPzA8WiISCka8f6SiooKamhrK\nyso4ffo069ato6ysbPj4xo0befnll8nIyGDNmjXcd999ACxevJgXXnhh8pKLXKfhtbSLNaQdCQqy\nkzhwys17Bxq4++ZsbQkqIWHEnnN5eTkrVqwAoKCggM7OTjweDwC1tbUkJSWRlZWF2Wxm+fLllJeX\nT25ikXEYGPSx/6QLZ3IceZl2o+PIFIiPtZLjsHGuxcPZpm6j44iMyojF2e12k5Ly8XW51NRUXC4X\nAC6Xi9TU1CseO3XqFN/+9rf5+te/zs6dOyc6t8h12X/KxeCQn8VzMtSDiiCzL0wMe+9gg8FJREZn\nzMsijeZ+wby8PB5//HHuv/9+amtrefjhh3nrrbeIjo6+6nNSUuKxWi1jjRM0HI7I6oWFansPnDoC\nwKrbZ47YBrst9op/jgTh1t7ZCTFUnmmj4mgzj3/tZmJjPj71hepn+XqpvaFhxOLsdDpxu93Dj1ta\nWnA4HFc81tzcjNPpJCMjg1WrVgGQm5tLeno6zc3NTJ8+/aqv094eupujOxx2XK7IGS4L1fZ6+obY\nd6yZ6U4bsWZGbEO35/yWg3Zb7PCfI0G4tndJUQb/Z9dZ3iqvZmnx+b27Q/WzfL3U3uByrV8cRhzW\nLikpYdu2bQBUVVXhdDqx2WwA5OTk4PF4qKurw+v1sn37dkpKSti6dSsvv/wycH7ou7W1lYwMTb4R\nY+073oLPH9BynRFq2dzzBbn8cJPBSURGNmLPeeHChRQXF1NaWorJZGL9+vVs2bIFu93OypUr2bBh\nA2vXrgVg1apV5Ofn43A4ePLJJ3nnnXcYGhpiw4YN1xzSFpkKF9fSXlzkNDiJGCEzNZ6Z0xKpOttG\nh2eAZFuM0ZFErmpU15yffPLJyx4XFhYO/3nRokWX3VoFYLPZeOmllyYgnsjEaO8e4Pi5DmblJJGe\nFGd0HDHI0uJMzjR0sbuqmc8uyTU6jshVaYUwiQgfHm0mANymIe2ItrjIicVsorxKQ9sS3FScJSLs\nOdqM2WTi1kINaUcye3w08wvSqG3xUNviMTqOyFWpOEvYa27rpbqxmzl5KSTGa+5DpLs4U1sTwySY\nqThL2Lu4XKdmaQvATbPSiY+xsvtIEz6/9nmW4KTiLGEtEAiw50gzUVYzC2c7jI4jQSDKamZRkZMO\nzyCVJ11GxxG5IhVnCWu1LR4aW3uZX5BGXMyYF8STMHVxYuB7++sNTiJyZSrOEtYu3tusWdpyqRum\nJ5Nij6H8UANDXp/RcUQ+RcVZwpY/EGDP0WbiYizML0gzOo4EEbPJxJKiDHr6vVSebjM6jsinqDhL\n2DpV10lb1wALZzuICuFNVWRyXJwgeHHCoEgwUXGWsKVZ2nItuRk2sh02Dp5y0zfgNTqOyGVUnCUs\neX1+PjzaQmJ8FEUzUkZ+gkQck8nE8oU5DHn97NesbQkyKs4Slo7WtOPpG2JRYQYWsz7mcmXLb84G\nYPcRDW1LcNFZS8LS7ioNacvIpjls5GXaOVLdTlfvoNFxRIapOEvYGRzy8dFJF2mJsRRkJxodR4Lc\nkjkZ+AMB9h5rMTqKyDAVZwk7ladbGRj0sWROBiaTyeg4EuQWF2Vg4uN74kWCgZZMkrBz8frhkjkZ\n7DigFaDk2lLsMdyYm8yxcx20dvaTlhRrdCQR9ZwlvPT2e6k83Up2egI5jgSj40iIuDg3oUL3PEuQ\nUHGWsPLRCRden5/FGtKWMbjlRicWs0mztiVoqDhLWNlz5PwevUuKnAYnkVBii4ti3sw0als81Lt7\njI4jomvOEhpGc+24b8DLkbPtpCfFcqSmnSM17VOQTELFlT5Ddlss3Z7+83+OjwLgN9tPcfMN6Zd9\n310Lsic/oMgl1HOWsHG2qZsAkJ+l26dk7HKcNqwWE2cbuwgEAkbHkQin4ixh42xjFyZgRqbd6CgS\ngqKsZnKcNrp7h2jt7Dc6jkQ4FWcJC929g7g6+slIiyc+Vldr5PrMvDDqUt3YbXASiXQqzhIWzl44\nmeZnqdcs1y8rPYHoKDNnm7rwa2hbDKTiLGGhurELs8lEboaKs1w/i9nEjAw7fQM+mtt6jY4jEUzF\nWUJee/cAHZ5Bsh0JxERZjI4jIS5fQ9sSBFScJeSdbewCIE9D2jIBnKlxxMdYOdfUjc/vNzqORCgV\nZwlpgUCA6sZurBYT0502o+NIGDCbTORl2Rn0+ql3aUESMcaoivOmTZtYvXo1paWlVFZWXnZs165d\nPPDAA6xevZrNmzdfdqy/v58VK1awZcuWiUsscgl3Zz+eviGmO21YLfpdUybGxaHtsxraFoOMeDar\nqKigpqaGsrIynn76aZ5++unLjm/cuJEXX3yR1157jZ07d3Lq1KnhYz//+c9JSkqa+NQiF1RfGNLW\nwiMykVITY7DHR1Hb4mHIq6FtmXojFufy8nJWrFgBQEFBAZ2dnXg8HgBqa2tJSkoiKysLs9nM8uXL\nKS8vB+D06dOcOnWKu+66a/LSS0TzBwKcbewmJsrCtHTtQCUTx2QykZ+ViM8foLbFY3QciUAjrtbg\ndrspLi4efpyamorL5cJms+FyuUhNTb3sWG1tLQDPPPMMf/d3f8cbb7wxqiApKfFYraE709bhiKzJ\nSFPdXrvt03vs1jZ30z/oo3hmGkmJcVP++uEsktp7tbbOLUin8nQr51o8YfXvO5zaMhqh2t4xL6U0\nmjVn33jjDRYsWMD06dNH/XPb20P3nkKHw47LFTnXpoxo78XNCS515EwrANnp8Vc8PlEu3RwhEkRS\ne6/VVqsZ0hJjqW3u5mS1m2RbzBSnm3g6VwWXa/3iMGJxdjqduN3u4cctLS04HI4rHmtubsbpdLJj\nxw5qa2vZsWMHTU1NREdHk5mZybJly8bTDpFhPr+fmuZu4mOsZKRMbq9ZIldBdiKtR/vZXdXMZ5fk\nGh1HIsiI15xLSkrYtm0bAFVVVTidTmy287es5OTk4PF4qKurw+v1sn37dkpKSvjHf/xHfve73/Gb\n3/yGBx98kO985zsqzDKh6l09DHn95GXZMZlMRseRMJWXlYjZZOKDQ43aqUqm1Ig954ULF1JcXExp\naSkmk4n169ezZcsW7HY7K1euZMOGDaxduxaAVatWkZ+fP+mhRaqH19LWLG2ZPLHRFqY7E6hp9nC2\nqVufN5kyo7rm/OSTT172uLCwcPjPixYtoqys7KrPfeKJJ64zmsiVDXn91LV4SIyPIjUx9K8DSnAr\nyE6iptnDB4caVZxlymjVBgk5tS3d+PwB8rISNaQtk25aegJJCdFUHGlmyOszOo5ECBVnCTka0pap\nZDabWDo3k55+LwdOtRodRyKEirOElP5BLw3uHlITY0iyRRsdRyJEydxMAHYeajQ4iUQKFWcJKTVN\nHgIB9ZplamU7bORn2Tl0ppW2rsi4B1yMpeIsIUXbQ4pRli/IJhCA9w42GB1FIoCKs4SMnr4hmtv7\nyEiJIyE2yug4EmEWFzmJjbbw3sEG7fMsk07FWULG2SZNBBPjxEZbWTo3kw7PIJWaGCaTTMVZQkZ1\nYxcmE+RmakhbjHHXgmwAdhzQ0LZMLhVnCQmdnkHaugaYlp5AbHTo7l4moW2600bBtEQOn2nF3dFn\ndBwJYyrOEhKqL0wE05C2GO3HHRszAAAgAElEQVSum7MJAO9qYphMIhVnCXqBQIDqxi4sZhPTnTaj\n40iEW1ToJD7GyvuVjXh9mhgmk0PFWYJeTXM33b1D5DhtRFn1kRVjRUdZWDYvk66eQfYddxkdR8KU\nznQS9PYcaQYgX/c2S5C4Z2EOJuDtvbVGR5EwpeIsQc0fCFBxtIVoq5lsR4LRcUQAyEiNZ35BGqcb\nujjT0GV0HAlDKs4S1E7WdtDePUBuph2LWR9XCR4rFk0H1HuWyaGznQS13RrSliA1Z0YK2ekJfHis\nhfbuAaPjSJhRcZag5fX52XushaSEaDJS442OI3IZk8nEPbfm4PMH2L6/3ug4EmZUnCVoVVW30dPv\nZVGRE7PJZHQckU9ZWpxJQqyVdw/UM+T1GR1HwoiKswSti7O0l8zJMDiJyJXFRFlYviCb7t4hyqua\njY4jYUTFWYLSwJCP/SfdOJJjmalVwSSI3XNLDhaziTf3nMMfCBgdR8KE1egAIldy8JSbgSEfS+bk\nYNKQthhsx4FrX1POy7Jzur6LV986Tm7Gx5MXL26UITJW6jlLUNp9YYhwSZGGtCX4FeenAnD4TBsB\n9Z5lAqg4S9Dp6R/i0JlWchw2sh1aS1uCX7IthulOG+7OflratVuVjJ+KswSdfcdd+PwBlsxxGh1F\nZNTmXtJ7FhkvFWcJOsOztDWkLSHEkRKHMyWOencP7d39RseREKfiLEGlwzPAsZp2ZmUnkZ4cZ3Qc\nkTFR71kmioqzBJWKoy0E0L3NEpqyHQkk26I529SNp3fI6DgSwkZVnDdt2sTq1aspLS2lsrLysmO7\ndu3igQceYPXq1WzevBmAvr4+vvvd77JmzRoefPBBtm/fPvHJJSztOdKMyQS3Fup6s4Qek8lEcX4q\ngQAcOaves1y/Ee9zrqiooKamhrKyMk6fPs26desoKysbPr5x40ZefvllMjIyWLNmDffddx8nTpxg\n7ty5PProo9TX1/PII49w9913T2pDJPS1tPdS3dhFcX4qSQnRRscRuS75WYkcOOnmZF0n3b2D2OP1\nWZaxG7HnXF5ezooVKwAoKCigs7MTj8cDQG1tLUlJSWRlZWE2m1m+fDnl5eWsWrWKRx99FIDGxkYy\nMjREKSPTRDAJB2aziTl5qfj8Ad7ZV2d0HAlRIxZnt9tNSkrK8OPU1FRcLhcALpeL1NTUKx4DKC0t\n5cknn2TdunUTmVnCUCAQYPeRZqwWMwtnO4yOIzIus3KSiI4y886+OgYGtSGGjN2Yl+8cy+o3v/71\nrzl69Cjf+9732Lp16zWXYUxJicdqtYw1TtBwOCJrv+GJbm91QyeNrb0snZfFjOkpnzput8VO6OuN\nldGvP9Uiqb2T1db5sxzsPdrMv75zkptuGPsvnJ9dmjfxodC5KlSMWJydTidut3v4cUtLCw6H44rH\nmpubcTqdHD58mLS0NLKysigqKsLn89HW1kZaWtpVX6e9vXc87TCUw2HH5eo2OsaUmYz2/mnnGQBu\nLki74s/u9hh336jdFmvo60+1SGrvZLZ1ZpaN/cdb2H+8hbwMG2bz2NaIn4xzis5VweVavziMOKxd\nUlLCtm3bAKiqqsLpdGKznV9SMScnB4/HQ11dHV6vl+3bt1NSUsLevXt55ZVXgPPD4r29vZcNjYtc\nyh8IUHGkhdhoC/MLrv4LnEgoiY22MisniZ5+L2ebgrdASHAasee8cOFCiouLKS0txWQysX79erZs\n2YLdbmflypVs2LCBtWvXArBq1Sry8/PJysrihz/8Id/4xjfo7+/nxz/+MWazbqmWKztV10lrVz/L\n5mYSHRW6lzZEPmlOXgonajuoqm4jP8uuHdZk1EyBINlCJZiHHkYS7EMnE+1623u1bfd2VzVxoraT\nFbfmMC09YbzxJlwkDfNCZLV3Ktr63sEGzjZ2c88tOWQ7Rv/5noztJnWuCi7jGtYWmUw+f4CzTd3E\nRlvITI03Oo7IhLu4nWRVtRYlkdFTcRZDNbh7GBzyk5+VOOYJMyKhIC0xlqy0eJraenF3ajtJGR0V\nZzHUmYYuAPKnJRqcRGTyzJ15ofesDTFklFScxTCDXh91LR4S46NIS4wxOo7IpMlMjSctMYaaZg9d\nPYNGx5EQoOIshqlt9uDzB8iflqhZrBLWLm6IAdoQQ0ZHxVkMc3FIe6aGtCUC5GbYscVFcaq+i74B\nr9FxJMipOIshevu9NLX2kp4Uq117JCKYzSaK81Pw+wMcq2k3Oo4EORVnMcTZpi4CqNcskaUgO4nY\naAvHz3Uw5PUbHUeCmIqzGKK6oQuTCfKyQnNRepHrYbWYKZyRwqDXz8naDqPjSBBTcZYp1+kZoLVr\ngGnpCcRGj3ljNJGQduP0ZKwWE0dq2vH5g2KBRglCKs4y5c40nl9OLz9LQ9oSeWKiLdyQk0xvv5ca\nbYghV6HiLFMqEAhQ3dCF1WJiutNmdBwRQxTOSAbQxDC5KhVnmVLujn48fUPkZtiJsurjJ5HJHh9N\njtOGu7MfV4eW9JRP09lRptSZxgvLdWpIWyJc0YXe81H1nuUKVJxlyvj9Ac42nt+BKitNO1BJZMtM\njSfZFk1NUze9/UNGx5Ego+IsU6bB3cPAkI+8LLt2oJKIZzKZKJyRQiAAx8/ptiq5nIqzTJnTF5fr\n1JC2CHB+EZ7oKDMnajvx+bQoiXxMxVmmRE//ELXNHpISoklLijU6jkhQsFrM3JCTzMCQj+pG3VYl\nH1NxlilRcaQZfyBAQbZ2oBK5VGFuMibT+YlhgYAWJZHzVJxlSnxwqAkTMHNaktFRRIJKQlwUuRl2\n2rsHaG7XbVVynoqzTLp6dw/VjV1MS08gPlbLdYp8UpEWJZFPUHGWSbfrUCMABdmaCCZyJY7kONIS\nY6ht9uDp1W1VouIsk8zn97OrqomEWKuW6xS5iuHbqoBj59R7FhVnmWRV1e10egZZPCcDi0UfN5Gr\nycuyExtt4VRdp/Z6FhVnmVwfXBjSvn1elsFJRIKbxWxm9vRkBr1+zjR0Gh1HDKbiLJOmq2eQ/Sdc\nZDsSyMu0Gx1HJOjdmJuM2QTHznXotqoIp+Isk2bn4UZ8/gDLb5qme5tFRiEuxsqMTDudnkGOaUnP\niKbiLJMiEAjw3oEGoqxmls7NNDqOSMgozE0B4M/76gxOIkYaVXHetGkTq1evprS0lMrKysuO7dq1\niwceeIDVq1ezefPm4a8/++yzrF69mq9+9au89dZbE5tagt7xcx00t/dx641OEmKjjI4jEjLSk2NJ\nTYzho5MuWjv7jY4jBhmxOFdUVFBTU0NZWRlPP/00Tz/99GXHN27cyIsvvshrr73Gzp07OXXqFLt3\n7+bkyZOUlZXxT//0T2zatGnSGiDB6d2DDQAsXzDN4CQiocVkMlGYe363qh0H6o2OIwYZsTiXl5ez\nYsUKAAoKCujs7MTj8QBQW1tLUlISWVlZmM1mli9fTnl5OYsWLeL5558HIDExkb6+Pnw+3yQ2Q4JJ\nd+8g+463kJUWzw05Wq5TZKzysuwkxFp590ADQ16dOyPRiMXZ7XaTkpIy/Dg1NRWXywWAy+UiNTX1\nU8csFgvx8fEAvP7669x5551YLJaJzi5BatfhJrw+TQQTuV5Wi5k7b5qGp2+ID4+1GB1HDDDmhY7H\nMr3/7bff5vXXX+eVV14Z8XtTUuKxWkO3gDsckXWr0NXaGwgE+OBQE1aLmc8vn0WSLWb4mN0WultF\nhnL26xFJ7Q3Wtt55Sy5vVpzjvcpGvnT37An7uTpXhYYRi7PT6cTtdg8/bmlpweFwXPFYc3MzTqcT\ngPfff5+XXnqJf/qnf8JuH/kvp729d8zhg4XDYcflipy9WK/V3qrqNupdHm4rzmCwbxBX3+DwsW5P\naE5usdtiQzb79Yik9gZzW80+HzcVpHPglJs9B+uZOW38a9PrXBVcrvWLw4jD2iUlJWzbtg2Aqqoq\nnE4nNtv5NZJzcnLweDzU1dXh9XrZvn07JSUldHd38+yzz/KLX/yC5OTkCWqGhIK399YCsPLW6QYn\nEQl999ySA8CfP9JtVZFmxJ7zwoULKS4uprS0FJPJxPr169myZQt2u52VK1eyYcMG1q5dC8CqVavI\nz8+nrKyM9vZ2/vIv/3L45zzzzDNMm6aZu+Gsub2XytOtzJyWSH6WdqASGa+ivBQyU+OpONrM1z4z\ni8T4aKMjyRQZ1TXnJ5988rLHhYWFw39etGgRZWVllx1fvXo1q1evnoB4Ekr+vK+eALDiwm/7IjI+\nZpOJzyzM5t/ePsn7Bxv43NI8oyPJFNEKYTIh+ge9fHCogaSEaG4tdBodRyRslMzLIibawvb99fj8\n2q0qUqg4y4TYdbiJvgEfd9+cjVVbQ4pMmLgYK8vmZtLWNcCBk61Gx5EporOojFsgEOCdfXVYzCat\nCCYyCT6zUBPDIo2Ks4zbwdOtNLb2srjIedl9zSIyMbLTEyiakcLRmnbq3T1Gx5EpoOIs4/bH3TUA\n3L9khsFJRMKXes+RRcVZxuVEbQen6jq5qSCNHKfN6DgiYWvBDWmkJsaw61ATvf1eo+PIJFNxlnG5\n2GtetVS9ZpHJZDGbufvmbAaGfOw63Gh0HJlkKs5y3c41d1N5upXZOUnckKOV4EQm2x03TcNqMfHO\nR/X4x7DPgYQeFWe5bn/acw5Qr1lkqiTGR7O4KIPmtl6Onm03Oo5MIhVnuS4t7b1UHG0mx2Fj3sw0\no+OIRIyL622/s08Tw8KZirNcl99/cJZAAD6/bIb2bBaZQvlZ59euP3jKTUtHn9FxZJKoOMuY1TR1\nsbuqiRyHTUt1ihhgxa05BIC3P6w1OopMEhVnGbN/ffMYAeAry2diVq9ZZMotKnSSYo/h/cpGevuH\njI4jk0DFWcakurGL8kONzMpO4qYCXWsWMYLVYmbFLTkMDPl490CD0XFkEoxqy0iRi7a8exqAmdmJ\nvHtQJwURoyxfMI2tO8/y9r46Vi6arg1nwozeTRm1ozXtVJ1tZ3qGjczUeKPjiES0+Ngo7pifRXv3\nAB8eazE6jkwwFWcZFZ/fz2tvnwTgtuIsg9OICMCKRdMxmeCtiloCWpQkrKg4y6i8e6CBOpeH2+dl\n4VSvWSQoOJPjWDjbQU1zN8fOdRgdRyaQirOMqLt3kH9/7wxxMRa+eleB0XFE5BKfXZwLfLzOvYQH\nFWcZ0b+/X01Pv5cvleSTlBBtdBwRuURBdhKFuclUVbdxtqnL6DgyQVSc5Zpqmrp5d38909IT+MyF\nZQNFJLh8blkeAP9Rrt5zuFBxlqvy+f38723HCQBfX3GDbtUQCVJzZqSQl2nno+MuGlt7jI4jE0D3\nOctVvfVhLdWNXdw2J4PivFSj44hElB0H6sf0/XlZds42dfPH3TV863NzJimVTBV1heSKGlt7+Pf3\nqklMiOYbK2cbHUdERjDdaSMpIZrdVc20dvYbHUfGST3nCHWt38r9gQDb9pzD6/OzcHY6e49fvsCB\n3RY72fFEZIxMJhNzZ6ay81ATf9xTw0P33mh0JBkH9ZzlU46dbcfV0U9epp3cDLvRcURklPKzEnGm\nxPHegQbcndpOMpSpOMtl2rr6+eiEm9hoC4vnaDtIkVBiNpv4Ukk+Pn+AP+w8a3QcGQcVZxk25PXz\n7oEG/IEAy+ZlEhutqx4ioWbJnAyy0uLZeaiJ5vZeo+PIdRpVcd60aROrV6+mtLSUysrKy47t2rWL\nBx54gNWrV7N58+bhr584cYIVK1bw6quvTmximRSBQIDdVU109w5RnJ9CjsNmdCQRuQ5ms4kv3Z6P\nPxBg6wdnjY4j12nE4lxRUUFNTQ1lZWU8/fTTPP3005cd37hxIy+++CKvvfYaO3fu5NSpU/T29vLU\nU0+xdOnSSQsuE+tUfSfVjd2kJ8Vy8w0Oo+OIyDjcWugkx2Fjd1UTDW7d9xyKRizO5eXlrFixAoCC\nggI6OzvxeDwA1NbWkpSURFZWFmazmeXLl1NeXk50dDS/+tWvcDp1zTIUtHb2U3GkhWirmTtvmobZ\nbDI6koiMg9lk4st35BMA3nj/jNFx5DqMeFHR7XZTXFw8/Dg1NRWXy4XNZsPlcpGamnrZsdraWqxW\nK1br2K5XpqTEY7VaxvScYOJwhNas5ou3Q/X0DbHjQAM+f4DP3pZHlnN07Yi026nU3vAVbm29eC5a\nmW7jrb117D3uwu0Zoig/9bLjkSJU2zvmGT+TtWdoewhPXHA47Lhc3UbHGJNuTz9en5+3Kmrp6Rti\n4Y0O0hKj6faMvHiB3RY7qu8LF2pv+ArHtl56LvrqnTPZ9Oo+fv67g/zw4VvIcCaG3LlqPIL93Hyt\nXxxGHNZ2Op243e7hxy0tLTgcjisea25u1lB2iAgEApQfbsLd2c/MaYkU56UYHUlEJtisnCQWFTqp\nbuyi4miz0XFkDEYsziUlJWzbtg2AqqoqnE4nNtv5mbw5OTl4PB7q6urwer1s376dkpKSyU0sE+LA\nqVaqG7txJMeytDgDk0nXmUXC0QN3FWC1mPjdjtMMDPmMjiOjNOKw9sKFCykuLqa0tBSTycT69evZ\nsmULdrudlStXsmHDBtauXQvAqlWryM/P5/DhwzzzzDPU19djtVrZtm0bL774IsnJyZPeIBnZ23tr\nOXS6FXt8FHfdnI1Fu02JhC1Hchwrbp3Om3vOsfW909w1P8voSDIKpsBkXUQeo2C+LjCSYL+ucamK\no8384vdVxMZY+OySXOzx0WP+GeF4ne5a1N7wFY5tvWtB9qe+1tvv5Qe/KMfn9/PUt5aQmhhek+Cu\nJtjPzeO65izh4/CZVn71hyPExli455ac6yrMIhJ64mOtPHhXAX0DPv71P08YHUdGQcU5QlRVt/HC\n7w5hNpt44ivzI+Y3ZxE57/b5WRTPTGP/STf7T7iMjiMjUHGOAEfOtvHC784vu/rEV+dROEMzs0Ui\njclk4rEHbsJiNvHqf56gb8BrdCS5Bu1sEOaOnm3jhdcrCQQCPPHV+czNTzM6kohMomvt1W63xVKc\nn0rl6VZ+tuUQi4ouv/X1SterxRjqOYexA6fc/MNvK/H5Azz25XnMm6nCLBLp5s1MxR4fxbGadlzt\n2vM5WKk4h6k9R5rZvOUQZhN894H53DQr3ehIIhIELBYzy+ZmEgDer2xkyOs3OpJcgYpzGNpxoJ5f\nbq0iOsrMX69ewFz1mEXkEhmp8czNT8XTN6SVw4KUinMYCQQCvPH+Gf73m8dJiIvib76+kNnTtfCL\niHzaTTekk5oYw+n6Lmqagvde4Eil4hwmvD4///ynY2zdeZb0pFjWPXQLMzJDczcWEZl8FrOJO+Zn\nYTGbKK9qoqd/yOhIcgkV5zDQN+Dlxd8d4oPKRmZk2vnhw7eSmRpvdCwRCXJJthhuLXQyOORnx0cN\nDGrt7aCh4hzi3B19bHp1H4fOtDJvZhrf/8bNJCVo5S8RGZ3Z05MomJZIa1c///LmsUnbFljGRvc5\nh7BTdZ28uKWS7t4h7lmYQ+mKWVjM+n1LREbPZDJxW3EGnT2DlFc1k5th577FuUbHing6k4eo9w42\n8OxrH9HT52XNvbP5L/fOVmEWketisZi56+ZskmzR/Gb7KSpPu42OFPF0Ng8xg0M+XvnjUf7Xn44R\nE2XhL782n88szDE6loiEuPhYK49/eR5Wi5nN/36Y4+fajY4U0VScQ4jrwvXlDyobmZFhZ/03F2k5\nThGZMAXZSTz25bn4/QH+8fVKTjd0Gh0pYumacxC41lq4F9W5PHxQ2cjgkJ9ZOUksKXJy+GzbFKQT\nkUgyvyCdv/hiMT///WH+oewgf/ONm8nN0G2ZU0095yDnDwQ4cNLNn/fV4/UFWDo3k2VzM7FY9NaJ\nyOS4tdDJf/vcHPoGvDzzbx9RpY7AlNMZPoj19A/x9t46Kk+3YouL4v4ludyQk2R0LBGJAEvnZvJ/\nf7GYIa+ff/zNQd4/2GB0pIiiYe0gVdPUTXlVE4NDfnIcCZTMyyIm2mJ0LBGJIEvmZJBij+HF31Xy\nz386RnN7H//XHflYNXI36fQ3HGQGBn18UNnIuwca8PkC3DYng7sXZqswi4ghZk9P5ocP34ozOY4/\n7q7hJ69+RHNbr9Gxwp6Kc5AIBAKcberm9x9Uc6ahi7TEWD6/LI/ZucmYTCaj44lIBMtMjefH31zE\n0uIMqhu7WP/PFezYX49fq4lNGg1rB4Hu3kH2HnNR2+LBYjax8EYHc2akYDarKItIcIiPtfLoF4qZ\nX5DO/7ftOP9723F27K/na5+ZxZy8VKPjhR0VZwP1D3r5j/Ia/rTnHH5/gIyUOJbOzSRRa2OLSJBa\nMieD2dOT+d27p9l1uImf/voA82amseq2XGZP10jfRFFxNsCQ18d7Bxv5P+Vn6fQMEh9r5ZYbHeRl\n2vXBFpGgl2KP4b99fg4rb51O2Z9PcuhMK4fOtJLrtHHPrTksKnQSG63yMh7625tCA0PnJ3v9cXcN\n7d0DREeZ+WJJHrb4KM1+FJGQMyPTzve+fjMn6zp5e18dHx138c9/PMa/vnWC+QVpLCrKYN7MVBXq\n66C/sSnQ0tHHjo/qeb+ygZ5+L9FRZj67JJfPLsklMT56VCuEiYhMtvGci+bkpTAjw8bJuk7ONnax\n97iLvcddmEzgSI4jKy2ejJR4UpNiiLZ+fPfJXQuyJyJ62FFxniTdvYPsO+Hiw6MtHKtpJwDY46P4\n/LIZrLhluq4ri0jYSYiLYsEN6dw0K40OzwBnmzw0untwtffR0t4HtAKQmBBNWmIMaUmxTEtLIDfD\npt71J4zqb2PTpk0cPHgQk8nEunXrmD9//vCxXbt28dxzz2GxWLjzzjt57LHHRnxOOPL7z98KdbSm\njarqNk7Udg7fZjArJ4m7b87m1hudRFk1fC0i4c1kMpFijyXFHsvNN6QzMOSjua0XV0cfrZ0DtHb1\nU904SHVjN3uPuQBIS4wlKy2ezNT48/9PSyArLZ6khOiInIszYnGuqKigpqaGsrIyTp8+zbp16ygr\nKxs+vnHjRl5++WUyMjJYs2YN9913H21tbdd8TigLBAL09Htpbuulqa2Xc80eGtp6OVXXwcCgb/j7\n8jLtLC7KYFGhk7SkWAMTi4gYKybKQm6GfXgDjUAgQHfvEK2d/cTHWjnX7KGxtYfD1W0crr58He9o\nq5m0pFjSk+Iu/P/8f6n2WBITokhMiCYmyhJ2BXzE4lxeXs6KFSsAKCgooLOzE4/Hg81mo7a2lqSk\nJLKysgBYvnw55eXltLW1XfU5UyEQCNDhGSQQCOAPBAgEzn8tEDi/kYT/wmO/P4DXF2DI62PI62fQ\n67/wfx99Az66ewfx9A3R3TtEd+8g3b1DdPYM0jfgvez1zCbISkugIDuJOXkpFM5IITFew9YiIldi\nMplITIgmMSH6smvOfQNemtp6aWztobG1l6bWXlydfbR29tPYevVVyaKt5uGflxgfTVyMhdgYK6lJ\ncQR8fmKjLcTFWImymrFazFgtJizmj/9vsZiwWsxYzCYsFhNmkwkTgAnMF4q+yWQiyRY9/HiyjVic\n3W43xcXFw49TU1NxuVzYbDZcLhepqamXHautraW9vf2qz5kK//Lmcd6b4EXaTSawxUWRao/BMT0Z\nZ0ocGanxTHfYuHlOJt1dfRP6eiIikSYuxkp+ViL5WYmfOtbb76W1q5/Wzn7cnX20dQ/Q3TNIV+8Q\nXT2DdPUOUtPUjc8/eauWLZmTwV98sXjkb5wAY74CH7iO5dpG8xyHY+L2C/3ew4v43oT9tNGJHUf+\nB1cWTmASEZHwNMPoAFNoxOLsdDpxu93Dj1taWnA4HFc81tzcjNPpJCoq6qrPERERkWsbcepwSUkJ\n27ZtA6Cqqgqn0zk8PJ2Tk4PH46Gurg6v18v27dspKSm55nNERETk2kyBUYw5//SnP2Xv3r2YTCbW\nr1/PkSNHsNvtrFy5kg8//JCf/vSnANx7771861vfuuJzCgs1dCsiIjIaoyrOIiIiMnW0IoaIiEiQ\nUXEWEREJMlrMdJwiaZnSPXv28N3vfpcbbrgBgNmzZ/N3f/d3BqeaeCdOnOA73/kO3/zmN1mzZg2N\njY38zd/8DT6fD4fDwd///d8THR0+i8x8sr0/+MEPqKqqIjk5GYBvfetb3HXXXcaGnCDPPvss+/bt\nw+v18hd/8RfMmzcvrN/bT7b3z3/+c9i+t319ffzgBz+gtbWVgYEBvvOd71BYWBiy76+K8ziMtLRp\nOFq8eDEvvPCC0TEmTW9vL0899RRLly4d/toLL7zAN77xDe6//36ee+45Xn/9db7xjW8YmHLiXKm9\nAH/913/N3XffbVCqybF7925OnjxJWVkZ7e3tfPnLX2bp0qVh+95eqb233XZbWL63ANu3b2fu3Lk8\n+uij1NfX88gjj7Bw4cKQfX81rD0OV1vaVEJXdHQ0v/rVr3A6ncNf27NnD/fccw8Ad999N+Xl5UbF\nm3BXam+4WrRoEc8//zwAiYmJ9PX1hfV7e6X2+ny+EZ4VulatWsWjjz4KQGNjIxkZGSH9/qo4j4Pb\n7SYlJWX48cVlSsPZqVOn+Pa3v83Xv/51du7caXScCWe1WomNvXyjkr6+vuGhsLS0tLB6j6/UXoBX\nX32Vhx9+mL/6q7+ira3tCs8MPRaLhfj4eABef/117rzzzrB+b6/UXovFEpbv7aVKS0t58sknWbdu\nXUi/vxrWnkDhfldaXl4ejz/+OPfffz+1tbU8/PDDvPXWWyFzDWcihPt7DPClL32J5ORkioqK+OUv\nf8nPfvYzfvzjHxsda8K8/fbbvP7667zyyivce++9w18P1/f20vYePnw4rN9bgF//+tccPXqU733v\ne5e9p6H2/qrnPA7XWto0HGVkZLBq1SpMJhO5ubmkp6fT3NxsdKxJFx8fT39/P/DxErXhbOnSpRQV\nFQHwmc98hhMnThicaOK8//77vPTSS/zqV7/CbreH/Xv7yfaG83t7+PBhGhsbASgqKsLn85GQkBCy\n76+K8zhE2jKlW7du5cgn2GoAAAIaSURBVOWXXwbA5XLR2tpKRkaGwakm37Jly4bf57feeos77rjD\n4EST64knnqC2thY4f7394uz8UNfd3c2zzz7LL37xi+HZyuH83l6pveH63gLs3buXV155BTh/ybG3\ntzek31+tEDZOkbRMqcfj4cknn6Srq4uhoSEef/xxli9fbnSsCXX48GGeeeYZ6uvrsVqtZGRk8NOf\n/pQf/OAHDAwMMG3aNH7yk58QFRVldNQJcaX2rlmzhl/+8pfExcURHx/PT37yE9LS0oyOOm5lZWW8\n+OKL5OfnD3/tf/yP/8GPfvSjsHxvr9Ter3zlK7z66qth994C9Pf388Mf/pDGxkb6+/t5/PHHmTt3\nLt///vdD8v1VcRYREQkyGtYWEREJMirOIiIiQUbFWUREJMioOIuIiAQZFWcREZEgo+IsEoF+8Ytf\nsHbt2su+9sYbb/DQQw8ZlEhELqXiLBKBHnnkEY4fP05FRQVwfsGK559/nv/+3/+7wclEBFScRSJS\nVFQUGzZs4KmnnsLr9fL888/zla98hZkzZxodTUTQIiQiEW3dunUAHDhwgDfeeCOiNjERCWYqziIR\nrL29nXvuuYd/+Id/CLulWEVCmYa1RSJYSkoKycnJ5OXlGR1FRC6h4vz/t2fHNAAAAAzC/LvGBkdr\ngiwDgBlxBoAZnzMAzFjOADAjzgAwI84AMCPOADAjzgAwI84AMCPOADAjzgAwE5tyOPx64Lf7AAAA\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "JvQeWRthGvht",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's pretty normal, let's make two binary categories--one balanced, one unbalanced, to see the difference.\n",
        "* balanced binary variable will be split evenly in half\n",
        "* unbalanced binary variable will indicate whether $Y <5$."
      ]
    },
    {
      "metadata": {
        "id": "fSQHngeGHrbj",
        "colab_type": "code",
        "outputId": "b31ce394-60be-4cec-f379-756203869b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "friedman['Y_bal'] = friedman['Y'].apply(lambda y: 1 if (y < friedman.Y.median()) else 0)\n",
        "friedman['Y_un'] = friedman['Y'].apply(lambda y: 1 if (y < 5) else 0)\n",
        "\n",
        "print(friedman.Y_bal.value_counts(), '\\n\\n', friedman.Y_un.value_counts())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    750\n",
            "0    750\n",
            "Name: Y_bal, dtype: int64 \n",
            "\n",
            " 0    1461\n",
            "1      39\n",
            "Name: Y_un, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "keb3N-12oT9U",
        "colab_type": "code",
        "outputId": "027ec6b0-5229-4480-bc12-e38682f81a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "friedman.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>Y</th>\n",
              "      <th>Y_bal</th>\n",
              "      <th>Y_un</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.374540</td>\n",
              "      <td>0.950714</td>\n",
              "      <td>0.731994</td>\n",
              "      <td>0.598658</td>\n",
              "      <td>0.156019</td>\n",
              "      <td>0.155995</td>\n",
              "      <td>0.058084</td>\n",
              "      <td>0.866176</td>\n",
              "      <td>0.601115</td>\n",
              "      <td>0.708073</td>\n",
              "      <td>0.413325</td>\n",
              "      <td>0.956360</td>\n",
              "      <td>0.698205</td>\n",
              "      <td>0.568729</td>\n",
              "      <td>16.730915</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.181825</td>\n",
              "      <td>0.183405</td>\n",
              "      <td>0.304242</td>\n",
              "      <td>0.524756</td>\n",
              "      <td>0.431945</td>\n",
              "      <td>0.291229</td>\n",
              "      <td>0.611853</td>\n",
              "      <td>0.139494</td>\n",
              "      <td>0.292145</td>\n",
              "      <td>0.366362</td>\n",
              "      <td>0.223650</td>\n",
              "      <td>0.167876</td>\n",
              "      <td>0.280668</td>\n",
              "      <td>0.561691</td>\n",
              "      <td>9.112092</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.592415</td>\n",
              "      <td>0.046450</td>\n",
              "      <td>0.607545</td>\n",
              "      <td>0.170524</td>\n",
              "      <td>0.065052</td>\n",
              "      <td>0.948886</td>\n",
              "      <td>0.965632</td>\n",
              "      <td>0.808397</td>\n",
              "      <td>0.304614</td>\n",
              "      <td>0.097672</td>\n",
              "      <td>0.617692</td>\n",
              "      <td>0.078302</td>\n",
              "      <td>0.583794</td>\n",
              "      <td>0.132396</td>\n",
              "      <td>3.017894</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.034389</td>\n",
              "      <td>0.909320</td>\n",
              "      <td>0.258780</td>\n",
              "      <td>0.662522</td>\n",
              "      <td>0.311711</td>\n",
              "      <td>0.520068</td>\n",
              "      <td>0.546710</td>\n",
              "      <td>0.184854</td>\n",
              "      <td>0.969585</td>\n",
              "      <td>0.775133</td>\n",
              "      <td>0.037718</td>\n",
              "      <td>0.917699</td>\n",
              "      <td>0.267895</td>\n",
              "      <td>0.699477</td>\n",
              "      <td>10.220976</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.088493</td>\n",
              "      <td>0.195983</td>\n",
              "      <td>0.045227</td>\n",
              "      <td>0.325330</td>\n",
              "      <td>0.388677</td>\n",
              "      <td>0.271349</td>\n",
              "      <td>0.828738</td>\n",
              "      <td>0.356753</td>\n",
              "      <td>0.280935</td>\n",
              "      <td>0.542696</td>\n",
              "      <td>0.045148</td>\n",
              "      <td>0.217475</td>\n",
              "      <td>0.017812</td>\n",
              "      <td>0.346724</td>\n",
              "      <td>9.770285</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         X1        X2        X3        X4        X5        X6        X7  \\\n",
              "0  0.374540  0.950714  0.731994  0.598658  0.156019  0.155995  0.058084   \n",
              "1  0.181825  0.183405  0.304242  0.524756  0.431945  0.291229  0.611853   \n",
              "2  0.592415  0.046450  0.607545  0.170524  0.065052  0.948886  0.965632   \n",
              "3  0.034389  0.909320  0.258780  0.662522  0.311711  0.520068  0.546710   \n",
              "4  0.088493  0.195983  0.045227  0.325330  0.388677  0.271349  0.828738   \n",
              "\n",
              "         X8        X9       X10       X11       X12       X13       X14  \\\n",
              "0  0.866176  0.601115  0.708073  0.413325  0.956360  0.698205  0.568729   \n",
              "1  0.139494  0.292145  0.366362  0.223650  0.167876  0.280668  0.561691   \n",
              "2  0.808397  0.304614  0.097672  0.617692  0.078302  0.583794  0.132396   \n",
              "3  0.184854  0.969585  0.775133  0.037718  0.917699  0.267895  0.699477   \n",
              "4  0.356753  0.280935  0.542696  0.045148  0.217475  0.017812  0.346724   \n",
              "\n",
              "           Y  Y_bal  Y_un  \n",
              "0  16.730915      0     0  \n",
              "1   9.112092      1     0  \n",
              "2   3.017894      1     1  \n",
              "3  10.220976      1     0  \n",
              "4   9.770285      1     0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "l8Ab5FKxjkDy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Finally, let's put it all into our usual X and y's\n",
        "# (I already have the X dataframe as friedmanX, but I'm working backward to\n",
        "# follow a usual flow)\n",
        "\n",
        "X = friedman.drop(columns=['Y', 'Y_bal', 'Y_un'])\n",
        "\n",
        "y = friedman.Y\n",
        "\n",
        "y_bal = friedman.Y_bal\n",
        "\n",
        "y_un = friedman.Y_un"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q11QTvNXI87Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Alright! Let's get to it! Remember, with each part, we are increasing complexity of the analysis and thereby increasing the computational costs and runtime."
      ]
    },
    {
      "metadata": {
        "id": "K_7irYK2QfES",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So even before univariate selection--which compares each feature to the output feature one by one--there is a [VarianceThreshold](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) object in sklearn.feature_selection. It defaults to getting rid of any features that are the same across all samples. Great for cleaning data in that respect. \n",
        "\n",
        "The `threshold` parameter defaults to `0` to show the above behavior. if you change it, make sure you have good reason. Use with caution."
      ]
    },
    {
      "metadata": {
        "id": "flvDcuYAbPxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1: univariate selection\n",
        "* Best for goal 2 - getting \"a better understanding of the data, its structure and characteristics\"\n",
        "* unable to remove redundancy (for example selecting only the best feature among a subset of strongly correlated features)\n",
        "* Super fast - can be used for baseline models or just after baseline\n",
        "\n",
        "[sci-kit's univariariate feature selection objects and techniques](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)"
      ]
    },
    {
      "metadata": {
        "id": "qZqzkGVmM_aj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Y (continuous output)\n",
        "\n",
        "options (they do what they sound like they do)\n",
        "* SelectKBest\n",
        "* SelectPercentile\n",
        "\n",
        "both take the same parameter options for `score_func`\n",
        "* `f_regression`: scores by correlation coefficient, f value, p value--basically automates what you can do by looking at a correlation matrix except without the ability to recognize collinearity\n",
        "* `mutual_info_regression`: can capture non-linear correlations, but doesn't handle noise well\n",
        "\n",
        "Let's take a look at mutual information (MI)"
      ]
    },
    {
      "metadata": {
        "id": "lsDZpC8PM-cc",
        "colab_type": "code",
        "outputId": "57baeec7-0d9f-4c21-e75b-81e002d103e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "import sklearn.feature_selection as fe\n",
        "\n",
        "MIR = fe.SelectKBest(fe.mutual_info_regression, k='all').fit(X, y)\n",
        "\n",
        "MIR_scores = pd.Series(data=MIR.scores_, name='MI_Reg_Scores', index=names)\n",
        "\n",
        "MIR_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.117499\n",
              "X2     0.155141\n",
              "X3     0.064104\n",
              "X4     0.239914\n",
              "X5     0.059098\n",
              "X6     0.006906\n",
              "X7     0.000000\n",
              "X8     0.000000\n",
              "X9     0.000000\n",
              "X10    0.000000\n",
              "X11    0.069175\n",
              "X12    0.113326\n",
              "X13    0.036673\n",
              "X14    0.220115\n",
              "Name: MI_Reg_Scores, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "uIUBoCf8NsIA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Y_bal (balanced binary output)\n",
        "\n",
        "options\n",
        "* SelectKBest\n",
        "* SelectPercentile\n",
        "\n",
        "these options will cut out features with error rates above a certain tolerance level, define in parameter -`alpha`\n",
        "* SelectFpr (false positive rate--false positives predicted/total negatives in dataset)\n",
        "* SelectFdr (false discovery rate--false positives predicted/total positives predicted)\n",
        "* ~~SelectFwe (family-wise error--for multinomial classification tasks)~~\n",
        "\n",
        "all have the same optons for parameter `score_func`\n",
        "* `chi2`\n",
        "* `f_classif`\n",
        "* `mutual_info_classif`"
      ]
    },
    {
      "metadata": {
        "id": "vEkioNaHbPxR",
        "colab_type": "code",
        "outputId": "01198e23-0db3-47df-97af-aa1c6b687fdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "MIC_b = fe.SelectFpr(fe.mutual_info_classif).fit(X, y_bal)\n",
        "\n",
        "MIC_b_scores = pd.Series(data=MIC_b.scores_, \n",
        "                              name='MIC_Bal_Scores', index=names)\n",
        "\n",
        "MIC_b_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.061386\n",
              "X2     0.111060\n",
              "X3     0.006320\n",
              "X4     0.113995\n",
              "X5     0.000000\n",
              "X6     0.000000\n",
              "X7     0.000000\n",
              "X8     0.000000\n",
              "X9     0.000000\n",
              "X10    0.018296\n",
              "X11    0.063916\n",
              "X12    0.091263\n",
              "X13    0.028924\n",
              "X14    0.109699\n",
              "Name: MIC_Bal_Scores, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "d0fi6NoHXG7u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Y_un (unbalanced binary output)"
      ]
    },
    {
      "metadata": {
        "id": "6CWdeeg0XGhh",
        "colab_type": "code",
        "outputId": "010effb2-8543-4524-8a84-1c667cff0302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "MIC_u = fe.SelectFpr(fe.mutual_info_classif).fit(X, y_un)\n",
        "\n",
        "MIC_u_scores = pd.Series(data=MIC_u.scores_, \n",
        "                              name='MIC_Unbal_Scores', index=names)\n",
        "\n",
        "MIC_u_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.007352\n",
              "X2     0.011942\n",
              "X3     0.010933\n",
              "X4     0.042207\n",
              "X5     0.015932\n",
              "X6     0.000000\n",
              "X7     0.000000\n",
              "X8     0.006586\n",
              "X9     0.000000\n",
              "X10    0.000000\n",
              "X11    0.004096\n",
              "X12    0.011106\n",
              "X13    0.006392\n",
              "X14    0.033513\n",
              "Name: MIC_Unbal_Scores, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "GBavXOYvbPxb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2: linear models and regularization\n",
        "* L1 Regularization (Lasso for regression) is best for goal 1: \"produces sparse solutions and as such is very useful selecting a strong subset of features for improving model performance\" (forces coefficients to zero, telling you which you could remove--will remove worst out of multicollinearly related features)\n",
        "* L2 Regularization (Ridge for regression) is best for goal 2: \"can be used for data interpretation due to its stability and the fact that useful features tend to have non-zero coefficients\n",
        "* Also fast\n",
        "\n",
        "[sci-kit's L1 feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection) (can easily be switched to L2 using the parameter `penalty='l2'` for categorical targets or using `Ridge` instead of Lasso for continuous targets)\n",
        "\n",
        "We won't do this here, because\n",
        "1.   You know regression\n",
        "2.   The same principles apply as shown in Part 3 below with `SelectFromModel`\n",
        "3.   There's way cooler stuff coming up"
      ]
    },
    {
      "metadata": {
        "id": "lrTUHbfwbPxk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3: random forests\n",
        "* Best for goal 1, not 2 because:\n",
        " * strong features can end up with low scores \n",
        " * biased towards variables with many categories\n",
        "* \"require very little feature engineering and parameter tuning\"\n",
        "* Takes a little more time depending on your dataset - but a popular technique\n",
        "\n",
        "[sci-kit's implementation of tree-based feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection)"
      ]
    },
    {
      "metadata": {
        "id": "RKDsc9AjP4fD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Y"
      ]
    },
    {
      "metadata": {
        "id": "8pP7OuQTbPxo",
        "colab_type": "code",
        "outputId": "c0c07bb3-f041-47e2-9e7f-0f681adab58d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor as RFR\n",
        "\n",
        "# Fitting a random forest regression\n",
        "rfr = RFR().fit(X, y)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "rfr_scores = pd.Series(data=rfr.feature_importances_, name='RFR', index=names)\n",
        "\n",
        "rfr_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.171971\n",
              "X2     0.080160\n",
              "X3     0.031443\n",
              "X4     0.277327\n",
              "X5     0.089315\n",
              "X6     0.006989\n",
              "X7     0.007058\n",
              "X8     0.007323\n",
              "X9     0.007486\n",
              "X10    0.007776\n",
              "X11    0.045425\n",
              "X12    0.133982\n",
              "X13    0.024321\n",
              "X14    0.109423\n",
              "Name: RFR, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "0dEoMeW2Z88Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Y_bal"
      ]
    },
    {
      "metadata": {
        "id": "O9JuV3HCaAJw",
        "colab_type": "code",
        "outputId": "02382c32-d84e-432b-f2d4-30f60fe4f822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "\n",
        "# Fitting a Random Forest Classifier\n",
        "rfc_b = RFC().fit(X, y_bal)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "rfc_b_scores = pd.Series(data=rfc_b.feature_importances_, name='RFC_bal', \n",
        "                           index=names)\n",
        "\n",
        "rfc_b_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.119943\n",
              "X2     0.115253\n",
              "X3     0.039748\n",
              "X4     0.114719\n",
              "X5     0.071332\n",
              "X6     0.027339\n",
              "X7     0.024874\n",
              "X8     0.026811\n",
              "X9     0.028619\n",
              "X10    0.018861\n",
              "X11    0.116147\n",
              "X12    0.103445\n",
              "X13    0.042709\n",
              "X14    0.150199\n",
              "Name: RFC_bal, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "pUt0DSzRaNWF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Y_un"
      ]
    },
    {
      "metadata": {
        "id": "uTuAcM7baYUv",
        "colab_type": "code",
        "outputId": "17955ba2-dd5f-4af3-f4da-b2f316a5e140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "# Fitting a Random Forest Classifier\n",
        "rfc_u = RFC().fit(X, y_un)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "rfc_u_scores = pd.Series(data=rfc_u.feature_importances_, \n",
        "                             name='RFC_unbal', index=names)\n",
        "\n",
        "rfc_u_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.075621\n",
              "X2     0.130585\n",
              "X3     0.030374\n",
              "X4     0.175625\n",
              "X5     0.064946\n",
              "X6     0.078695\n",
              "X7     0.024836\n",
              "X8     0.029368\n",
              "X9     0.031935\n",
              "X10    0.030780\n",
              "X11    0.059017\n",
              "X12    0.104890\n",
              "X13    0.015534\n",
              "X14    0.147796\n",
              "Name: RFC_unbal, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "LR0vdLluatex",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SelectFromModel \n",
        "is a meta-transformer that can be used along with any estimator that has a `coef_` or `feature_importances_` attribute after fitting. The features are considered unimportant and removed, if the corresponding `coef_` or `feature_importances_` values are below the provided `threshold` parameter. Apart from specifying the `threshold` numerically, there are built-in heuristics for finding a `threshold` using a string argument. Available heuristics are `'mean'`, `'median'` and float multiples of these like `'0.1*mean'`."
      ]
    },
    {
      "metadata": {
        "id": "3ZeUn8PTatE8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Random forest regression transformation of X (elimination of least important\n",
        "# features)\n",
        "rfr_transform = fe.SelectFromModel(rfr, prefit=True)\n",
        "\n",
        "X_rfr = rfr_transform.transform(X)\n",
        "\n",
        "\n",
        "# Random forest classifier transformation of X_bal (elimination of least important\n",
        "# features)\n",
        "rfc_b_transform = fe.SelectFromModel(rfc_b, prefit=True)\n",
        "\n",
        "X_rfc_b = rfc_b_transform.transform(X)\n",
        "\n",
        "\n",
        "# Random forest classifier transformation of X_un (elimination of least important\n",
        "# features)\n",
        "rfc_u_transform = fe.SelectFromModel(rfc_u, prefit=True)\n",
        "\n",
        "X_rfc_u = rfc_u_transform.transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnXIN6lkc5hr",
        "colab_type": "code",
        "outputId": "d98e8e70-ebc8-4abd-b622-97b94ea001ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "cell_type": "code",
      "source": [
        "RF_comparisons = pd.DataFrame(data=np.array([rfr_transform.get_support(),\n",
        "                                    rfc_b_transform.get_support(),\n",
        "                                    rfc_u_transform.get_support()]).T,\n",
        "                              columns=['RF_Regressor', 'RF_balanced_classifier',\n",
        "                                     'RF_unbalanced_classifier'],\n",
        "                              index=names)\n",
        "\n",
        "RF_comparisons"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RF_Regressor</th>\n",
              "      <th>RF_balanced_classifier</th>\n",
              "      <th>RF_unbalanced_classifier</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X1</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X2</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X4</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X5</th>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X6</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X7</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X8</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X9</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X10</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X11</th>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X12</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X13</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X14</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     RF_Regressor  RF_balanced_classifier  RF_unbalanced_classifier\n",
              "X1           True                    True                      True\n",
              "X2           True                    True                      True\n",
              "X3          False                   False                     False\n",
              "X4           True                    True                      True\n",
              "X5           True                   False                     False\n",
              "X6          False                   False                      True\n",
              "X7          False                   False                     False\n",
              "X8          False                   False                     False\n",
              "X9          False                   False                     False\n",
              "X10         False                   False                     False\n",
              "X11         False                    True                     False\n",
              "X12          True                    True                      True\n",
              "X13         False                   False                     False\n",
              "X14          True                    True                      True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "Nzas2GMebPxv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 4: stability selection, RFE, and everything side by side\n",
        "* These methods take longer since they are *wrapper methods* and build multiple ML models before giving results. \"They both build on top of other (model based) selection methods such as regression or SVM, building models on different subsets of data and extracting the ranking from the aggregates.\"\n",
        "* Stability selection is good for both goal 1 and 2: \"among the top performing methods for many different datasets and settings\"\n",
        " * For categorical targets\n",
        "   * [RandomizedLogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLogisticRegression.html)(Deprecated) ~~use [RandomizedLogisticRegression](https://thuijskens.github.io/stability-selection/docs/randomized_lasso.html#stability_selection.randomized_lasso.RandomizedLogisticRegression)~~\n",
        "  \n",
        "   * [ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier)\n",
        " \n",
        " * For continuous targets\n",
        "   * [RandomizedLasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLasso.html) (Deprecated) ~~use [RandomizedLasso](https://thuijskens.github.io/stability-selection/docs/randomized_lasso.html#stability_selection.randomized_lasso.RandomizedLogisticRegression)~~ \n",
        "   * [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor)\n",
        "   \n",
        "    Welcome to open-source, folks! [Here](https://github.com/scikit-learn/scikit-learn/issues/8995) is the original discussion to deprecate `RandomizedLogisticRegression` and `RandomizedLasso`. [Here](https://github.com/scikit-learn/scikit-learn/issues/9657) is a failed attempt to resurrect it. It looks like it'll be gone for good soon. So we shouldn't get dependent on it.\n",
        "\n",
        "    The alternatives from the deprecated scikit objects come from an official scikit-learn-contrib module called [stability_selection](https://github.com/scikit-learn-contrib/stability-selection). They also have a `StabilitySelection` object that acts similarly scikit's `SelectFromModel`.\n",
        "\n",
        "* recursive feature elimination (RFE) is best for goal 1\n",
        " * [sci-kit's RFE and RFECV (RFE with built-in cross-validation)](https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination)"
      ]
    },
    {
      "metadata": {
        "id": "6722EqvCbPxx",
        "colab_type": "code",
        "outputId": "60d7dd55-400d-420c-96fe-22055942be06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/scikit-learn-contrib/stability-selection.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/scikit-learn-contrib/stability-selection.git\n",
            "  Cloning https://github.com/scikit-learn-contrib/stability-selection.git to /tmp/pip-req-build-ob4i4ieh\n",
            "Requirement already satisfied: nose>=1.1.2 in /usr/local/lib/python3.6/dist-packages (from stability-selection==0.0.1) (1.3.7)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.6/dist-packages (from stability-selection==0.0.1) (0.20.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from stability-selection==0.0.1) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from stability-selection==0.0.1) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19->stability-selection==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->stability-selection==0.0.1) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->stability-selection==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->stability-selection==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->stability-selection==0.0.1) (2.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0.0->stability-selection==0.0.1) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->stability-selection==0.0.1) (40.7.0)\n",
            "Building wheels for collected packages: stability-selection\n",
            "  Building wheel for stability-selection (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-bkrq6z9s/wheels/58/be/39/79880712b91ffa56e341ff10586a1956527813437ddd759473\n",
            "Successfully built stability-selection\n",
            "Installing collected packages: stability-selection\n",
            "Successfully installed stability-selection-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mhLZ-il_rm_a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Okay, I tried this package... it seems to have some problems... hopefully a good implementation of stability selection for Lasso and Logistic Regression will be created soon! In the meantime, scikit's RandomLasso and RandomLogisticRegression have not been removed, so you can fiddle some! Just alter the commented out code!\n",
        "* import from scikit instead of stability-selection\n",
        "* use scikit's `SelectFromModel` as shown above!\n",
        "\n",
        "Ta Da!"
      ]
    },
    {
      "metadata": {
        "id": "etgVb1BqkyGX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Y"
      ]
    },
    {
      "metadata": {
        "id": "ViYGwRzVbk7N",
        "colab_type": "code",
        "outputId": "53f0eafd-d1b0-42a0-a00a-41a1ca8d24f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "'''from stability_selection import (RandomizedLogisticRegression,\n",
        "                                 RandomizedLasso, StabilitySelection,\n",
        "                                 plot_stability_path)\n",
        "\n",
        "# Stability selection using randomized lasso method\n",
        "rl = RandomizedLasso(max_iter=2000)\n",
        "rl_selector = StabilitySelection(base_estimator=rl, lambda_name='alpha',\n",
        "                                 n_jobs=2)\n",
        "rl_selector.fit(X, y);\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from stability_selection import (RandomizedLogisticRegression,\\n                                 RandomizedLasso, StabilitySelection,\\n                                 plot_stability_path)\\n\\n# Stability selection using randomized lasso method\\nrl = RandomizedLasso(max_iter=2000)\\nrl_selector = StabilitySelection(base_estimator=rl, lambda_name='alpha',\\n                                 n_jobs=2)\\nrl_selector.fit(X, y);\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "Vnza3BUPmiRo",
        "colab_type": "code",
        "outputId": "9ea7b3b2-e0b4-45fd-f101-4e49c4f09383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor as ETR\n",
        "\n",
        "# Stability selection using randomized decision trees\n",
        "etr = ETR(n_estimators=50).fit(X, y)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "etr_scores = pd.Series(data=etr.feature_importances_, \n",
        "                             name='ETR', index=names)\n",
        "\n",
        "etr_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.124593\n",
              "X2     0.116771\n",
              "X3     0.030910\n",
              "X4     0.204025\n",
              "X5     0.086907\n",
              "X6     0.008103\n",
              "X7     0.008218\n",
              "X8     0.008241\n",
              "X9     0.007985\n",
              "X10    0.007440\n",
              "X11    0.106018\n",
              "X12    0.104079\n",
              "X13    0.029207\n",
              "X14    0.157503\n",
              "Name: ETR, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "uiwjyAya5YWL",
        "colab_type": "code",
        "outputId": "57c963f2-f06d-496a-967d-82a24a64c968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Recursive feature elimination with cross validaiton using linear regression \n",
        "# as the model\n",
        "lr = LinearRegression()\n",
        "# rank all features, i.e continue the elimination until the last one\n",
        "rfe = fe.RFECV(lr)\n",
        "rfe.fit(X, y)\n",
        "\n",
        "# Make negative so it works better for comparison to other scores after scaling\n",
        "rfe_score = pd.Series(data=(-1*rfe.ranking_), name='RFE', index=names)\n",
        "\n",
        "rfe_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1    -1\n",
              "X2    -1\n",
              "X3    -1\n",
              "X4    -1\n",
              "X5    -1\n",
              "X6    -8\n",
              "X7    -5\n",
              "X8    -7\n",
              "X9    -6\n",
              "X10   -9\n",
              "X11   -2\n",
              "X12   -4\n",
              "X13   -1\n",
              "X14   -3\n",
              "Name: RFE, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "FcGTYITXk0tb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Y_bal"
      ]
    },
    {
      "metadata": {
        "id": "bwfhOxBykwPx",
        "colab_type": "code",
        "outputId": "4749a143-bef2-44b7-c58c-86c318961247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "# stability selection using randomized logistic regression\n",
        "'''rlr_b = RandomizedLogisticRegression()\n",
        "rlr_b_selector = StabilitySelection(base_estimator=rlr_b, lambda_name='C',\n",
        "                                 n_jobs=2)\n",
        "rlr_b_selector.fit(X, y_bal);'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"rlr_b = RandomizedLogisticRegression()\\nrlr_b_selector = StabilitySelection(base_estimator=rlr_b, lambda_name='C',\\n                                 n_jobs=2)\\nrlr_b_selector.fit(X, y_bal);\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "8TLTMsPkoLzX",
        "colab_type": "code",
        "outputId": "3d7ed25c-6945-4b98-c097-2008f11bd9a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier as ETC\n",
        "\n",
        "# Stability selection using randomized decision trees\n",
        "etc_b = ETC(n_estimators=50).fit(X, y_bal)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "etc_b_scores = pd.Series(data=etc_b.feature_importances_, \n",
        "                             name='ETC_bal', index=names)\n",
        "\n",
        "etc_b_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.105576\n",
              "X2     0.113267\n",
              "X3     0.044241\n",
              "X4     0.133987\n",
              "X5     0.079805\n",
              "X6     0.028492\n",
              "X7     0.026542\n",
              "X8     0.029527\n",
              "X9     0.030618\n",
              "X10    0.026719\n",
              "X11    0.110347\n",
              "X12    0.096993\n",
              "X13    0.044999\n",
              "X14    0.128890\n",
              "Name: ETC_bal, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "cRaI4yA97kLo",
        "colab_type": "code",
        "outputId": "52d837e9-aef0-46be-9018-2663b1c0cb81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Recursive feature elimination with cross validaiton using logistic regression \n",
        "# as the model\n",
        "logr_b = LogisticRegression(solver='lbfgs')\n",
        "# rank all features, i.e continue the elimination until the last one\n",
        "rfe_b = fe.RFECV(logr_b)\n",
        "rfe_b.fit(X, y_bal)\n",
        "\n",
        "# Make negative so it works better for comparison to other scores after scaling\n",
        "rfe_b_score = pd.Series(data=(-1*rfe_b.ranking_), name='RFE_bal', index=names)\n",
        "\n",
        "rfe_b_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1    -1\n",
              "X2    -1\n",
              "X3    -1\n",
              "X4    -1\n",
              "X5    -1\n",
              "X6    -4\n",
              "X7    -1\n",
              "X8    -2\n",
              "X9    -3\n",
              "X10   -1\n",
              "X11   -1\n",
              "X12   -1\n",
              "X13   -5\n",
              "X14   -1\n",
              "Name: RFE_bal, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "ZDe5AYa1lO53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Y_un"
      ]
    },
    {
      "metadata": {
        "id": "jThOwdBqlSEj",
        "colab_type": "code",
        "outputId": "f7da12d4-3570-4938-fb96-1667b68b1176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "# stability selection uisng randomized logistic regression\n",
        "'''rlr_u = RandomizedLogisticRegression(max_iter=2000)\n",
        "rlr_u_selector = StabilitySelection(base_estimator=rlr_u, lambda_name='C')\n",
        "\n",
        "rlr_u_selector.fit(X, y_un);'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"rlr_u = RandomizedLogisticRegression(max_iter=2000)\\nrlr_u_selector = StabilitySelection(base_estimator=rlr_u, lambda_name='C')\\n\\nrlr_u_selector.fit(X, y_un);\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "G2og3pRNoYXo",
        "colab_type": "code",
        "outputId": "34b81e7d-a87d-4390-97dd-e2837a97b459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# Stability selection using randomized decision trees\n",
        "etc_u = ETC(n_estimators=50).fit(X, y_un)\n",
        "\n",
        "# Creating scores from feature_importances_ ranking (some randomness here)\n",
        "etc_u_scores = pd.Series(data=etc_u.feature_importances_, \n",
        "                             name='ETC_unbal', index=names)\n",
        "\n",
        "etc_u_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1     0.080087\n",
              "X2     0.091999\n",
              "X3     0.056712\n",
              "X4     0.122631\n",
              "X5     0.082176\n",
              "X6     0.039102\n",
              "X7     0.046938\n",
              "X8     0.042750\n",
              "X9     0.052450\n",
              "X10    0.058599\n",
              "X11    0.080919\n",
              "X12    0.091330\n",
              "X13    0.046816\n",
              "X14    0.107490\n",
              "Name: ETC_unbal, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "fCLO-2tH8KBe",
        "colab_type": "code",
        "outputId": "0ff05979-5ba9-4c2d-fe37-6a7018fb56d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "# Recursive feature elimination with cross validaiton using logistic regression \n",
        "# as the model\n",
        "logr_u = LogisticRegression(solver='lbfgs')\n",
        "# rank all features, i.e continue the elimination until the last one\n",
        "rfe_u = fe.RFECV(logr_u)\n",
        "rfe_u.fit(X, y_un)\n",
        "\n",
        "# Make negative so it works better for comparison to other scores after scaling\n",
        "rfe_u_score = pd.Series(data=(-1*rfe_u.ranking_), name='RFE_unbal', index=names)\n",
        "\n",
        "rfe_u_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "X1    -1\n",
              "X2    -1\n",
              "X3    -6\n",
              "X4    -1\n",
              "X5    -1\n",
              "X6    -4\n",
              "X7    -5\n",
              "X8    -1\n",
              "X9    -3\n",
              "X10   -1\n",
              "X11   -1\n",
              "X12   -1\n",
              "X13   -2\n",
              "X14   -1\n",
              "Name: RFE_unbal, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "RND1UIMRo6kk",
        "colab_type": "code",
        "outputId": "96780d9f-613f-4974-b251-45aa44ca2eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "'''RL_comparisons = pd.DataFrame(data=np.array([rl_selector.get_support(),\n",
        "                                    rlr_b_selector.get_support(),\n",
        "                                    rlr_u_selector.get_support()]).T,\n",
        "                              columns=['RandomLasso', 'RandomLog_bal',\n",
        "                                     'RandomLog_unbal'],\n",
        "                              index=names)\n",
        "\n",
        "RL_comparisons'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"RL_comparisons = pd.DataFrame(data=np.array([rl_selector.get_support(),\\n                                    rlr_b_selector.get_support(),\\n                                    rlr_u_selector.get_support()]).T,\\n                              columns=['RandomLasso', 'RandomLog_bal',\\n                                     'RandomLog_unbal'],\\n                              index=names)\\n\\nRL_comparisons\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "U0-G9GGjpq1Z",
        "colab_type": "code",
        "outputId": "a079d731-0165-48ed-ff24-b7b733b357c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "cell_type": "code",
      "source": [
        "comparisons = pd.concat([MIR_scores, MIC_b_scores, MIC_u_scores, rfr_scores,\n",
        "                         rfc_b_scores, rfc_u_scores, etr_scores, etc_b_scores,\n",
        "                         etc_u_scores, rfe_score, rfe_b_score, rfe_u_score], \n",
        "                        axis=1)\n",
        "\n",
        "comparisons"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MI_Reg_Scores</th>\n",
              "      <th>MIC_Bal_Scores</th>\n",
              "      <th>MIC_Unbal_Scores</th>\n",
              "      <th>RFR</th>\n",
              "      <th>RFC_bal</th>\n",
              "      <th>RFC_unbal</th>\n",
              "      <th>ETR</th>\n",
              "      <th>ETC_bal</th>\n",
              "      <th>ETC_unbal</th>\n",
              "      <th>RFE</th>\n",
              "      <th>RFE_bal</th>\n",
              "      <th>RFE_unbal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X1</th>\n",
              "      <td>0.117499</td>\n",
              "      <td>0.061386</td>\n",
              "      <td>0.007352</td>\n",
              "      <td>0.171971</td>\n",
              "      <td>0.119943</td>\n",
              "      <td>0.075621</td>\n",
              "      <td>0.124593</td>\n",
              "      <td>0.105576</td>\n",
              "      <td>0.080087</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X2</th>\n",
              "      <td>0.155141</td>\n",
              "      <td>0.111060</td>\n",
              "      <td>0.011942</td>\n",
              "      <td>0.080160</td>\n",
              "      <td>0.115253</td>\n",
              "      <td>0.130585</td>\n",
              "      <td>0.116771</td>\n",
              "      <td>0.113267</td>\n",
              "      <td>0.091999</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X3</th>\n",
              "      <td>0.064104</td>\n",
              "      <td>0.006320</td>\n",
              "      <td>0.010933</td>\n",
              "      <td>0.031443</td>\n",
              "      <td>0.039748</td>\n",
              "      <td>0.030374</td>\n",
              "      <td>0.030910</td>\n",
              "      <td>0.044241</td>\n",
              "      <td>0.056712</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X4</th>\n",
              "      <td>0.239914</td>\n",
              "      <td>0.113995</td>\n",
              "      <td>0.042207</td>\n",
              "      <td>0.277327</td>\n",
              "      <td>0.114719</td>\n",
              "      <td>0.175625</td>\n",
              "      <td>0.204025</td>\n",
              "      <td>0.133987</td>\n",
              "      <td>0.122631</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X5</th>\n",
              "      <td>0.059098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015932</td>\n",
              "      <td>0.089315</td>\n",
              "      <td>0.071332</td>\n",
              "      <td>0.064946</td>\n",
              "      <td>0.086907</td>\n",
              "      <td>0.079805</td>\n",
              "      <td>0.082176</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X6</th>\n",
              "      <td>0.006906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006989</td>\n",
              "      <td>0.027339</td>\n",
              "      <td>0.078695</td>\n",
              "      <td>0.008103</td>\n",
              "      <td>0.028492</td>\n",
              "      <td>0.039102</td>\n",
              "      <td>-8</td>\n",
              "      <td>-4</td>\n",
              "      <td>-4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007058</td>\n",
              "      <td>0.024874</td>\n",
              "      <td>0.024836</td>\n",
              "      <td>0.008218</td>\n",
              "      <td>0.026542</td>\n",
              "      <td>0.046938</td>\n",
              "      <td>-5</td>\n",
              "      <td>-1</td>\n",
              "      <td>-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006586</td>\n",
              "      <td>0.007323</td>\n",
              "      <td>0.026811</td>\n",
              "      <td>0.029368</td>\n",
              "      <td>0.008241</td>\n",
              "      <td>0.029527</td>\n",
              "      <td>0.042750</td>\n",
              "      <td>-7</td>\n",
              "      <td>-2</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007486</td>\n",
              "      <td>0.028619</td>\n",
              "      <td>0.031935</td>\n",
              "      <td>0.007985</td>\n",
              "      <td>0.030618</td>\n",
              "      <td>0.052450</td>\n",
              "      <td>-6</td>\n",
              "      <td>-3</td>\n",
              "      <td>-3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018296</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007776</td>\n",
              "      <td>0.018861</td>\n",
              "      <td>0.030780</td>\n",
              "      <td>0.007440</td>\n",
              "      <td>0.026719</td>\n",
              "      <td>0.058599</td>\n",
              "      <td>-9</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X11</th>\n",
              "      <td>0.069175</td>\n",
              "      <td>0.063916</td>\n",
              "      <td>0.004096</td>\n",
              "      <td>0.045425</td>\n",
              "      <td>0.116147</td>\n",
              "      <td>0.059017</td>\n",
              "      <td>0.106018</td>\n",
              "      <td>0.110347</td>\n",
              "      <td>0.080919</td>\n",
              "      <td>-2</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X12</th>\n",
              "      <td>0.113326</td>\n",
              "      <td>0.091263</td>\n",
              "      <td>0.011106</td>\n",
              "      <td>0.133982</td>\n",
              "      <td>0.103445</td>\n",
              "      <td>0.104890</td>\n",
              "      <td>0.104079</td>\n",
              "      <td>0.096993</td>\n",
              "      <td>0.091330</td>\n",
              "      <td>-4</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X13</th>\n",
              "      <td>0.036673</td>\n",
              "      <td>0.028924</td>\n",
              "      <td>0.006392</td>\n",
              "      <td>0.024321</td>\n",
              "      <td>0.042709</td>\n",
              "      <td>0.015534</td>\n",
              "      <td>0.029207</td>\n",
              "      <td>0.044999</td>\n",
              "      <td>0.046816</td>\n",
              "      <td>-1</td>\n",
              "      <td>-5</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X14</th>\n",
              "      <td>0.220115</td>\n",
              "      <td>0.109699</td>\n",
              "      <td>0.033513</td>\n",
              "      <td>0.109423</td>\n",
              "      <td>0.150199</td>\n",
              "      <td>0.147796</td>\n",
              "      <td>0.157503</td>\n",
              "      <td>0.128890</td>\n",
              "      <td>0.107490</td>\n",
              "      <td>-3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     MI_Reg_Scores  MIC_Bal_Scores  MIC_Unbal_Scores       RFR   RFC_bal  \\\n",
              "X1        0.117499        0.061386          0.007352  0.171971  0.119943   \n",
              "X2        0.155141        0.111060          0.011942  0.080160  0.115253   \n",
              "X3        0.064104        0.006320          0.010933  0.031443  0.039748   \n",
              "X4        0.239914        0.113995          0.042207  0.277327  0.114719   \n",
              "X5        0.059098        0.000000          0.015932  0.089315  0.071332   \n",
              "X6        0.006906        0.000000          0.000000  0.006989  0.027339   \n",
              "X7        0.000000        0.000000          0.000000  0.007058  0.024874   \n",
              "X8        0.000000        0.000000          0.006586  0.007323  0.026811   \n",
              "X9        0.000000        0.000000          0.000000  0.007486  0.028619   \n",
              "X10       0.000000        0.018296          0.000000  0.007776  0.018861   \n",
              "X11       0.069175        0.063916          0.004096  0.045425  0.116147   \n",
              "X12       0.113326        0.091263          0.011106  0.133982  0.103445   \n",
              "X13       0.036673        0.028924          0.006392  0.024321  0.042709   \n",
              "X14       0.220115        0.109699          0.033513  0.109423  0.150199   \n",
              "\n",
              "     RFC_unbal       ETR   ETC_bal  ETC_unbal  RFE  RFE_bal  RFE_unbal  \n",
              "X1    0.075621  0.124593  0.105576   0.080087   -1       -1         -1  \n",
              "X2    0.130585  0.116771  0.113267   0.091999   -1       -1         -1  \n",
              "X3    0.030374  0.030910  0.044241   0.056712   -1       -1         -6  \n",
              "X4    0.175625  0.204025  0.133987   0.122631   -1       -1         -1  \n",
              "X5    0.064946  0.086907  0.079805   0.082176   -1       -1         -1  \n",
              "X6    0.078695  0.008103  0.028492   0.039102   -8       -4         -4  \n",
              "X7    0.024836  0.008218  0.026542   0.046938   -5       -1         -5  \n",
              "X8    0.029368  0.008241  0.029527   0.042750   -7       -2         -1  \n",
              "X9    0.031935  0.007985  0.030618   0.052450   -6       -3         -3  \n",
              "X10   0.030780  0.007440  0.026719   0.058599   -9       -1         -1  \n",
              "X11   0.059017  0.106018  0.110347   0.080919   -2       -1         -1  \n",
              "X12   0.104890  0.104079  0.096993   0.091330   -4       -1         -1  \n",
              "X13   0.015534  0.029207  0.044999   0.046816   -1       -5         -2  \n",
              "X14   0.147796  0.157503  0.128890   0.107490   -3       -1         -1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "Wf-zJWLnt7xG",
        "colab_type": "code",
        "outputId": "49508682-d28f-45d2-96c8-9dee77b1888c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_df = scaler.fit_transform(comparisons)\n",
        "scaled_comparisons = pd.DataFrame(scaled_df, columns=comparisons.columns,\n",
        "                                 index=names)\n",
        "\n",
        "scaled_comparisons"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MI_Reg_Scores</th>\n",
              "      <th>MIC_Bal_Scores</th>\n",
              "      <th>MIC_Unbal_Scores</th>\n",
              "      <th>RFR</th>\n",
              "      <th>RFC_bal</th>\n",
              "      <th>RFC_unbal</th>\n",
              "      <th>ETR</th>\n",
              "      <th>ETC_bal</th>\n",
              "      <th>ETC_unbal</th>\n",
              "      <th>RFE</th>\n",
              "      <th>RFE_bal</th>\n",
              "      <th>RFE_unbal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X1</th>\n",
              "      <td>0.489754</td>\n",
              "      <td>0.538498</td>\n",
              "      <td>0.174183</td>\n",
              "      <td>0.610281</td>\n",
              "      <td>0.769634</td>\n",
              "      <td>0.375329</td>\n",
              "      <td>0.595941</td>\n",
              "      <td>0.735575</td>\n",
              "      <td>0.490660</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X2</th>\n",
              "      <td>0.646649</td>\n",
              "      <td>0.974248</td>\n",
              "      <td>0.282945</td>\n",
              "      <td>0.270663</td>\n",
              "      <td>0.733926</td>\n",
              "      <td>0.718660</td>\n",
              "      <td>0.556153</td>\n",
              "      <td>0.807153</td>\n",
              "      <td>0.633275</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X3</th>\n",
              "      <td>0.267194</td>\n",
              "      <td>0.055444</td>\n",
              "      <td>0.259036</td>\n",
              "      <td>0.090456</td>\n",
              "      <td>0.159033</td>\n",
              "      <td>0.092698</td>\n",
              "      <td>0.119387</td>\n",
              "      <td>0.164721</td>\n",
              "      <td>0.210819</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X4</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.729859</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X5</th>\n",
              "      <td>0.246328</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.377462</td>\n",
              "      <td>0.304530</td>\n",
              "      <td>0.399514</td>\n",
              "      <td>0.308646</td>\n",
              "      <td>0.404238</td>\n",
              "      <td>0.495718</td>\n",
              "      <td>0.515675</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X6</th>\n",
              "      <td>0.028786</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064555</td>\n",
              "      <td>0.394528</td>\n",
              "      <td>0.003373</td>\n",
              "      <td>0.018143</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.045787</td>\n",
              "      <td>0.058103</td>\n",
              "      <td>0.003961</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093808</td>\n",
              "      <td>0.500</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156048</td>\n",
              "      <td>0.001235</td>\n",
              "      <td>0.060536</td>\n",
              "      <td>0.086412</td>\n",
              "      <td>0.004077</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>0.043674</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001839</td>\n",
              "      <td>0.074299</td>\n",
              "      <td>0.102447</td>\n",
              "      <td>0.002772</td>\n",
              "      <td>0.037931</td>\n",
              "      <td>0.159792</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X10</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002909</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.095233</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001641</td>\n",
              "      <td>0.233413</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X11</th>\n",
              "      <td>0.288332</td>\n",
              "      <td>0.560691</td>\n",
              "      <td>0.097037</td>\n",
              "      <td>0.142179</td>\n",
              "      <td>0.740731</td>\n",
              "      <td>0.271614</td>\n",
              "      <td>0.501454</td>\n",
              "      <td>0.779983</td>\n",
              "      <td>0.500626</td>\n",
              "      <td>0.875</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X12</th>\n",
              "      <td>0.472359</td>\n",
              "      <td>0.800586</td>\n",
              "      <td>0.263120</td>\n",
              "      <td>0.469758</td>\n",
              "      <td>0.644020</td>\n",
              "      <td>0.558153</td>\n",
              "      <td>0.491588</td>\n",
              "      <td>0.655689</td>\n",
              "      <td>0.625267</td>\n",
              "      <td>0.625</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X13</th>\n",
              "      <td>0.152859</td>\n",
              "      <td>0.253733</td>\n",
              "      <td>0.151435</td>\n",
              "      <td>0.064112</td>\n",
              "      <td>0.181579</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110728</td>\n",
              "      <td>0.171775</td>\n",
              "      <td>0.092352</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X14</th>\n",
              "      <td>0.917472</td>\n",
              "      <td>0.962307</td>\n",
              "      <td>0.794023</td>\n",
              "      <td>0.378910</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.826162</td>\n",
              "      <td>0.763350</td>\n",
              "      <td>0.952563</td>\n",
              "      <td>0.818731</td>\n",
              "      <td>0.750</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     MI_Reg_Scores  MIC_Bal_Scores  MIC_Unbal_Scores       RFR   RFC_bal  \\\n",
              "X1        0.489754        0.538498          0.174183  0.610281  0.769634   \n",
              "X2        0.646649        0.974248          0.282945  0.270663  0.733926   \n",
              "X3        0.267194        0.055444          0.259036  0.090456  0.159033   \n",
              "X4        1.000000        1.000000          1.000000  1.000000  0.729859   \n",
              "X5        0.246328        0.000000          0.377462  0.304530  0.399514   \n",
              "X6        0.028786        0.000000          0.000000  0.000000  0.064555   \n",
              "X7        0.000000        0.000000          0.000000  0.000256  0.045787   \n",
              "X8        0.000000        0.000000          0.156048  0.001235  0.060536   \n",
              "X9        0.000000        0.000000          0.000000  0.001839  0.074299   \n",
              "X10       0.000000        0.160500          0.000000  0.002909  0.000000   \n",
              "X11       0.288332        0.560691          0.097037  0.142179  0.740731   \n",
              "X12       0.472359        0.800586          0.263120  0.469758  0.644020   \n",
              "X13       0.152859        0.253733          0.151435  0.064112  0.181579   \n",
              "X14       0.917472        0.962307          0.794023  0.378910  1.000000   \n",
              "\n",
              "     RFC_unbal       ETR   ETC_bal  ETC_unbal    RFE  RFE_bal  RFE_unbal  \n",
              "X1    0.375329  0.595941  0.735575   0.490660  1.000     1.00        1.0  \n",
              "X2    0.718660  0.556153  0.807153   0.633275  1.000     1.00        1.0  \n",
              "X3    0.092698  0.119387  0.164721   0.210819  1.000     1.00        0.0  \n",
              "X4    1.000000  1.000000  1.000000   1.000000  1.000     1.00        1.0  \n",
              "X5    0.308646  0.404238  0.495718   0.515675  1.000     1.00        1.0  \n",
              "X6    0.394528  0.003373  0.018143   0.000000  0.125     0.25        0.4  \n",
              "X7    0.058103  0.003961  0.000000   0.093808  0.500     1.00        0.2  \n",
              "X8    0.086412  0.004077  0.027778   0.043674  0.250     0.75        1.0  \n",
              "X9    0.102447  0.002772  0.037931   0.159792  0.375     0.50        0.6  \n",
              "X10   0.095233  0.000000  0.001641   0.233413  0.000     1.00        1.0  \n",
              "X11   0.271614  0.501454  0.779983   0.500626  0.875     1.00        1.0  \n",
              "X12   0.558153  0.491588  0.655689   0.625267  0.625     1.00        1.0  \n",
              "X13   0.000000  0.110728  0.171775   0.092352  1.000     0.00        0.8  \n",
              "X14   0.826162  0.763350  0.952563   0.818731  0.750     1.00        1.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "fOOPHKCkv0XE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### What do you notice from the diagram below?"
      ]
    },
    {
      "metadata": {
        "id": "UI33JVjEvUKh",
        "colab_type": "code",
        "outputId": "6d48acb5-a5af-4c0b-dcea-4a694dd8f12e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "cell_type": "code",
      "source": [
        "sns.heatmap(scaled_comparisons);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAGbCAYAAAC77PVsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHxERNSosfERE1Kj8P55nMO4uYViqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "trnLEttcbPx2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Head, Tim [Cross Validation Gone Wrong](https://betatim.github.io/posts/cross-validation-gone-wrong/)\n",
        "\n",
        ">Choosing your input features is just one of the many choices you have to make when building your machine-learning application. Remember to make all decisions during the cross validation, otherwise you are in for a rude awakening when your model is confronted with unseen data for the first time."
      ]
    },
    {
      "metadata": {
        "id": "TlcV0N7lbPx4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Image of Machine Learning](https://www.capgemini.com/wp-content/uploads/2017/07/machinelearning_v2.png)\n",
        "\n",
        "\\[Image by Angarita, Natalia for [Capgemini](https://www.capgemini.com/2016/05/machine-learning-has-transformed-many-aspects-of-our-everyday-life/)\\]"
      ]
    },
    {
      "metadata": {
        "id": "3nfX15mDbPx7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**I don't fully support this diagram for a few reasons.**\n",
        "\n",
        "* I would replace \"Feature Engineering\" with \"Data Cleaning\"\n",
        "* Feature engineering can be done alongside either data cleaning or training your model--it can be done before *or* after splitting your data. (But it will need to be part of the final pipeline.)\n",
        "* Any feature standardization happens **after** the split\n",
        "* And you can use cross validation instead of an independent validation set\n",
        "\n",
        "However **feature selection (Goal 1) is part of choosing and training a model and should happen *after* splitting**. Feature selection belongs safely **inside the dotted line**.\n",
        "\n",
        "\"But doesn't it make sense to make your decisions based on all the information?\"\n",
        "\n",
        "NO! Mr. Head has a point!\n",
        "\n",
        "## The number of features you end up using *is* a hyperparameter. Don't cross the dotted line while hyperparameter tuning!!! Work on goal 1 AFTER splitting.\n",
        "\n",
        "I know you want to see how your model is performing... \"just real quick\"... but don't do it!\n",
        "\n",
        "...\n",
        "\n",
        "Don't!\n",
        "\n",
        "*(Kaggle does the initial train-test split for you. It doesn't even let you **see** the target values for the test data. How you like dem apples?)*\n",
        "\n",
        "![](https://media.giphy.com/media/3o7TKGoQ8721rQQ0es/giphy.gif)\n",
        "\n",
        "What you **can** do is create multiple \"final\" models by hyperparameter tuning different types of models (all inside the dotted line!), then use the final hold-out test to see which does best.\n",
        "\n",
        "**All this is said with the caveat that you have a large enough dataset to support three way validation or a test set plus cross-validation**\n",
        "\n",
        "## On the flip side, feature *interpretation* (Goal 2) can be done with all the data, before splitting, since you are looking to get a full understanding underlying the relationships in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "Bc6bcXVrbPx9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}